[
  {
    "url": "https://developer.bitmovin.com/encoding/docs/supported-output-options-for-per-title-encoding",
    "title": "Supported Output Options for Per-Title Encoding",
    "text": "Per-Title encodings work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nYou can see an example for setting up a per-title encoding with a supported storage option in this\ntutorial about how to create a per-title encoding\n.\nYou can check\nour release notes\nto see the latest updates.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/understanding-the-bitmovin-encoding-object-model",
    "title": "Understanding the Bitmovin Encoding Object Model",
    "text": "Introduction\nThis tutorial describes the various objects that make up a Bitmovin encoding, how they interrelate, and how they are created and manipulated through the APIs and SDKs. It will help you understand the necessary foundations to become an efficient Bitmovin video developer.\nIn the next few sections, we will go through a high level overview of the fundamental objects that model an encoding workflow.\nAPIs and SDKs\nBut first, let's quickly talk about the relationship between these objects and the APIs that you use to instantiate them. The Bitmovin solution at its core is a platform that exposes an HTTP-based REST API for interaction with other systems, in particular your own. A number of\nendpoints\nare provided to create, retrieve and delete configuration objects and trigger the workflows. The objects manipulated by each endpoint follow models that are defined in a specification\n(*1)\n. Most of the time, you will use one of our\nSDKs\nto interact with the platform. The role of the SDKs is fundamentally to allow you to work with the model, instead of having to work with the REST API directly. Their main tasks is to call the APIs for you, and in doing so to translate your objects into the appropriate JSON payloads sent in the HTTP requests, and translating the JSON responses back into those objects. Code completion and type checking are some of the advantages of this approach, all allowed by the strict modelisation of the solution.\nWhen you browse the\nAPI reference\n, you are presented with the models associated with each endpoint. Click the\nModel\nhyperlink to see them.\nTip\n:\nyou'll often find that the payload for the \"create\" (POST) request for any particular endpoint provides the clearest representation of one of the fundamental models associated with that endpoint. Indeed, the \"list\" and \"get\" (GET) requests often return multiple objects and additional request-level information in their responses and therefore get wrapped into other models.\nDocument conventions\nSome of the objects used (actually many of them) do not have endpoints dedicated to them. They are just \"internal\" models that are combined into more complex ones managed through these endpoints.\nTo try and make the text below clear and provide some disambiguation of often used terms, we will adopt the following notation:\nObject\nrefers to one of our models. The plural form\nObjects\nis just used as required for the text to read as correct English and does not refer to a different model!\nThese \"internal\" models that are not managed through dedicated endpoints will be indicated as\nObject*\nthe first time they appear.\nSome models are high level and get \"specialised\" before being exposed as different actual endpoints. We'll note those as\nObject^\nthe first time they are introduced. Developers could think of them as prototypes or superclasses.\nFinally, we use singular or plural form interchangeably in the text below, for clarity of the expression, but both forms refer to the same object, ie.\nInput\n==\nInputs\nPrincipal Objects\nThere are a number of objects that you will use every single time you work on an encoding workflow. They are the following:\nIn this diagram, boxes with solid border represent objects associated with API endpoints, dotted boxes are the \"internal\" models. Arrows represent the direction of dependencies. Orange markers are added to the objects whose endpoints have web methods used to trigger the associated generation workflows\nAn\nInput^\nrepresents a service (or server) that will be connected to in order to access the files or livestreams that will be ingested. There are implementations of this object into specific ones for different service providers, such as S3 (\nS3Input\n), SFTP (\nSftpInput\n) or RTMP (\nRtmpInput\n).\n(full list)\nSimilarly, an\nOutput^\nis a service or server that will be connected to in order to write files or livestreams generated by the encoder. There are multiple implementations of this object as well, such as GCS (\nGcsOutput\n) or Akamai MSL (\nAkamaiMslOutput\n).\n(full list)\nAn\nEncoding\nis the top-level object when it comes to an encoding workflow, and is the one to which will be associated all other objects that create the chain between those\nInput\nand\nOutput\nservices.\nA\nStream\nrepresents the video (or audio, caption, etc) data itself, the elementary stream\n(*2)\nwhich will be generated by the encoder. A\nStream\nneeds the following relationships to be functional:\nA\nStream\nwill be encoded from one or multiple input streams. These are retrieved from files (or from a livestream) located on one of the\nInputs\n.\nInputStreams\nare used to define those. There are different specialisations, based on the type and function of the input stream used to make the encoding. The principal ones are\nIngestInputStream\n,\nFileInputStream\n, and there are others used to define manipulations of input streams (such as concatenation, trimming, audio mixing, etc.). A\nStream\ncan have more than one\nInputStream\nassociated with it.\nThe\nStream\nwill be encoded with a codec, according to a particular\nConfiguration^\n, which will set all the codec parameters (such as resolution, bitrate, quality settings, etc.). There are implementations of the object specific for each codec, eg.\nH264VideoConfiguration\n,\nEac3Configuration\n, etc. (\nfull list\n)\nMultiplexing (in short: muxing) is the process of combining encoded bitstreams into the containers that can be written to file or streamed as livestreams. The\nMuxing^\nobjects represent those containers. There are different specialisations of\nMuxings\nfor different containers, such as for fragmented MP4 (\nFmp4Muxing\n), progressive TS (\nProgressiveTsMuxing\n) or MP3 (\nMp3Muxing\n).\nMuxings\nhave the following fundamental relationships:\nA\nMuxing\ncan contain one or multiple\nStreams\n. The association is done through\nMuxingStream*\nobjects\nThe\nMuxings\ncan be written as files or livestreams onto multiple\nOutput\nservices at the same time. To make the association between those objects, and provide the necessary additional information (such as output paths, filenames and access permissions), we use\nEncodingOutput*\nobjects. A\nMuxing\ncan have more than one.\nWe now have all the fundamentals pieces in place that chain together. Let's summarize it:\nIn short...\nAn\nEncoding\nrepresents a workflow allowing the Bitmovin encoder to grab one or more\nInputStreams\nfrom\nInput\nservices, encode them with codec\nConfigurations\ninto\nStreams\n, multiplex them into\nMuxings\nand output them onto\nOutput\nservices in locations defined by\nEncodingOutputs\nDRM\nEveryone's favourite topic and the object of primal fears among video developers... At least the modelling of it is quite simple: A\nDrm^\nobject contains all the information required to configure the encryption of a\nMuxing\n. It has different implementations for different DRM methodologies such as\nCencDrm\n,\nFairplayDrm\n,\nClearKeyDrm\n, etc.\nThe\nDrm\nobject also links to\nOutput\nservices (via\nEncodingOutput\nobjects), to represent the encrypted version of the files or livestreams to be generated. One of the great advantages of this model is that since the\nMuxing\ncan now be written both as an unencrypted version (through its own link with an\nOutput\n) and as an encrypted one (via the\nDrm\nobject). Perfect if you want to send the DRM'ed content to your origin service, and the clear one to long-term storage!\nAdaptive Streaming\nWhen considering adaptive bitrate (ABR) streaming, there is another concept to add to the previous ones. There is a component in the Bitmovin solution that is distinct (but closely related) to the encoder: the manifest service. It is in charge of writing the manifest files that associate multiple generated files and streams into payloads that an ABR player can consume and stream.\nNumerous new objects are used to model ABR formats, worth a dedicated tutorial, but one in particular needs highlighting here:\nThe\nManifest^\nobject is used to represent those, through implementations such as\nHlsManifest\n,\nDashManifest\nand\nSmoothStreamingManifest\n. It has the following relationships:\nTo\nOutput\nservices, via\nEncodingOutput\n, just like before.\nTo one or more\nEncodings\n, from which it will link (indirectly) to the\nMuxings\nand\nStreams\nthat are combined as individual representations (or renditions) for the ABR stream.\nInfrastructure\nAt Bitmovin, we are proud to have engineered an encoding solution that works very much the same way, whether used in one of our managed clouds (whether on Amazon AWS, Google GCP or Azure), on our customers' own clouds, or even on on-premise infrastructure. The object model reflects that, by allowing an\nEncoding\nobject to be linked with infrastructure objects such as\nKubernetesCluster\nor\nAwsAccount\nto simply control where a particular encoding will be performed.\nAnd also...\nThere are loads of other objects that can be used to create more complex encoding configurations. Some of the ones worth highlighting:\nFilter^\nobjects are used to apply a transformation onto a\nStream\n. For example, a\nWatermarkFilter\nwill embed an image into a video, a\nDeinterlaceFilter\nwill turn an interlaced video into a progressive one, an\nAudioMixFilter\nwill allow you to remap audio channels. One or multiple\nFilters\nare applied on a\nStream\nthrough\nStreamFilters*\n, which also define their order of application.\nThumbnail\nand\nSprite\nobjects define the way in which thumbnails and sprites will be captured from a\nStream\nand via what\nEncodingOutput\nthey will be written to an\nOutput\nservice.\nSidecarFile^\nobjects control the way that additional files retrieved from an\nInput\ncan be pushed to one or more\nOutput\ns (via an\nEncodingOutput\n).\nWebVttSidecarFile\nis a prime example of it.\nA number of objects handle subtitles, generally by connecting to an\nInput\nto retrieve a file or extract a stream, and either embedding them into a\nStream\nor a\nMuxing\n, or converting them and transferring them through an\nEncodingOutput\nto an\nOutput\n, eg.\nSccCaption\n,\nTtmlEmbed\n,\nConvertSccCaption\n,\nWebVttEmbed\n,\nWebVttExtract\n,\nBurnInSubtitleSrt\n, etc.\nKeyframe\nobjects are used to control the insertion of keyframes into an\nEncoding\nat a high level, whereas\nId3Tag\ninsert ID3 tags into\nMuxings\n.\nAnd pretty much every single object that is managed through an endpoint also supports the ability to have\nCustomData\nassociated with it, in the form of key/value pairs\nFootnotes\n(*1) - We use the OpenAPI 3.0 standard to define our API spec. It offers many advantages, including the ability to generate SDKs automatically!\n(*2) - It is worth stopping for a minute on this word: \"stream\". It has the potential to be very ambiguous, as it is used extensively both by developers and video engineers to mean more than one thing. Here are some of them that you should know and be able to differentiate. The context will help you differentiate between them.\nthe\nelementary stream\n, the output of an encoder before multiplexing takes place. By and large that is what our\nStream\nobject relates to. If you look at the output of a tool like\nffprobe\nor\nmediainfo\nfor a given file, you will see that it lists the streams contained in that file.\nthe generic term used to portray the act of transmitting and receiving\nmedia\n, such as a \"live stream\", or an \"ABR stream\".\nIn Java 8, the awesome\nnew functionality\nthat allows us to use functional-style operations on collections. You may see those in our Java SDK examples.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/167bf89-image.png",
      "https://files.readme.io/1dd046e-image.png",
      "https://files.readme.io/f5817bc-image.png",
      "https://files.readme.io/3934345-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/muting-and-unmuting-webhooks",
    "title": "Muting and Unmuting Webhooks",
    "text": "Webhook notifications are a great way to handle your encoding workflows in an event-based manner instead of polling the status of its processes all the time.\nWhy would I have to unmute a webhook notification?\nIt can happen though that the URL of an webhook notification becomes unreachable (e.g. no longers responds with\n200 OK\n), therefore the Bitmovin API can't deliver its message. A Webhook notification delivery counts as failed if:\nIt takes longer than 30s to establish a connection (connection timeout)\nIt takes longer than 10s to complete the request (read timeout)\nIf the notification delivery fails 10 times a row, this webhook notification will be\nauto-muted\n. This means that the API will no longer attempt to deliver any events to that webhook until the user\nensured the availability of the URL for their backend again\nunmutes the notification in the Bitmovin API\nHow can I mute/unmute notifications?\nList all notifications\nAPI Reference:\nGET /notifications\nThis step is needed to obtain the ID of the notification you want to mute or unmute.\nShell\ncurl --location --request GET 'https://api.bitmovin.com/v1/notifications' \\\n--header 'x-api-key: API-KEY-HERE' \\\n--header 'x-tenant-org-id: ORGANISATION-ID-HERE'\nBitmovin Open API SDK for Java Example:\nJava\nbitmovinApi =\n        BitmovinApi.builder()\n            .withApiKey(\"API-KEY-HERE\")\n            .withTenantOrgId(\"ORGANISATION-ID-HERE\");\n  \n//Returns the ID of the first notification found in the API response\nbitmovinApi.notifications.list().getItems().get(0).getId();\nMute a Notification\nAPI Reference:\nPOST /notifications/{NOTIFICATION-ID-HERE}/mute\nShell\ncurl --location --request POST 'https://api.bitmovin.com/v1/notifications/NOTIFICATION-ID-HERE/mute' \\\n--header 'x-api-key: API-KEY-HERE' \\\n--header 'x-tenant-org-id: ORGANISATION-ID-HERE'\nBitmovin Open API SDK for Java Example:\nJava\nbitmovinApi =\n        BitmovinApi.builder()\n            .withApiKey(\"API-KEY-HERE\")\n            .withTenantOrgId(\"ORGANISATION-ID-HERE\");\n\nbitmovinApi.notifications.mute(\"NOTIFICATION-ID-HERE\");\nUnmute Notification\nAPI Reference:\nPOST /notifications/{NOTIFICATION-ID-HERE}/unmute\nShell\ncurl --location --request POST 'https://api.bitmovin.com/v1/notifications/NOTIFICATION-ID-HERE/unmute' \\\n--header 'x-api-key: API-KEY-HERE' \\\n--header 'x-tenant-org-id: ORGANISATION-ID-HERE'\nBitmovin Open API SDK for Java Example:\nJava\nbitmovinApi =\n        BitmovinApi.builder()\n            .withApiKey(\"API-KEY-HERE\")\n            .withTenantOrgId(\"ORGANISATION-ID-HERE\");\n\nbitmovinApi.notifications.unmute(\"NOTIFICATION-ID-HERE\");",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/zixi-inputs",
    "title": "Zixi Inputs",
    "text": "Bitmovin supports Zix Inputs from a licensed Zixi Broadcaster device, with the Bitmovin Live Encoder running in ZEC Receiver mode.\nCurrently it is only possible to configure Zixi Inputs using the\nAPI",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/2a9ee8a-zixi-bitmovin-diagrama-cloud-logos.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/contribution-devices",
    "title": "Contribution devices",
    "text": "Contribution devices are any hardware or software platforms that originate further up stream in the transmission chain, and provide the Live Encoder with, typically, a single bit rate input signal.\nIn theory any device that is capable of outputting the supported input\nstreaming formats\n,\nvideo codecs\nand\naudio codecs\nwill be compatible with the Bitmovin Live Encoder.\nHowever in certain cases, we have seen vendors and suppliers implementing the standards in a non-conventional manner, in instances where a device is not able to connect please reach out to support and we'll assist in troubleshooting.\nSet up guides\nFor ease of use, we have provided some guides on how to connect with common devices and platforms.\nConnecting with OBS",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/bitmovin-approved-contribution-devices",
    "title": "Bitmovin approved contribution devices",
    "text": "The following devices have been tested with the Bitmovin Live Encoder and confirmed to be interoperable by Bitmovin Engineering or trusted customers.\nSupplier\nProduct\nRTMP\nSRT\nZixi\nVideon\nEdgecaster, Edge Max\n✅\n✅\nGrass Valley\nAMPP\n✅\n✅\nOsprey\nTalon 4K\n✅\n✅\nOBS\n✅\nRestream\n✅\nCastr\n✅\nZixi\nZixi Broadcaster\n✅\nStreamYard\n✅\nHaivision\nMakito FX Encoder\n✅\nHaivision\nMakito X4 Encoder\n✅\nHaivision\nMakito X Encoder\n✅\nHaivision\nMakito X1\n✅\nHaivision\nStreamHub\n✅\nVolkert Software\nMulti-Streamer\n✅\nSienna\nND Processing Engine\n✅\nSwitcher.ai\n✅\nNanocosmos\nnanoStream Cloud\n✅\nLive X\nVirtual Video Control Room\n✅\nLive X\nRivet\n✅\nLiveU\nLU800, LU300S, Solo\n✅\n✅\nLiveU\nMatrix\n✅\nCloudflare\nCloudflare Stream\n✅\nTechex\nMWEdge\n✅\n✅\nGlobalM\nGlobalM network\n✅\nPIxellot\n✅\n✅\n✅",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/managing-your-payment-billing-details",
    "title": "Managing Your Payment & Billing Details",
    "text": "To subscribe to one of our product plans, you need to complete your payment & billing information first. This is also important to create a correct invoice for you or your organization.\nGo to\nPayment and Billing\nin the Bitmovin Dashboard.\nManaging your\npayment details\nAdd or update your payment method:\nWhy can't I add my credit card to my account?\nIf you can't add your credit card to your account, the most common reasons are:\nIncorrect details:\nPlease make sure to enter the correct card number, expiration date and CVV code.\nExpired credit card:\nPlease get a valid card from your provider, or use a different one.\nIncorrect Billing Address:\nMake sure the Billing Address details and postal code details are correct.\nUnsupported Credit Card Provider:\ne.g. virtual credit cards can have restrictions applied to them that prevents from being used with our services. (supported Credit Card Providers: Visa, Mastercard, American Express, Discover, JCB, Diners Club, China UnionPay, debit cards)\nDebit Card + PIN requirement:\nIf their card requires a PIN to do payments it won’t work with our online service. Please use a different card.\nGeo Restrictions:\nOur payment gateway is located in the US. If the card prohibits to be used there it could be a reason for it to be declined.\nActively Declined by Bank/Provider:\nIf none of the above applies it can still be that the bank/provider declines the usage of their card for our services. E.g. because their protection system recognized us as a threat, or a small test charge when adding a credit card failed, so did the validation. They would have to get in touch with their provider and have them allowlist us or try a different credit card.\nIf you are a Tenant user who wants to adjust billing details in another Organisation:\nIf you are facing issues changing any billing settings, contact the account owner if your user account has sufficient permissions to do so. The minimum set of required permissions is \"Billing - View & Manage\".\nIf you need further assistance, please create a ticket through the\nBitmovin Dashboard\n.\nManaging your\nbilling details\nAdd or update your billing address:\nExporting invoices and statements\nYou can download your invoices & statements by clicking on the export button in the \"PDF\" column:",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/a4966c6-credit_card_update_screen.png",
      "https://files.readme.io/1071b24-billing_address_screen.png",
      "https://files.readme.io/375f13065e2018bee71117f5324582524ccd3a65817467c046c7b52fb4e2e97e-Screenshot_2024-10-25_at_11.00.42_AM.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-to-vod-workflows",
    "title": "Live-to-VOD workflows",
    "text": "Overview\nLive-to-VoD is a common feature used in live streaming applications, and roughly covers scenarios that convert live streams into on-demand assets. The goal is to allow users to watch the replay of a live event after it has ended. It also creates opportunities to reuse the content while a live event is still running, for example, for doing clipping and promotional highlights for social media marketing, or “latest news” snippets.\nThanks to the flexibility of the Bitmovin API, a Live-to-VoD use case is easily supported by making just a few API calls. Over the next sections, we will cover a general overview of this feature in a Bitmovin workflow, some API details to consider when implementing this in practice, and some code snippets to get started.\nLive-to-VoD workflows\nCustomers can initiate a Live-to-VoD workflow at any point in time after the encoding either has been created, is still running or even has finished\n(1)\n. In this way, we can convert a live stream to VoD almost independently of the main live encoding workflow status. In practice, this workflow enables use cases where a Live to VoD conversion is needed either for a live event that is waiting to get started, being streamed or has already ended.\n(1) See our\nencoding status API\nfor more details about the encoding status we report.\nWith Bitmovin there are two options for Live-to-VoD:\nVoD manifest generation\nLive Recording",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-per-title-encoding-with-ssai",
    "title": "Using Per-Title Encoding with SSAI",
    "text": "Yes, that is possible!\nSSAI providers are usually encoding the ads to a pre-defined set of resolutions and bitrates which is quite static. Changes in resolution are problematic, changes in bitrate not so much. That is why you can just configure the Per-Title encoding to restrict the possible resolutions to a set that is provided by the SSAI provider. The configuration works in the same way as\nconfiguring required resolutions for your Per-Title encoding\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-access-and-secret-keys-for-google-cloud-storage",
    "title": "Creating Access and Secret Keys for Google Cloud Storage",
    "text": "Those keys are called \"interoperable keys\" or \"migration keys\". As Google Cloud Storage does provide an S3 interface you could use an S3 Client, like \"S3 Browser\", or CLI tool like \"s3cmd\" in order to work with Google Cloud Storage buckets as well.\nIn order to create an access/secret key pair to access your Google cloud storage bucket:\nGo to\nhttps://console.cloud.google.com/storage/settings\nSelect the \"Interoperability\" tab\nIf you haven't enabled it already, click on \"Interoperable Access\"\nNow you should see an empty list and a \"Create new Key\" button\nClick on the button in order to create an access/secret keypair\nBe aware\n: Those keys doesn't belong to the google project but your own account, which you are using to login to the google cloud console.\nMore information about those keys and how to create them are available in the Google Cloud Storage documentation:\nhttps://cloud.google.com/storage/docs/migrating#keys",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/472fe09-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/changes-to-fmp4-outputs-in-encoder-version-21530",
    "title": "Changes to fMP4 outputs in Encoder version 2.153.0",
    "text": "Overview\nThis article describes the changes to fMP4 outputs starting from version 2.153.0 of the Bitmovin Encoder. Starting with this version, fMP4 outputs with codecs H.264, AAC, HE-AAC and HE-AACv2 will use an overhauled implementation that aims to improve stability and correctness of ISO-BMFF files. AV1 has already been using the overhauled fMP4 implementation.\nIn this article, we're going to explicitly show the differences in MP4 outputs, by comparing excerpts from\nmp4dump\noutputs of fMP4 encodings up to version 2.152.0 versus encodings starting from 2.153.0 for the same configuration.\nFinally, we'll list the devices/platforms used for testing playback.\nChanges to ISO-BMFF boxes\nftyp boxes\nUp to version 2.152.0, the ftyp box for an H.264 initialization segment looks like\n[ftyp] size=8+20\n  major_brand = mp42\n  minor_version = 1\n  compatible_brand = isom\n  compatible_brand = mp42\n  compatible_brand = mp41\nWith version 2.153.0, the ftyp box of an H.264 initialization segment looks like\n[ftyp] size=8+32\n  major_brand = mp41\n  minor_version = 0\n  compatible_brand = iso8\n  compatible_brand = isom\n  compatible_brand = mp41\n  compatible_brand = dash\n  compatible_brand = avc1\n  compatible_brand = cmfc\nAudio initialization segments should look the same, except for the\navc1\ncompatible brand, which is not present.\nThis should not have any practical side effect for demuxers, so we don't expect and have not found any issues with the change.\nTimescale in mvhd and tkhd boxes\nUp to version 2.152.0, the timescale in Movie Header (mvhd) box had value of 1000, and the timescale in Track Header (tkhd) box depended on:\nframe rate for video\nsampling rate for audio.\nFrom version 2.153.0, the mvhd timescale is the same as the tkhd timescale.\nVideo timescale\nThe video timescale is the video framerate rounded to nearest integer multiplied by 1000. Some examples:\n24 FPS: Timescale 24000\n23.976 FPS: Timescale 24000\nAudio timescale\nThe audio timescale equals the sampling rate. This applies to all audio codecs, including HE-AAC and HE-AACv2 codecs, which up to version 2.152.0 had a timescale of half the sampling rate.\nmax_bitrate and avg_bitrate in DecoderConfig for audio\nWe noticed that our audio initialization segments, up to version 2.152.0, always reported 96 kbps as max_bitrate and avg_bitrate under the DecoderConfig box for audio, regardless of the configured bitrate. This was a problem only in muxing. The audio was encoded at the correct bitrate.\n[esds] size=12+27\n  [ESDescriptor] size=2+25\n    es_id = 0\n    stream_priority = 0\n    [DecoderConfig] size=2+17\n    stream_type = 5\n    object_type = 64\n    up_stream = 0\n    buffer_size = 6144\n    max_bitrate = 96000\n    avg_bitrate = 96000\n    DecoderSpecificInfo = 12 10\n    [Descriptor:06] size=2+1\nStarting from version 2.153.0, this issue is fixed and the values are correctly signaled depending on the specified bitrate from the codec configuration.\nSample flags in trun entries\nIn video segments up to version 2.152.0, the sample flags were always optimized using\ndefault sample flags\nfrom the tfhd box and\nfirst sample flags\nfrom the trun box, as shown below:\n[traf] size=8+832\n    [tfhd] size=12+8, flags=20020\n      track ID = 1\n      default sample flags = 0x1010000\n    [tfdt] size=12+8, version=1\n      base media decode time = 0\n    [trun] size=12+780, flags=a05\n      sample count = 96\n      data offset = 872\n      first sample flags = 0x2000000\n      entries:\n        (       0) sample_size = 429, sample_composition_time_offset = 2002\n        (       1) sample_size = 72, sample_composition_time_offset = 5005\n        (       2) sample_size = 70, sample_composition_time_offset = 2002\nWhile this optimization is good for reducing muxing overhead, our muxer was optimizing the flags even in situations when it shouldn't, e.g. when there are more key frames than the first frame. This resulted in warnings on the Media inspector tab on Chrome:\nISO-BMFF container metadata for video frame indicates that the frame is not a keyframe, but the video frame contents indicate the opposite.\nStarting from version 2.153.0, sample flags in the trun boxes in a given segment are only optimized if it is possible. Otherwise, the flags are written per sample, like below:\n[traf] size=8+1212\n    [tfhd] size=12+12, flags=2000a\n      track ID = 1\n      sample description index = 1\n      default sample duration = 1001\n    [tfdt] size=12+4\n      base media decode time = 0\n    [trun] size=12+1160, flags=e01\n      sample count = 96\n      data offset = 1252\n      entries:\n        (       0) sample_size = 473, sample_flags = 0, sample_composition_time_offset = 2002\n        (       1) sample_size = 78,  sample_flags = 0x1000, sample_composition_time_offset = 5005\n        (       2) sample_size = 76, sample_flags = 0, sample_composition_time_offset = 2002\n        (       3) sample_size = 75, sample_flags = 0x1000, sample_composition_time_offset = 0\nWhile this may increase the muxing overhead with 4 extra bytes for sample when the optimization can not be applied, it provides the most correct outputs, which may also fix playback in older players. Our muxer will still optimize the flags when it is possible.\nFurthermore, the value set in sample_flags at version 2.153.0 (0 and 0x10000) differ slightly from the ones set in 2.152.0 (0x2000000 and 0x1010000) because we don't make use of the\nsample_depends_on\nbits anymore (ISO/IEC 14496-12:2012 8.6.4.3).\nEdit Lists with the edts box\nWe noticed that up to version 2.152.0, an edit list could be missing for H.264 streams that make use of B-frames. This edit list is required due to a delay between decoding and presenting frames (signaled in trun entries via\nsample_composition_time_offset\n), and due to this, the first segment of some streams could have a non-zero reported start time, for example when checking it with ffprobe:\nDuration: 00:00:04.00, start: 0.083417, bitrate: 4396 kb/s\nFrom version 2.153.0, the edit list is placed in the initialization segment when needed:\n[edts] size=8+28\n  [elst] size=12+16\n    entry_count = 1\n    entry/segment duration = 0\n    entry/media time = 2002\n    entry/media rate = 1\nWith this, the first segment correctly starts at 0 now:\nDuration: 00:00:04.00, start: 0.000000, bitrate: 4398 kb/s\nFor avoiding the use of edit lists, it is also possible to use of the ALIGN_ZERO_NEGATIVE_CTO in the \"ptsAlignMode\" configuration of fMP4 muxings, which makes use of trun v1 boxes that allow for negative\nsample_composition_time_offset\n.\nPlayback testing\nBefore releasing this change, Bitmovin has conducted extensive device testing to make sure that the new outputs won't have playback regressions. The following devices/platforms were tested with non-DRM and DRM outputs, using the Bitmovin player:\nChrome and Edge (stable, beta and dev) on MacOS, Linux and Windows\nFirefox on MacOS, Linux and Windows\nSafari on iPad Air 2 (iOS 13), iPad Mini 6 (iOS 15), iPhone 11 (iOS 14), iPhone 8+ (iOS 12)\nSamsung Tizen TVs, from 2016 to 2022 models\nLG WebOS TVs, from 2016 to 2022 models\nPanasonic TV 2018\nXbox One and Xbox Series S\nPlaystation 5\nChromecast and Chromecast Ultra\nAndroid Pixel2 with browsers Chrome, Firefox and Samsung Internet\nFire TV Stick 4K and Fire TV Stick 4K Max\nRoku Streaming Stick, Roku Streaming Stick 4K",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoder-200-2490",
    "title": "Encoder 2.0.0 - 2.49.0",
    "text": "2.49.0\nReleased 2020-07-14\nAdded\nFor video encodings a display aspect ratio (DAR) for the output can be specified. If the DAR does not match with the width and height, then the sample aspect ratio (SAR) will be adjusted accordingly.\nAdded support for\nHLS iFrame playlist generation\nfor AVC/HEVC in fMP4 with and without encryption.\nAdded support for passthrough of\nDTS audio codec\n.\nFixed\nFor Zixi/SRT live streams, if the ingest disconnects and reconnects again with a different stream layout: audio silence will be written for audio streams, which cannot be found in the new ingest; if the video stream is not found again the live stream will go into\nERROR\nstate.\nFixed invalid DASH fMP4 output with non-integer segment lengths. This resolves the related playback issues.\nA timeout with retry mechanism has been added for live manifests uploads to S3, Generic S3, and GCS in order to avoid player buffering for uploads that take too long.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.48.1\nReleased 2020-07-21\nFixed\nFixed an issue when the\ncustomData\nproperty of a resource is configured and the value is not valid JSON.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.48.0\nReleased 2020-06-30\nAdded\nAllow manual restart of live encodings in state FINISHED or ERROR\nEnabled restart of live encodings if the encoder is unresponsive due to an instance failure (hardware, software, network issue etc.). This will lead to the live encoding being started on an instance with a new IP address.\nImproved download speed of\nGCS Service Account input\nup to 15x\nAdded option to set\nsuggestedPresentationDelay\n,\nminimumUpdatePeriod\nand\navailabilityStartTimeMode\nto the\nDashManifests\nwhen\nstarting a live encoding\nFixed\nFixed a bug which prevented the creation of Per-Title encodings in combination with h.265, BroadcastTS and AutoRepresentations.\nFixed a bug in the parameter set signaling of segmented TS outputs. The parameter sets were signaled with a 3 byte start code (0x00 0x00 0x01) where according to the standards the start code must be 4 bytes (0x00 0x00 0x00 0x01).\nA possible stall situation with high-bitrate input files when quality metrics are enabled has been fixed\nFixed a stall situation caused by a high amount of logs produced in a small amount of time\nFixed an issue where the\npublishTime\nin a\nSegmentTimeline\nDASH Manifest might be updated, although the content of the manifest hasn't changed\nFixed webm segments content-type detection.\nWhen using ProgressiveWebM with manifestType DASH_ON_DEMAND and DRM, the first 5 seconds are also encrypted now.\nFixed an encoding job error when thumbnail width or height was not a multiple of 4.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.47.2\nReleased 2020-06-09\nChanged\nWhen using\nProgressiveWebM\nwith\nmanifestType\nDASH_ON_DEMAND\nand DRM, the first 5 seconds are also encrypted now.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nEncoding jobs will fail if the thumbnail width or height is not a multiple of 4\n2.47.1\nReleased 2020-06-08\nFixed\nFixed an issue where the\npublishTime\nin a\nSegmentTimeline\nDASH Manifest might be updated, although the content of the manifest hasn't changed\nResolved an internal condition where the encoder might stall while processing content\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nEncoding jobs will fail if the thumbnail width or height is not a multiple of 4\n2.47.0\nReleased 2020-06-03\nFixed\nA possible stall situation with input chunks (GOPs) of a size of about 2 GB has been fixed and the stabililty for even larger chunks has been improved.\nFixed a bug in the parameter set signaling of segmented TS outputs. The parameter sets were signaled with a 3 byte start code (0x00 0x00 0x01) where according to the standards the start code must be 4 bytes (0x00 0x00 0x00 0x01).\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nThe encoder might stall on certain conditions. Please use the next hotfix version.\nEncoding jobs will fail if the thumbnail width or height is not a multiple of 4\n2.46.2\nReleased 2020-06-18\nFixed\nFixed an encoding job error when thumbnail width or height was not a multiple of 4\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.46.1\nReleased 2020-06-08\nFixed\nFixed an issue where the\npublishTime\nin a\nSegmentTimeline\nDASH Manifest might be updated, although the content of the manifest hasn't changed\nResolved an internal condition where the encoder might stall while processing content\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nEncoding jobs will fail if the thumbnail width or height is not a multiple of 4\n2.46.0\nReleased 2020-05-19\nAdded\nAdded support for\nstream conditions\nfor checking the video rotation metadata\nEnable\ntimecode-track\ntrimming based on the Quicktime timecode track in input files\nAdded upload verification of segmented muxings for Azure Storage\nFixed\nFixed a rare bug that caused the failure of a progressive TS muxing due to restrictions of the underlying operating system\nFixed encoding failures for muxings with DROP_MUXING stream condition mode and multiple streams with one of them being removed by stream conditions\nThe encoder might stall on certain conditions. Please use the next hotfix version.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nEncoding jobs will fail if the thumbnail width or height is not a multiple of 4\n2.45.0\nReleased 2020-05-05\nAdded\nOptimized memory utilization and enhanced resiliency against memory errors\nFixed\nFixed a crash for Zixi live input after a restart. Discontinuity occurrence right at the beginning of the stream or after a restart request generated a crash. This is handled correctly now.\nFixed an issue where fMP4 last audio segment was sometimes missing, as it consisted of only 1 audio frame and was incorrectly encoded into next-to-last segment.\nFixed a bug that failed an encoding with CEA-608/708 captions from a separate input stream in specific cases\nImproved error resiliency for decoding audio streams for live encodings.\nFixed the\nencodedBytes\nfor progressive muxings with multiple streams in the\nencoding statistic endpoint\nfor the\nmuxings\nitems.\nFixed a bug in the Per-Title logic for fixed resolutions with bitrate constraints, where a fixed resolution between two fixed resolutions with bitrate constraints was not handled properly.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nEncoding jobs will fail if the thumbnail width or height is not a multiple of 4\n2.44.2\nReleased 2020-04-24\nFixed\nFixed bug that when using short input files with Progressive WebM muxings and DASH On-Demand manifests the resulting manifests might contain the wrong information for the referenced streams and muxings\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.44.1\nReleased 2020-04-21\nFixed\nSupport for Burn-in DVB subtitles that are relative to the input resolution instead of subtitle decoding context resolution\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.44.0\nReleased 2020-04-21\nAdded\nCollects muxing information for fMP4 muxings accessible via\nthis endpoint\n.\nFixed\nFix for H264 in fMP4: The\ndefault-sample-flags\nfield is now always set\nAES-128 encryption for fMP4 now also works for Per-Title\nFixed EAC3 audio input support for live streams\nUndecodable DVB subtitles will not cause the live encoding to go into\nERROR\nstate but they will be skipped instead\nImproved stability of Per-Title encodings using a VP9 codec configuration and fixed resolution template streams\nImproved stability of live stream restarts\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.43.0\nReleased 2020-04-07\nAdded\nAdded Support for Service Account based GCS\ninputs\nand\nouputs\nAdded\nsourceChannels\nto\nAudioMixInputStreamChannel\nfor\naudio mix input streams\nto allow to mix and merge channels.\nAdded the possibility to select subtitle streams via\nselectionMode\nSUBTITLE_RELATIVE\n. See\nstreams\n.\nFixed\nThe\nignoredBy\nproperty will be set correctly for text muxing streams\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.42.0\nReleased 2020-03-24\nAdded\nAdded AES encryption support for\nHLS with fMP4\nFixed\nFixed an issue where fMP4 muxings reported an incorrect bitrate when using multiple outputs for the muxing\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.41.0\nReleased 2020-03-10\nAdded\nAdded support for\nSegment List\nfor MP4 Dash On-Demand Manifests\nAdded support for\nAES-128 Encryption for Progressive TS\nmuxings\nAdded support for\nstream conditions\nfor checking if CC608/708 is available\nAdded retries with exponential backoff for connecting to SRT and Zixi live input ingests.\nFixed\nEncodings of MPEG2 source inputs with faulty interlaced packets, first field present but the second field absent, failed earlier. This is handled correctly now.\nEncoding with mov containers containing an AAC audio stream without extradata failed earlier.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.40.2\nReleased 2020-04-14\nFixed\nAdded missing default-sample-flags in container for fragmented MP4 output, in order to fix playback and seeking issues on some players (e.g., FireTV 4K)\n2.40.1\nReleased 2020-03-04\nFixed\nAdd correct\ncenc:pssh\nand\nmspr:pro\nelements to\nContentProtection\nsection of DASH manifests when using CENC PlayReady DRM for\nfMP4\nmuxings in live encodings.\n2.40.0\nReleased 2020-02-25\nAdded\nAdded support for the audio codec\naac_latm (HE_AAC)\nfor live ingest streams\nTime-based trimming of inputs with a different offset value is now supported\nSupport CENC Widevine and CENC Clearkey for\nprogressive WebM muxings\nand via\nSPEKE\nCEA-608/708 captions from a separate input track can be added to a H.264 and HEVC video stream.\nSCC Subtitles from a seperate text file can now be passed through into an HEVC output.\nFixed\nAdapted the format of the\ncenc:default_KID\nin dash manifests to an UUID format for\nfMP4\nmuxings.\nFixed a rounding error which caused in rare cases that the DASH manifest contained one segment too much\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.39.0\nReleased 2020-02-24\nAdded\nSCC\nto CEA 608/708 subtitle conversion for\nH265 codec configurations\n.\nCEA 608/708 subtitle passthrough for\nH265 codec configurations\nvia property\ncea608708SubtitleConfig\n.\nAdded\nsshPort\nand\nfaspPort\nto Aspera inputs to be able to configure non-standard ports for session intialization and data transfer.\nCustomers using their own\nAWS account\ncan now opt-in, so that all SSH communication comes from one specific IP address. This enables a more rigid network security policy where only one IP is allowlisted for SSH connections.\nFixed\nAdd correct\ncenc:pssh\nand\nmspr:pro\nelements to\nContentProtection\nsection of DASH manifests when using CENC PlayReady DRM for\nMP4\nand\nfMP4\nmuxings.\nCENC Marlin is now also applied to muxings generated by the Per-Title algorithm.\nEnhanced stability of our 24/7 live encoders by fixing smaller memory leaks that led to failures on long-running streams.\nFixed an issue where the burn-in of subtitles stops after some time in long running 24/7 live encodings.\nImproved sync of burn-in subtitles to audio/video streams. Reduced the risk of stalled encodings by making the transcoding tasks time-boxed.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.38.0\nReleased 2020-01-28\nFixed\nFixed a bug that potentially produced SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps. (caused the segment to be silent in Safari browser)\nFixed a bug which prevented the update of DASH Live Manifests\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.37.0\nReleased 2020-01-14\nAdded\nBurning in of\nDVB subtitles into the video stream\nis now supported for live encodings.\nFixed\nInsertable Content no longer requires to have the same sample aspect ratio as the livestream. Additional requirement for Insertable Content: video stream must be at position 0 of the movie file.\nFixed bug which caused encoding to fail when setting stream conditions on Per-Title template streams.\nFixed an issue with audio codec detection on specific input files that lead to no audio and 2x video duration output.\nFixed bug that potentially caused stalled encodings in case that video filters were applied and several output renditions were encoded in parallel.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.36.2\nReleased 2020-08-03\nAdded\nEnable\ntimecode-track\ntrimming based on the Quicktime timecode track in input files\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.36.1\nReleased 2020-05-19\nFixed\nFixed a bug that potentially produced SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps. (caused the segment to be silent in Safari browser)\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.36.0\nReleased 2019-12-17\nAdded\nSupport\nImage Overlay\nfor live encodings. The specified image will replace the ingested video of the live stream for the defined time. Audio will be muted.\nSupport concatenation for different resolution inputs\nSpecify\nAspectMode\nwhen adding a concatenated input stream\nSupport for\ncustom XML in PlayReady DRM WRM header\nadded for DASH (\nplayReady\n->\nadditionalInformation\n)\nTrimming for Dolby Vision mezzanine files is now supported\nH.265/HEVC 2pass and 3pass encoding jobs will process up to 10% faster\nEnhanced detection of open GOP files to correctly handle them.\nFixed\nCC to\nWebVTT conversion\nwas uncompliant after one hour when\nappendOptionalZeroHour\nwas set to true\nThere has been a potential stalling condition related to big output configurations (e.g. when excessive thumbnails creation was enabled).\nHDR-10 encoding jobs with big output configurations where likely to stall in previous versions.\nTS descriptor for E-AC-3 audio codecs is now correctly set\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.35.1\nReleased 2019-12-03\nFixed\nImproved analyzing of input for fail-fast in case erroneous DTS and unsupported open-GOP files\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.35.0\nReleased 2019-12-03\nAdded\nConcatenation input streams\nnow support inputs with different frame rates.\nInsertable content\nnow is also scheduled when a live stream was restarted.\nAdded warning and auto-correction for possibly harmful\ntileColumns\nand\ncpuUsed\nparameter combinations of\nVP9 codec configurations\n.\nFixed\nPer-Title is now supported for Dolby Vision workflows.\nIn the concatenation workflow, if more than 10 inputs were provided, encoding produced incorrect output.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.34.0\nReleased 2019-11-19\nAdded\nAdded native support for\nAzure Blob output\n. This makes the additional transfer step unnecessary.\nAdded support for output of Dolby Vision and non Dolby Vision output streams in a single encode\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nPer-Title is not supported for Dolby Vision\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.33.0\nReleased 2019-11-05\nAdded\nIf a running VoD encoding is\nstopped\nit will now go to status\nCANCELED\n, live encodings will still go to\nFINISHED\n.\nImproved resilience against\nDocker Hub registry outages\n.\nFixed\nEncodings using\nenhanced watermark filter\nmight have failed before when setting opacity and\nPERCENTS\nas unit.\nFixed a bug, that caused a failure during retries for uploading files.\nA special log redirection in the encoder could have led to stalles in previous versions.\nFixed a SegFault that occured related to cluster communication.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\nPer-Title is not supported for Dolby Vision\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.32.0\nReleased 2019-10-29\nAdded\nAVC/H.264 2-pass and 3-pass encoding jobs are now up to 12% faster\nHEVC/H.265 2-pass and 3-pass encoding jobs are now up to 12% faster\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.31.0\nReleased 2019-10-24\nAdded\nSupport for\nDolby Vision\nCan be encoded from Dolby Vision MXF mezzanine\nPackaging to MPEG-DASH\nsegmented fMP4 output\nin clear and with DRM\nPackaging to HLS\nbyte-range fMP4 output\nin clear and with FairPlay\nPackaging to\nProgressive MP4\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.30.0\nReleased 2019-10-22\nAdded\nAdded possibility to add (black) padding sequences (\nconcatenation -> paddingBefore / paddingAfter\n) between input streams inside a\nconcatenation input stream\nRestructured distribution and scheduling components for efficiency which will enable faster encoding turnaround speeds in the following releases.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.29.1\nReleased 2019-10-08\nFixed\nIncreased read timeout for\nAkamai NetStorage\nuploads\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.29.0\nReleased 2019-10-08\nAdded\nAdded\nS3 role-based output\nfor segmented muxings.\nAdded support for segmented\nWebVTT\nfor the\nHLS\nmanifest.\nImproved encoding speed for high bitrate files.\nFixed\nFixed one bug that caused stalled encodings that occurred sometimes on very high bitrate files.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nS3 role-based output for segmented muxings: No upload verification available\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.28.0\nReleased 2019-09-25\nAdded\nAdded\nConform filter\nwhich allows for small changes of the video frame rate where the playback speed of video as well as audio will be adapted instead of frame interpolation.\nFixed\nMany consecutive timestamp discontinuity events in a live stream do not result in audio/video sync issues anymore\nInsertable content\nnow works for AC-3 audio streams in the input\nAdding keyframes with encodingMode\nTHREE_PASS\ndoes not result in an error anymore\nHEVC video streams can now be muxed to segmented TS\nHEVC coding delay will now be correctly signaled in fMP4 muxings\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.27.0\nReleased 2019-09-10\nAdded\nAdded dynamic scaling of\nwatermarks\nwith new properties\nwidth\nand\nheight\nAdded option to exclude region for\nWebVTT\nconversions (\nignoreRegion\n)\nAdded option to auto detect interlaced content when using\ndeinterlace filter\n(\nautoEnable\n)\nAdd\ncutoffFrequency\nfor\nAAC\n,\nAC3\n,\nEAC3\nFixed\nIf more then one\ninsertable content\nare scheduled consecutively, this is not leading to audio/video sync issues anymore.\nImproved stability for long running live streams.\nAkamai MSL4 with CMAF\noutput works as expected again\nFixed encoding to E-AC3 for cases where the last segment would be very short\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nAdding\nkeyframes\nwith encodingMode\nTHREE_PASS\ndoes result in an error\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.26.1\nReleased 2019-09-04\nFixed\nImproved efficiency during encoding setup when using Per-Title encodings\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nIf more then one insertable content are scheduled consecutively, this could lead to audio/video sync issues when playing the insertable content. The normal live feed is not affected.\nSupport for Akamai MSL CMAF output is broken\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.26.0\nReleased 2019-08-27\nAdded\nFallback from multipart to regular S3 upload when using s3 servers that do not support multipart uploads\nFurther J2k decoding improvements with another speedup of about 50%\nImproved startup stability when using Zixi live input\nFixed\nFixed rounding error in frame rate change logic. Now exactly every second frame will be dropped when down converting to half of the origianl frame rate.\nHE-AAC will hit the correct target bitrate now for progressive outputs as well\nFixed audio/video sync issues on HE-AAC progressive outputs\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nIf more then one insertable content are scheduled consecutively, this could lead to audio/video sync issues when playing the insertable content. The normal live feed is not affected.\nSupport for Akamai MSL CMAF output is broken\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.25.1\nReleased 2019-08-23\nFixed\nFixed copying of some codec configuration settings from Per-Title templates to Per-Title results\nFixed HLS manifest update issue for live encodings\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nIf more then one insertable content are scheduled consecutively, this could lead to audio/video sync issues when playing the insertable content. The normal live feed is not affected.\nSupport for Akamai MSL CMAF output is broken\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.25.0\nReleased 2019-08-13\nAdded\nAdded\nLive Encoding Events\nto provide feedback in case an error occurs during scheduling of insertable content\nDoubled the decoding speed for J2K inputs\nLive encodings are now more resilient to upload failures to cloud storage for audio segments\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nIf more then one insertable content are scheduled consecutively, this could lead to audio/video sync issues when playing the insertable content. The normal live feed is not affected.\nSupport for Akamai MSL CMAF output is broken\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.24.1\nReleased 2019-08-02\nFixed\nRevert to previous J2K decoder because of stability issues with certain input files\nFixed increased turnaround times for BroadcastTS muxings\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nIf more then one insertable content are scheduled consecutively, this could lead to audio/video sync issues when playing the insertable content. The normal live feed is not affected.\nWhen scheduling insertable content and there are errors with the content to insert, there is currently only minimal error feedback.\nSupport for Akamai MSL CMAF output is broken\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.24.0\nReleased 2019-07-30\nAdded\nAdded progress information for 2pass and 3pass analysis\nAdded\ninsertable content\nfor live streams to inject VoD files into the stream at\nscheduled times\nAdded the capability to add multiple CENC DRMs (FairPlay, Widevine, PlayReady) to HLS playlists\nFixed\nStability improvements for 3pass encoding, where an invalid internal state of the encoder led to a crash\nUpload of segmented content to\nGeneric S3\nwas failing in certain cases, e.g., with Scality\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3 or Akamai NetStorage output\nBroadcast TS muxing workflows suffer from increased turnaround times during the muxing stage\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\nIf more then one insertable content are scheduled consecutively, this could lead to audio/video sync issues when playing the insertable content. The normal live feed is not affected.\nWhen scheduling insertable content and there are errors with the content to insert, there is currently only minimal error feedback.\nSupport for Akamai MSL CMAF output is broken\nMinimal chance of producing SAMPLE-AES encrypted TS segments with non-monotonically increasing timestamps for encodings with at least 2 TS muxings, causing the segment to be silent in Safari browser.\n2.23.1\nReleased 2019-07-23\nFixed\nFixed an issue that caused encodings to fail during the muxing step if a Progressive MP4 muxing without output in combination with DRM was configured\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nBroadcast TS muxing workflows suffer from increased turnaround times during the muxing stage\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling\n2.23.0\nReleased 2019-07-16\nAdded\nAdded the\nLive Media and Metadata Ingest Protocol\nas\nadditional output option\nfor live encodings. This is a compatible ingest format for\nUnified Streaming\nDefault manifests can now be used during the\nstart of a Per-Title encoding\nImproved turnaround times for encoding jobs with JPEG-2000 inputs by up to 20%\nImproved transfer speeds to/from Google Cloud Storage (GCS)\nProvide better feedback for thumbnail generation errors\nAdded support for\nBroadcast TS\nand\nTS Segment\nmuxings for Per-Title workflows\nImproved speed of Per-Title analysis\nProvides better feedback for incorrect trimming configurations\nFixed\nFixed handling of encrypted content sent to\nZixi ingest\nFixed failing Per-Title encodes using PlayReady DRM encryption\nFiller data are now retained in\nProgressive MP4 muxings\nwhen setting\nnalHrd=CBR\nin the\nH.264/AVC codec settings\nImproved stability of 3-pass encodings\nAdded missing audio stream language metadata to TS container (HLS) when muxed together with video\nLive encodings are now more resilient to upload failures to cloud storage\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nBroadcast TS and Progressive MP4 muxing workflows suffer from increased turnaround times during the muxing stage\n2.22.0\nReleased 2019-07-02\nAdded\nCEA-\n608\n/\n708\nto\nSidecar\nWebVTT\nSidecar TTML\nto\nSidecar\nWebVTT\nAdded\nvariableMuxRate\n,\ninitialPresentationTimeStamp\nand\ninitialProgramClockReference\nto\nBroadcast TS muxing\nAdded generation of\nBIF\noutput\nAdded additional properties for\nprogramNumber\n,\npmt\n, and\npcr\nto\nTS muxing\nImproved timeout handling when downloading files from HTTP(s) storage to prevent stalling\nProvides better turnaround times for encoding jobs with several output renditions now also for non 4k content\nFixed\n4K input content might stall during the encoding phase when the duration of the input is less than 15 seconds\nWhen setting a different FPS for the output than the FPS of the input stream plus additionally configuring\nminKeyframeInterval\nand/or\nmaxKeyframeInterval\n, this let to incorrect GOP sizes before.\nDeinterlacing DVCProHD content sometimes let to bad results as internally the field parity of the source was not correctly detected.\nMuxing of AC-3 audio streams now work correctly for\nMP4 muxing\nwith a\nfragmentDuration\nset.\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n2.21.1\nReleased 2019-06-18\nFixed\nMuxing of an E-AC-3 stream to fMP4 was failing in case the last segment was very short\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n4K input content might stall during the encoding phase when the duration of the input is less than 15 seconds\n2.21.0\nReleased 2019-06-18\nAdded\nAdded parameter\ninterval\nto the\nThumbnails endpoint\nto create thumbnails every x seconds\nProvides better turnaround times for encoding jobs, especially with 4k content and several output renditions\nEnable muxing of E-AC-3 audio streams with\nBroadcastTS\nFixed\nImproved writing of audio backup for SRT and Zixi live ingests after a disconnect.\nSeveral stability improvements for 4k encoding jobs\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n4K input content might stall during the encoding phase when the duration of the input is less than 15 seconds\n2.20.1\nReleased 2019-06-04\nFixed\nThumbnails and Sprites might have failed when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n2.20.0\nReleased 2019-06-04\nAdded\nAdded support for HE-AAC audio codec with FairPlay encryption and fMP4 muxing\nAdded additional codec settings to\nVP9\n:\nerrorResiliencyEnabled\n,\nclientBufferSize\n,\nclientInitialBufferSize\n,\nbiasPct\n,\ncpuUsed\n,\nautomaticAltRefFramesEnabled\n,\ntargetLevel\n,\nrowMultiThreadingEnabled\nImproved speed up for live encoding restart\nImproved Broadcast TS muxing error detection\nAdded Akamai MSL Qualification for CMAF Dash\nIncreased stability for multiple input files feature\nFixed\nPreview manifests were not created when input was missing audio\nPSNR data for a stream was occasionally not created\nLive Encodings might still occur as running from the API status call, although they have been stopped and shut down\nFixed an error with frame rate detection of variable FPS WebM input files\nFixed a decoding error with PCM DVD audio codec\nFixed a stalling issue with Akamai MSL upload\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.19.0\nReleased 2019-05-22\nAdded\nRedundant SRT\nImproved error messages of trimming offset is bigger than the source duration for Picture Timing Trimmings\nEBU-R128 Audio Normalization\nSupport for AC3/E-AC3 with Progressive MP4\nOpacity option for watermarks\nis now available\nSupport for audio only live streams\nAkamai MSL4 with support for CMAF and HLS\nFixed\nProgressive muxings with a space in the file name will not fail anymore\nBroadcast TS muxings with audio only will not fail anymore\nPer-Title encoding with many fixed resolutions was exceeding\nmaxBitrate\nsetting.\nCorrectly copy primaries, transfer characteristics and colorspace for files where they are signaled in the container format only, but not in the bitstream.\nFiller data are retained when setting\nnalHrd=CBR\nin the codec settings.\nMP4 muxings had PPS signaled on every keyframe which is not required and was leading to problems for some JIT packaging solutions.\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.18.5\nReleased 2019-05-10\nFixed\nSetting long values for HEVC codec parameter\nmasterDisplay\nwas leading to failed encodings\nEnhanced graceful handling of decoding errors for ProRes video codec\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.18.4\nReleased 2019-05-03\nFixed\nDifferent segment lengths or fragment durations configured on muxings of one encoding job will be handled correctly and not lead to the segment length of the output to be signaled incorrectly. - The GOP lenght of a video stream will be set to the greatest common divisor of all configured segment and fragment lengths.\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.18.3\nReleased 2019-05-01\nFixed\nFixed Akamai NetStorage to not produce incomplete segments if maximum parallel connections are exceeded\nUpload failures for Akamai NetStorage will be propagated via the API correctly and let the encoding job fail\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nIf different segment lengths or fragment durations are configured on muxings of one encoding job, the segment length of the output can be signaled incorrectly which leads to invalid DASH/HLS manifests\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.18.2\nReleased 2019-04-30\nFixed\nEnhanced upload resiliency to Akamai NetStorage by improving the retry mechanisms on upload failures that were caused by a huge amount of concurrent uploads.\nSegment duration of audio streams was incorrect (too long or too short) if both sample rates 44.1 kHz and 48 kHz where mixed in one single encode job.\nSpeedup of Per-Title analysis by up to 40%\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nUpload to Akamai NetStorage could produce incomplete segments if maximum parallel connections are exceeded without the encoding job failing.\nIf different segment lengths or fragment durations are configured on muxings of one encoding job, the segment length of the output can be signaled incorrectly which leads to invalid DASH/HLS manifests\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.18.1\nReleased 2019-04-25\nFixed\nfMP4 muxings with applied DRM AC3 and EAC3 streams did not get encrypted\nWhen running many encodings in parallel for a specific region, the encoding cluster might get too less worker nodes, which slows down the encoding process\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.18.0\nReleased 2019-04-24\nAdded\nAdded detailed statistics for SRT ingest protocol\nAdded the flag\nwriteDurationPerSample\nfor fMP4 muxing to enable/disable writing a duration per sample. This is required for some legacy players.\nAdded support for Akamai MSL4 DASH output\nAdded ability to upload segmented WebVTT sidecar file\nFixed\nRate-control enhancement for 3-pass encoding in VP9 for improved quality distribution and to avoid drops in quality\nFixed Per-Title analysis errors for files where we could not extract correct timing information.\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.17.2\nReleased 2019-04-12\nFixed\nFor HEVC resolutions where width or height was not divisible by 8, the wrong width or height was written to the container format. This let to visual artifacts on the frame edges on some players.\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.17.1\nReleased 2019-04-11\nFixed\nWhen using\nsegmentNamingTemplate\nfor Live encodings, the resolved\nsegmentNaming\nwill be patched, so that it can be retrieved via API. This enables Live to VoD workflow for that use case. The patching also applies for\ninitSegmentNameTemplate\nDash Manifests always had a value of 2 in\nAudioChannelConfiguration\n, also if the channel had a higher count (e.g. 6 for 5.1)\nAllow for huge configurations with many streams, muxings and DRM configurations that failed before\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.17.0\nReleased 2019-04-10\nAdded\nObject detection for encodings, that uses our new machine learning technology\nCMAF muxing for Live and VoD workflows with the ability to configure CMAF chunks\nEnabled the combination HLS + fMP4 + Widevine for VoD workflows\nAttribute\navailabilityStartTime\nof DASH manifests has millisecond precision now\nImproved thumbnail and sprite generation as this is done now in parallel to the encoding and won't slow down the end to end encoding time anymore\nFixed\nRe-enabled the combination fMP4 + Fairplay + H264\nEncoding does not fail anymore, when thumbnail configuration is out-of-bounds\nImproved decoding of DVCProHD source files to avoid video corruption at the bottom of the frame\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nThumbnails and Sprites may fail when not using GCS, S3 or Akamai NetStorage output and the thumbnail position is out-of-bound\n2.16.1\nReleased 2019-03-28\nFixed\nRemoved the occurence of blocking artifacts for dark scenes using HEVC 3pass introduced by using low bitrates\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n2.16.0\nReleased 2019-03-26\nAdded\nAuto detect audio sample rate for live inputs\nAdded option to specify the character encoding for Burn-In subtitles\nSupport concatenation for input files with differing audio channel layout\nFixed\nEncoding jobs with multiple inputs and GOP sizes longer than the configured segment length where failing in previous versions.\nSome decoding errors where not correctly propagated to the API.\nFrame rate signaling in a DASH manifest was incorrect if FPS was not configured for a live stream.\nMPEG-DASH SegmentTimeline will now playback without errors on Roku devices\nVery short segments where given too much bitrate for 3pass encodings\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n2.15.0\nReleased 2019-03-19\nAdded\nAdded listener mode for SRT\nAdded\n{rand_chars}\nplaceholder for\nsegmentNamingTemplate\nand\ninitSegmentNameTemplate\nto generate a random character sequence. This is especially useful when restarting a live stream and having storage / CDN caching enabled.\nBetter handling of disconnects and reconnects for SRT streams\nFixed\nFixed an issue where the HLS manifest might get stalled during a live streaming when using\nCLOSED_CAPTION\nin the media resource along with\nAUDIO\nresource\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n2.14.1\nReleased 2019-03-15\nAdded\nFixed distortion of applied watermarks in case the source content contains anamorphic pixels\nFixed Per-Title algorithm to work with multiple bitrate constraints for same resolution\nFixed Per-Title algorithm where the analysis would fail for large step sizes\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n2.14.0\nReleased 2019-03-12\nAdded\nAdded support for SRT Live Input\nAuto detect Frame Rate for live input\nImproved VP9 turn-around times by up to 3x\nVP9 rate-control improvements\nFixed VP9 with 3pass to hit the target bitrate\nFixed HEVC rate control to avoid quality drops after scenes following black frames\nBe more robust to incorrect timestamps of audio and video input files\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\n2.13.0\nReleased 2019-02-28\nAdded\nSupport for UTC Timing and Accessability element in Live DASH manifests\nAdded EXT-X-PROGRAM-DATE-TIME for HLS Live playlists\nAdded support for HLS manifests with fMP4 muxing and CENC FairPlay DRM\nSupport of international characters for Burn-in subtitles\nFixed\nFixed an issue where the combination of more than one Audio Mix filter might result in an unexpected audio output\nFixed an issue where the thumbnail creation failed when a whitespace was in the input file\nFixed an issue where the combination of fMP4 muxing and HLSv3 TS muxing causes an invalid DASH manifest\nImproved 3pass HEVC quality after a scene change in combination with Per-Title. In some cases blocking artifacts where visible for a few frames.\nFixed an issue where too many warning logs where recorded for JPEG 2000 encoded source assets. This was resulting in a failed encoding.\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.12.3\nReleased 2019-02-21\nAdded\nEnabled Live-to-VoD for Zixi ingest streams\nFixed\nFixed a bug where encodings could fail when trimming is applied in combination with long GOP sizes of the input file\nFixed decoding of JPEG2000 assets where our encoder detected a wrong source FPS\nImproved turnaround time of 3pass encodes when doing lots of encodes in parallel\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.12.2\nReleased 2019-02-15\nFixed\nFixed an issue where an upload to Akamai NetStorage failed. The initialization for calculating the MD5 hash crashed when uploading a large amount of files concurrently.\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.12.1\nReleased 2019-02-13\nAdded\nAdded native output support for Akamai NetStorage\nAdded new HEVC codec settings:\nwavefrontParallelProcessing\n,\nparallelModeDecision\n,\nparallelMotionEstimation\n,\nslices\n,\ncopyPicture\n,\nlevelHighTier\n,\nskipSplitRateDistortionAnalysis\n,\ncodingUnitLossless\n,\ntransformSkip\n,\nrefineRateDistortionCost\n,\nlimitTransferUnitDepthRecursion\n,\nnoiseReductionIntra\n,\nnoiseReductionInter\n,\nrateDistortionPenalty\n,\nmaximumTransferUnitSize\n,\ndynamicRateDistortionStrength\n,\nssimRateDistortionOptimization\n,\ntemporalMotionVectorPredictors\n,\nanalyzeSourceFramePixels\n,\nstrongIntraSmoothing\n,\nconstrainedIntraPrediction\n,\nscenecutBias\n,\nallowedRADLBeforeIDR\n,\ngopLookahead\n,\nbframeBias\n,\nforceFlush\n,\nadaptiveQuantizationStrength\n,\nadaptiveQuantizationMotion\n,\nquantizationGroupSize\n,\nstrictCbr\n,\nqpOffsetChromaCb\n,\nqpOffsetChromaCr\n,\nipRatio\n,\npbRatio\n,\nquantizerCurveCompressionFactor\n,\nqpStep\n,\ngrainOptimizedRateControl\n,\nblurQuants\n,\nblurComplexity\n,\nsaoNonDeblock\n,\nlimitSao\n,\nlowpassDct\nAdded I-Frame Playlist for Progressive MP4 and Progressive TS, for both unencrypted and encrypted output\nAdded\nCOMPLEXITY_RANGE\nas new Per-Title bitrate selection mode\nAdded support for AC-3 and HEVC for Broadcast TS muxings\nAdded option to only deinterlace interlaced frames (\nframeSelectionMode\non the deinterlace filter)\nFixed\nFixed an issue with audio tracks in the concatenation workflow\nFixed an issue with 3pass where our encoder generated invalid NAL units\nFixed an issue where visible decoding errors could occur for RTMP ingests\nKnown Issues\nPer-Title encodings only work with GCS, S3 or Akamai NetStorage output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.11.0\nReleased 2019-01-31\nAdded\nRedundant RTMP input\nPer-Title with fixed resolutions and bitrates constraints\nImproved error messages when downloading of subtitle files fail\nImproved error message when authentication with output storage fails when uploading thumbnails or sprites\nAdded support for multiple audio streams in Broadcast TS muxings\nFixed\nImproved download speed from S3 storages with role-based authentication\nReduced delay of writing backup packets for live streams with RTMP ingest\nFixed a/v sync issue for H.264 files that contain open as well as closed GOPs\nFixed an issue where interlaced MP4 files would result in an error during encoding\nFixed a bug that would lead in an encoding error for 2pass/3pass when changing FPS\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nVisible decoding errors could occur for RTMP ingests\n2.10.5\nReleased 2019-01-20\nAdded\nAdded addtional HEVC codec parameters:\nminCodingUnitSize\n,\nlookaheadSlices\n,\nlimitReferences\n,\nrectangularMotionPartitionsAnalysis\n,\nasymetricMotionPartitionsAnalysis\n,\nlimitModes\n,\nmaxMerge\n,\nearlySkip\n,\nrecursionSkip\n,\nfastSearchForAngularIntraPredictions\n,\nevaluationOfIntraModesInBSlices\n,\nsignHide\n,\nrateDistortionLevelForModeDecision\n,\nrateDistortionLevelForQuantization\n,\nqpMin\n,\nqpMax\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.10.4\nReleased 2019-01-18\nAdded\nImproved accuracy of first pass to improve quality in difficult scenes for 2pass and 3pass modes for HEVC\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.10.3\nReleased 2019-01-18\nFixed\nFixed an issue where a live encoding with RTMP ingest will only output audio segments, but no video segments\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.10.2\nReleased 2019-01-16\nAdded\nImproved accuracy of first pass to improve quality in difficult scenes for 2pass and 3pass modes\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.10.0\nReleased 2019-01-15\nAdded\nAdded support for trimmings with drop frame timecodes (contain a\n;\n)\nAdded support for DRM only output for HLS MP4 byte-range muxings\nAdded support for correct HDR10 signaling in HLS manifest\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.9.2\nReleased 2019-01-29\nFixed\nFixed a concurrency issue where the internal error rate increased when the encoding load increased heavily in a short amount of time\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.9.1\nReleased 2019-01-08\nFixed\nFixed an issue where a network issue prevented a Three-Pass encoding to succeed\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.9.0\nReleased 2019-01-07\nAdded\nAdded HEVC\ncutree\nsetting\nFixed\nFixed an issue where CENC configuration was not updated correctly for Per-Title stream results when using VP9\nFixed an issue where a segment duration of 4 seconds was always used when using progressive TS with audio only\nFixed an issue where the TimeCode track ID of MP4 muxings was not unique\nFixed an issue where the MP4 HLS byte-range configuration was not correctly applied for Per-Title stream results\nFixed an issue where FairPlay DRM was not updated correctly for Per-Title stream results when using progressive TS\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.8.0\nReleased 2018-12-24\nAdded\nAdded VP9 + Widevine\nAdded option to set fixed resolutions for Per-Title encodings\nImproved sanity checks of\nminBitrate\nand\nmaxBitrate\nfor Per-Title encodings\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.7.0\nReleased 2018-12-16\nAdded\nImproved quality for HEVC with low bitrates\nImproved quality for 2-pass and 3-pass encodings by up to 3 dB in PSNR\nAdded additional settings for HEVC:\nadaptiveQuantizationMode\n,\npsyRateDistortionOptimization\n,\npsyRateDistortionOptimizedQuantization\nand\nenableHrdSignaling\nAdded\ncomplexityFactor\nto Per-Title configuration that allows to modify the assumed complexity of a title.\nBetter reach the configured\nmaxBitrate\nfor H264 Per-Title\nFixed\nFixed an issue where Per-Title encoding might not work when using a multi-tenant account\nFixed an issue where a low\ntargetQualityCrf\nvalue would cause the highest resolution to be ignored\nFixed an issue where the protection header was not placed in the smooth manifest\nFixed an issue where encodings might stall when using 3-pass\nFixed an issue where trimming with H264 Picture Timing might lead to audio loss\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.6.1\nReleased 2018-12-11\nAdded\nAdded option to set internal chunk length for progressive muxings\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nHEVC may encounter decreased quality when using low bitrates\nEncodings might stall when using 3-pass encoding\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.6.0\nReleased 2018-12-10\nAdded\nConcatenate multiple input files\nAutomatically restart live streams based on timeouts for segments, bytes, frames, manifests and with a schedule expression\nSupport for HLS input for VoD encodings from HTTP(s) locations\nOption to set an interval for reuploading static DASH manifests and init files for live streams\nAdded an option to disable remote verification for FTP uploads\nAdded an option to set a custom internal chunk length to improve quality for progressive muxings\nImproved visual quality for H264 2pass and 3pass modes\nFixed\nFixed an issue where AC3 was not supported for progressive TS muxings\nFixed an issue where an audio only progressive TS muxing caused an error\nFixed an issue where the wrong KID was placed in the DASH manifest for PlayReady when multiple DRM configurations for a single stream have been used\nFixed an issue where the Smooth manifest always had a channel count of 2 regardless of the channel count of the audio stream\nFixed an issue where the timecode for MP4 muxings could not be set for MP4 files bigger than 2GB\nFixed an issue, where too many errors or wanings in a source file would lead to a failed encoding\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nHEVC may encounter decreased quality when using low bitrates\nEncodings might stall when using 3-pass encoding\nLive encodings with RTMP ingest may only output audio segments, but no video segments (fixed with\n2.10.3\n)\n2.5.1\nReleased 2018-11-27\nFixed\nFixed an issue where Per-Title analysis might fail for some long source files\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\n2.5.0\nReleased 2018-11-25\nAdded\nPer-Title support for HEVC and VP9\nAdded support for FairPlay and HLS with progressive TS files\nFixed\nFixed an issue where IV size and PIFF compatibility for PlayReady where not copied to a Per-Title result stream\nSegment length was not applied correctly for WebM and progressive MP4\nFixed an issue where a 3-pass encoding might stall for a couple of minutes\nFixed an issue where\nframe_field_info_present_flag\nwas incorrectly asserted in SPS VUI information of H.265 outputs\nFixed an issue where the Per-Title encoding may fail when using PCM_DVD source\nFixed an issue where the output mapping of audio might not work as expected when using the same audio input for multiple streams\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nVP9 in combination with 3-pass does not hit the target bitrate, please use 2-pass instead\nPer-Title analysis might fail for some long source files\n2.4.4\nReleased 2019-03-21\nFixed\nFixed a bug where encodings could fail when trimming is applied in combination with long GOP sizes of the input file\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\n2.4.3\nReleased 2019-01-27\nFixed\nFixed a concurrency issue where the internal error rate increased when the encoding load increased heavily in a short amount of time\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\n2.4.2\nReleased 2019-01-22\nFixed\nImproved download speed from S3 storages with role-based authentication\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\n2.4.1\nReleased 2019-01-20\nFixed\nFixed an issue where a 3-pass encoding might stall for a couple of minutes\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\n2.4.0\nReleased 2018-11-12\nAdded\nBurn-In of SRT subtitles\nAdded additional H264 codec parameters (\ndeblockAlpha\n,\ndeblockBeta\n,\nadaptiveQuantizationMode\n,\nadaptiveQuantizationStrength\n,\nmixedReferences\n,\nadaptiveSpatialTransform\n,\nfastSkipDetectionPFrames\n,\nweightedPredictionBFrames\n,\nweightedPredictionPFrames\n,\nmacroblockTreeRatecontrol\n,\nquantizerCurveCompression\n,\npsyRateDistortionOptimization\n,\npsyTrellis\n)\nImproved handling with audio only MXF files\nSupport open-gop for H264 and H265\nSupport for PCM_DVD Audio coded as input\nAdded size of IV and flag for\npiff-compatible\nto PlayReady DRM\nAdded FairPlay DRM for HLS with MP4 byte range requests\nImproved stability for live\nFixed\nFixed an issue where PAR was not correctly handled when using Per-Title\nFixed an issue where Per-Title encoding does not work with input files that have high bitrates\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\n3pass encodings might suffer from a stall up to 10 minutes\n2.3.0\nReleased 2018-10-29\nAdded\nImplemented support for HLS with MP4 Byte-Range request\nImplemented DASH Segment Timeline for Live\nExplicitly set AWS S3 Signature Version to output\nFixed\nFixed an issue where audio only could not be encoded when the input file was in MXF format\nFixed an issue where the trimming could be off by a couple of frames when using timecode\nRestarts of live encoding is now up to 5x faster\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nPer-Title encoding fails with input files that have a high bitrate\n2.2.0\nReleased 2018-10-22\nAdded\nPer-Title analysis step is now 4x faster\nImproved writing of live HLS manifest to be more resilient\nAdded new applied settings to the stream that are patched with the width and height used for encoding\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\nPer-Title encoding fails with input files that have a high bitrate\n2.1.0\nReleased 2018-10-16\nAdded\nReturn PAR in stream input analysis\nImproved error message when a file on S3 cannot be downloaded\nEnable HLG signaling options for HEVC\nSupport for vbv codec settings for Per-Title\nFixed\nFixed an issue where an encoding might stall at the very end when using a lot of renditions\nFixed IDR placement for HEVC\nKnown Issues\nPer-Title encodings only work with GCS or S3 output\n2.0.0\nReleased 2018-10-09\nAdded\nPer Title Encoding\nAV1 speed improvements by 40%. That means it's twice as fast as the reference implementation, 20 times slower than HEVC/VP9 and 40 times slower than H.264\nEnhanced 2-pass and 3-pass encoding\nSupport de-multiplexed A/V input files\nSupport 2-Pass for live encoding\nAdded Keyframe settings to VP8/VP9\nAdded muxing information for Broadcast TS\nChanged\nBreaking Change: When adding a stream, the property\ndecodingErrorMode\nis set to\nDUPLICATE_FRAMES\nper default\nBreaking Change: When starting an encoding, the property\nencodingMode\nis set to\nTWO_PASS\nper default\nBreaking Change: When starting an encoding, the property\ntrimming -> ignoreDurationIfInputTooShort\nis set to\ntrue\nper default\nBreaking Change: When starting an encoding, the property\nhandleVariableInputFps\nis set to\ntrue\nper default\nBreaking Change: When starting an encoding, the property\ntweaks -> audioVideoSyncMode\nis set to\nRESYNC_AT_START\nper default\nFixed\nFixed an upload bug that may occur when uploading Progressive MP4 files to Azure\nFixed an issue where the encoding may fail when using a small trimming duration\nKnown Issues\nPer-Title encodings only work with GCS or S3 output",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/go-sdk",
    "title": "Go SDK",
    "text": "The Bitmovin API enables you to build cutting edge video workflows while leveraging emmy-award winning encoding solutions, that automatically optimize your catalog for high quality and cost efficient video streaming.\nQuick Start Guide\nThis Quick Start Video will demonstrate different approaches to setup an encoding workflow using a\nWatch Folder Workflow (No Code Required)\nand a\nfull Python API SDK example\nusing this\nColabortory Notebook\n.\nundefined\nGet Started with your own Project\nThe following steps will walk you through adding the SDK to an new or existing Project step by step.\nOverview\nStarting an encoding triggers a request for encoding instances. Once available, the input file will be downloaded, divided into chunks, and distributed between all of them. Those instances transcode each chunk into segments for all configured video/audio representations (muxings) in parallel and write them to the designated output destination.\nThis approach makes Bitmovin's encoding the fastest in the world.\nIn this tutorial you will implement a simple VOD online streaming scenario, where an input file containing a video stream and stereo audio stream is transcoded into ...\na fixed ladder of 3 video representations at different resolutions and bitrates using the H264 codec, and\nan audio rendition with the AAC codec.\nEach individual stream is wrapped into fragmented MP4 (fMP4) containers. HLS and DASH manifests are generated so the encoded content is stream- and playable on the majority of modern devices and browsers out there today.\nStep 1: Add Bitmovin SDK to Your Project\nTo get started add the Bitmovin SDK to your project.\nFor the latest version please refer to the\nGitHub repository\nor check the\nchangelog\nof the Bitmovin API. Open API SDK versions and the Bitmovin API share the same versioning.\nbash\ngo get github.com/bitmovin/bitmovin-api-sdk-go\nStep 2: Setup a Bitmovin API Client Instance\nThe Bitmovin API client instance is used for all communication with the Bitmovin REST API, pass your API key to authenticate with the API. You can find your\nAPI_KEY\nin the dashboard in your\nAccount Settings\n.\nGo\nbitmovinApi, err := bitmovin.NewBitmovinAPI(apiclient.WithAPIKey(\"<API_KEY>\"))\nStep 3: Create an Input\nCreate Input\nAn input holds the configuration to access a specific file storage, from which the encoder will download the source file.\nWe support various types of input sources including HTTP(S), (S)FTP, Google Cloud Storage (GCS), Amazon Simple Storage Service (S3), Azure Blob Storage, and all cloud storage vendors that offer an S3 compatible interface.\nBelow we are creating an input. You can change the input type on the top right of the code panel.\nGo\nhost := \"<HTTPS_INPUT_HOST>\"\n  \nhttpsInput := &model.HttpsInput{\n  Host: &host,\n}\n\nhttpsInput, err = bitmovinApi.Encoding.Inputs.Https.Create(*httpsInput)\nReuse Inputs\nInputs refer to a location, not a specific file.\nThis makes them easily reusable. Create an input once and reuse it for all your encodings.\nHint: All resources in our API are identified by a unique id which enables you to reuse many resources. So you only have to create one input for each location and can reuse it for all encodings.\nGo\nhttpsInput, err := bitmovinApi.Encoding.Inputs.Https.Get(\"<INPUT_ID>\");\nStep 4: Create an Output\nCreate Output\nOutputs define where the manifests and encoded segments will be written to.\nLike inputs, outputs represent a set of information needed to access a specific storage. From this storage players can access the content for playback.\nGo\naccessKey := \"<GCS_ACCESS_KEY>\"\nsecretKey := \"<GCS_SECRET_KEY>\"\nbucketName := \"<GCS_BUCKET_NAME>\"\n\ngcsOutput := &model.GcsOutput{\n  AccessKey: &accessKey,\n  SecretKey: &secretKey,\n  BucketName: &bucketName,\n}\n\ngcsOutput, err = bitmovinApi.Encoding.Outputs.Gcs.Create(*gcsOutput)\nReuse Output\nSimilar to inputs, outputs also refer to a location, not a specific file.\nSo they're exactly as reusable as inputs. Create it once and reuse it for all your encodings.\nGo\ngcsOutput, err := bitmovinApi.Encoding.Outputs.Gcs.Get(\"<OUTPUT_ID>\")\nStep 5: Create Codec Configurations\nCodec Configurations\nCodec configurations specify the codec and its parameters (eg. media codec, bitrate, frame rate, sampling rate).\nThey are used to encode the input stream coming from your input file.\nVideo Codec Configurations\nBelow we are creating video codec configurations. We support a multitude of codecs, including H264, H265, VP9, AV1 and many more.\nFor simplicity, we use H264 now, but you can easily change that after finishing the getting started.\nGo\nname := \"Getting Started H264 Codec Config 1\"\nheight := int32(1024)\nbitrate := int64(1_500_000)\n\nvideoCodecConfiguration1 := &model.H264VideoConfiguration{\n\tName:                &name,\n\tPresetConfiguration: model.PresetConfiguration_VOD_STANDARD,\n\tHeight:              &height,\n\tBitrate:             &bitrate,\n}\n\nvideoCodecConfiguration1, err = bitmovinApi.Encoding.Configurations.Video.H264.Create(*videoCodecConfiguration1)\n\nname = \"Getting Started H264 Codec Config 2\"\nheight = int32(768)\nbitrate = int64(1_000_000)\n\nvideoCodecConfiguration2 := &model.H264VideoConfiguration{\n\tName:                &name,\n\tPresetConfiguration: model.PresetConfiguration_VOD_STANDARD,\n\tHeight:              &height,\n\tBitrate:             &bitrate,\n}\n\nvideoCodecConfiguration2, err = bitmovinApi.Encoding.Configurations.Video.H264.Create(*videoCodecConfiguration2)\n\nname = \"Getting Started H264 Codec Config 2\"\nheight = int32(640)\nbitrate = int64(750_000)\n\nvideoCodecConfiguration3 := &model.H264VideoConfiguration{\n\tName:                &name,\n\tPresetConfiguration: model.PresetConfiguration_VOD_STANDARD,\n\tHeight:              &height,\n\tBitrate:             &bitrate,\n}\n\nvideoCodecConfiguration3, err = bitmovinApi.Encoding.Configurations.Video.H264.Create(*videoCodecConfiguration3)\nAudio Codec Configurations\nAs audio codec configuration we are creating one AAC configuration.\nWe also support many more audio codecs including Opus, Vorbis, AC3, E-AC3 and MP2.\nGo\nname := \"Getting Started Audio Codec Config\"\nbitrate := int64(128_000)\n\naudioCodecConfiguration := &model.AacAudioConfiguration{\n\tName:    &name,\n\tBitrate: &bitrate,\n}\n\naudioCodecConfiguration, err = bitmovinApi.Encoding.Configurations.Audio.Aac.Create(*audioCodecConfiguration)\nReusing Codec Configurations\nAs inputs and outputs, codec configurations can also be reused.\nTo use existing video or audio codec configurations, loading them with their id.\nGo\nvideoCodecConfiguration, err := bitmovinApi.Encoding.Configurations.Video.H264.Get(\"<H264_CC_ID>\")\n\naudioCodecConfiguration, err := bitmovinApi.Encoding.Configurations.Audio.Aac.Get(\"<AAC_CC_ID>\")\nStep 6: Create & Configure an Encoding\nCreate Encoding\nAn encoding is a collection of resources (inputs, outputs, codec configurations etc.), mapped to each other.\nThe cloud region defines in which cloud and region your encoding will be started.\nIf you don't set it, our encoder takes the region closest to your input and output.\nInputs, outputs and codec configurations can be reused among many encodings. Other resources are specific to one encoding, like streams, which we'll create below.\nGo\nname := \"Getting Started Encoding\"\nencoding := &model.Encoding{\n  Name: &name,\n  CloudRegion: model.CloudRegion_GOOGLE_EUROPE_WEST_1,\n}\n\nencoding, err = bitmovinApi.Encoding.Encodings.Create(*encoding)\nStreams\nA stream maps an audio or video input stream of your input file (called input stream) to one audio or video codec configuration.\nThe following example uses the selection mode\nAUTO\n, which will select the first available video input stream of your input file.\nYou can choose an input file from 3 predefined examples on the top right of the code panel.\nVideo Stream\nGo\ninputPath := \"<INPUT_PATH>\"\n\nvideoStreamInput := model.StreamInput{\n\tInputId:       httpsInput.Id,\n\tInputPath:     &inputPath,\n\tSelectionMode: model.StreamSelectionMode_AUTO,\n}\n\nvideoStream1 := &model.Stream{\n\tInputStreams:  []model.StreamInput{videoStreamInput},\n  CodecConfigId: videoCodecConfiguration1.Id,\n}\nvideoStream1, err = bitmovinApi.Encoding.Encodings.Streams.Create(*encoding.Id, *videoStream1)\n\nvideoStream2 := &model.Stream{\n\tInputStreams:  []model.StreamInput{videoStreamInput},\n  CodecConfigId: videoCodecConfiguration2.Id,\n}\nvideoStream2, err = bitmovinApi.Encoding.Encodings.Streams.Create(*encoding.Id, *videoStream2)\n\nvideoStream3 := &model.Stream{\n\tInputStreams:  []model.StreamInput{videoStreamInput},\n  CodecConfigId: videoCodecConfiguration3.Id,\n}\nvideoStream3, err = bitmovinApi.Encoding.Encodings.Streams.Create(*encoding.Id, *videoStream3)\nAudio Stream\nGo\ninputPath := \"<INPUT_PATH>\";\n\naudioStreamInput := model.StreamInput{\n\tInputId:       httpsInput.Id,\n\tInputPath:     &inputPath,\n\tSelectionMode: model.StreamSelectionMode_AUTO,\n}\n\naudioStream := &model.Stream{\n\tInputStreams:  []model.StreamInput{audioStreamInput},\n  CodecConfigId: audioCodecConfiguration.Id,\n}\naudioStream, err = bitmovinApi.Encoding.Encodings.Streams.Create(*encoding.Id, *audioStream)\nStep 7: Create a Muxing\nMuxings\nA muxing defines which container format will be used for the encoded video or audio files (segmented TS, progressive TS, MP4, FMP4, ...). It requires a stream, an output, and the output path, where the generated segments will be written to.\nFMP4 muxings are used for DASH manifest representations, TS muxings for HLS playlist representations.\nFMP4\nGo\noutputPath := \"<OUTPUT_PATH>\"\nsegmentLength := float64(4.0)\nsegmentNaming := \"seg_%number%.m4s\"\ninitSegmentName := \"init.mp4\"\n\naclEntry := model.AclEntry{\n\tPermission: model.AclPermission_PUBLIC_READ,\n}\n\nfullOutputPath := fmt.Sprintf(\"%s/video/1024_1500000/fmp4/\", outputPath)\nvideoMuxingOutput1 := model.EncodingOutput{\n\tOutputId:   gcsOutput.Id,\n\tOutputPath: &fullOutputPath,\n\tAcl:        []model.AclEntry{aclEntry},\n}\nmuxingStream1 := model.MuxingStream{\n\t\tStreamId: videoStream1.Id,\n}\n\nvideoMuxing1 := &model.Fmp4Muxing{\n  SegmentLength:   &segmentLength,\n  InitSegmentName: &initSegmentName,\n  SegmentNaming:   &segmentNaming,\n\tStreams:         []model.MuxingStream{muxingStream1},\n  Outputs:         []model.EncodingOutput{videoMuxingOutput1},\n}\nvideoMuxing1, err = bitmovinApi.Encoding.Encodings.Muxings.Fmp4.Create(*encoding.Id, *videoMuxing1)\n\nfullOutputPath = fmt.Sprintf(\"%s/video/768_1000000/fmp4/\", outputPath)\nvideoMuxingOutput2 := model.EncodingOutput{\n\tOutputId:   gcsOutput.Id,\n\tOutputPath: &fullOutputPath,\n\tAcl:        []model.AclEntry{aclEntry},\n}\nmuxingStream2 := model.MuxingStream{\n\t\tStreamId: videoStream2.Id,\n}\n\nvideoMuxing2 := &model.Fmp4Muxing{\n  SegmentLength:   &segmentLength,\n  InitSegmentName: &initSegmentName,\n  SegmentNaming:   &segmentNaming,\n\tStreams:         []model.MuxingStream{muxingStream2},\n  Outputs:         []model.EncodingOutput{videoMuxingOutput2},\n}\nvideoMuxing2, err = bitmovinApi.Encoding.Encodings.Muxings.Fmp4.Create(*encoding.Id, *videoMuxing2)\n\nfullOutputPath = fmt.Sprintf(\"%s/video/640_750000/fmp4/\", outputPath)\nvideoMuxingOutput3 := model.EncodingOutput{\n\tOutputId:   gcsOutput.Id,\n\tOutputPath: &fullOutputPath,\n\tAcl:        []model.AclEntry{aclEntry},\n}\nmuxingStream3 := model.MuxingStream{\n\t\tStreamId: videoStream3.Id,\n}\n\nvideoMuxing3 := &model.Fmp4Muxing{\n  SegmentLength:   &segmentLength,\n  InitSegmentName: &initSegmentName,\n  SegmentNaming:   &segmentNaming,\n\tStreams:         []model.MuxingStream{muxingStream3},\n  Outputs:         []model.EncodingOutput{videoMuxingOutput3},\n}\nvideoMuxing3, err = bitmovinApi.Encoding.Encodings.Muxings.Fmp4.Create(*encoding.Id, *videoMuxing3)\nAudio Muxings\nGo\nfullOutputPath := fmt.Sprintf(\"%s/audio/128000/fmp4/\", outputPath)\naudioMuxingOutput := model.EncodingOutput{\n\tOutputId:   gcsOutput.Id,\n\tOutputPath: &fullOutputPath,\n\tAcl:        []model.AclEntry{aclEntry},\n}\naudioMuxingStream := model.MuxingStream{\n\t\tStreamId: audioStream.Id,\n}\n\naudioMuxing1 := &model.Fmp4Muxing{\n  SegmentLength:   &segmentLength,\n  InitSegmentName: &initSegmentName,\n  SegmentNaming:   &segmentNaming,\n\tStreams:         []model.MuxingStream{audioMuxingStream},\n  Outputs:         []model.EncodingOutput{audioMuxingOutput},\n}\naudioMuxing1, err = bitmovinApi.Encoding.Encodings.Muxings.Fmp4.Create(*encoding.Id, *audioMuxing1)\nHint: To optimize your content for a specific use-case you can also provide a custom segment length. Further, you can also customize the naming pattern for the resulting segments and the initialization segment as well.\nStep 8: Create a Manifest\nDASH & HLS Manifests\nIts recommended to use Default Manifests, one each for DASH (\nAPI-Reference\n) and HLS (\nAPI-Reference\n). They are a construct that leaves it to the encoder to determine how to best generate the manifest based on what resources have been created by the encoding. This simplifies the code greatly for simple scenarios like this guide is about.\nCreate a DASH Manifest\nGo\nmanifestName := \"stream.mpd\"\noutputPath := \"<OUTPUT_PATH>\"\n\naclScope := \"*\"\naclEntryManifest := model.AclEntry{\n  Scope: &aclScope,\n\tPermission: model.AclPermission_PUBLIC_READ,\n}\n  \nmanifestOutput := model.EncodingOutput{\n\tOutputId:   gcsOutput.Id,\n\tOutputPath: &outputPath,\n\tAcl:        []model.AclEntry{aclEntryManifest},\n}\n\ndashManifest := &model.DashManifestDefault{\n\tManifestName: &manifestName,\n\tEncodingId:   encoding.Id,\n\tVersion:      model.DashManifestDefaultVersion_V2,\n\tOutputs:      []model.EncodingOutput{manifestOutput},\n}\n\ndashManifest, err = bitmovinApi.Encoding.Manifests.Dash.Default.Create(*dashManifest)\nCreate a HLS manifest\nGo\nmanifestName := \"stream.m3u8\"\n\nhlsManifest := &model.HlsManifestDefault{\n\tManifestName: &manifestName,\n\tEncodingId:   encoding.Id,\n\tVersion:      model.HlsManifestDefaultVersion_V1,\n\tOutputs:      []model.EncodingOutput{manifestOutput},\n}\n\nhlsManifest, err = bitmovinApi.Encoding.Manifests.Hls.Default.Create(hlsManifest)\nStep 9: Start an Encoding\nLet's recap - We have defined:\nan\ninput\n, specifying the input file host\nan\noutput\n, specifying where the manifest and the encoded files will be written to\ncodec configurations\n, specifying how the input file will be encoded\nstreams\n, specifying which input stream will be encoded using what codec configuration\nmuxings\n, specifying the containerization of the streams\nmanifests\n, to play the encoded content using the HLS or DASH streaming format.\nNow we can start the encoding with one API call, including additional configuration so the DASH and HLS Default-Manifests are created as part of the encoding process too:\nGo\nstartEncodingRequest := model.StartEncodingRequest{\n\tManifestGenerator: model.ManifestGenerator_V2,\n\tVodDashManifests: []model.ManifestResource{{\n\t\tManifestId: dashManifest.Id,\n\t}},\n\tVodHlsManifests: []model.ManifestResource{{\n\t\tManifestId: hlsManifest.Id,\n\t}},\n}\n\n_, err = bitmovinApi.Encoding.Encodings.StartWithRequestBody(*encoding.Id, startEncodingRequest)\nWhat happens next:\nAfter the successful start of the encoding, it will change its state from\nCREATED\nto\nSTARTED\n, and shortly after that to\nQUEUED\n.\nRUNNING\nit will be in as soon as the input file starts to get downloaded and processed. Eventually\nFINISHED\nwill indicate a successful encoding and upload of the content to the provided Output destination. More details about each encoding status can be\nfound here\n.\nStep 10: Final Review\nCongratulations!\nYou successfully started your first encoding :) and this is only the beginning. The Bitmovin Encoding API is extremely flexible and allows many customizations and provides with access to the latest codecs and video streaming optimizations and technologies out there. Have a look at the\nAPI Reference\nto learn more.\nMore Examples\nYou can find many more fully functional code examples in our Github Repository:\nGithub",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/ecfc7d1-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-a-per-title-encoding",
    "title": "Creating a Per-Title Encoding",
    "text": "Overview\nWhat does Per-title Encoding do? In short: It introduces an additional step during the encoding process, analyzing the video asset. The findings are used to calculate the optimal bitrate ladder for every single asset and thus achieve higher perceptual quality.\nIf you are not yet sure what Per-Title Encoding is, and what it means, please have a look at this\nFAQ article\nfirst before you continue.\nConfiguring a Per Title Encoding\nSo far, when you create an\nEncoding\n, you add a\nStream\nset to their\nSTANDARD\nmode (enabled by default), for every output rendition you want to create out of your input file. Each\nStream\nis linked to an\nCodec Configuration\nthat defines which codec, resolution, bitrate, etc. has to be used for this rendition. Then you create\nMuxing\ns to define which outputs (fMP4 (DASH), MP4, TS,...) have to be created out of theses\nStream\ns.\nWhen you create an\nPer Title Encoding\nthe workflow and configuration is a little bit different as it depends on your requirements or use-case.\nA Simple Setup (Example)\nThe most simple use-case is where you let our encoding service decide everything to achieve the best possible outcome with regards to bandwidth savings and quality improvements.\nInstead of adding multiple\nStream\ns to your\nEncoding\n, you add just one which mode is set to\nPER_TITLE_TEMPLATE\nand has as an plain\nCodec configuration\nlinked to it. This\nStream\n, its Codec Configuration, as well as all Muxings that are using it, will be used as a\ntemplate\nby the Per-Title encoding, to create a\nPer-Title profile\n.\nOur Per-Title Algorithm will then add additional\nStream\ns to this\nper title profile\nbased on the\nPer-Title Template\n, which it considers to be most beneficial for your encoding in terms of bandwidth and quality. These\nStreams\nwill have set their mode to\nPER_TITLE_RESULT\n, as they got added automatically by the Per-Title Algorithm.\nAn\nHTTPS Input\nand\nAWS S3 Output\nwill be used in the following example. The\nfull example\nstates their creation and configuration as well.\n1) Create an Encoding\nAs always the first step is to create an encoding in our API, which holds all the streams and muxings that are used to configure the encoding of your input file. In order to be able to use the Per Title Encoding you have to use the encoder version v1.53.0 or higher.\nJava\nEncoding encoding = new Encoding();\nencoding.setName(\"Java Example - Per-Title\");\nencoding.setCloudRegion(CloudRegion.AUTO);\nencoding = bitmovinApi.encoding.create(encoding);\n2) Add a Stream and Codec Configuration\nAs for regular encodings you do have to provide the Stream with the information about which video stream of your input file has to be used for the encoding.\nJava\nInputStream videoInputStream = new InputStream();\nvideoInputStream.setSelectionMode(StreamSelectionMode.AUTO);\nvideoInputStream.setInputPath(INPUT_PATH);\nvideoInputStream.setInputId(s3Input.getId());\n\nInputStream audioInputStream = new InputStream();\naudioInputStream.setSelectionMode(StreamSelectionMode.AUTO);\naudioInputStream.setInputPath(INPUT_PATH);\naudioInputStream.setInputId(s3Input.getId());\nThe next step is to create a single “Stream” which is configured to use a plain H264 Codec configuration, that is configured with its required options only, which is the “profile” option, and the “mode” of the Stream set to “PER_TITLE_TEMPLATE”. These steps are encapsulated in the \"createAudioStream\" and \"createVideoStream\" method used in the\nfull example\n.\nJava\nStream audioStream = createAudioStream(encoding, audioInputStream);\nStream videoStream = createVideoStream(encoding, videoInputStream);\n3) Add Muxings\nHaving finished the configuration which video stream of the input file shall be used for the per title encoding, the configuration of the desired output formats is needed (fMP4, TS, MP4, …). As done for the Codec configuration, only the required options will be set of the muxing.\nThe example is creating fMP4 content, so its required parameters are:\nSegment length\nStreams to be used by the muxing\nOutputs to be used, and a path where to store the content\nWhen creating content for an adaptive streaming format like MPEG-DASH, you typically store the segments of each rendition in a separate folder, where its name typically states the bitrate and/or resolution as well as an unique identifier to make it clearly identifiable.\nAs you yet don’t know the bitrate or resolution that is going to be used by the Per Title Encoding, you can use the following placeholders to specify your output path:\n{uuid} - e.g. 3f759845-75d7-4df0-91b6-53aef29b91dd\n{bitrate} in bps\n{width} in px\n{height} in px\nThese steps are encapsulated in the \"createMp4Muxing\" method used in the\nfull example\n.\ncreateMp4Muxing(encoding, s3Output, videoStream, audioStream);\n4) Start the Per Title Encoding\nThe configuration of the Per Title Encoding is now complete, therefore its\nstart Encoding\nAPI call and the according\nPer Title Configuration\ncan be prepared. Right now, Per Title Encodings support H.264, HEVC, and VP9 only, however other codecs like AV1 will be supported soon as well.\nWhile the\nPer Title Configuration\nallows you to customize the\nPer Title Algorithm\n, all its settings are optional. However in order start one, the\nPer Title configuration\nas to be present in the “start Encoding” API request.\nThe Per Title Configuration can be added to the\nStartEncodingRequest\nconfiguration. There you can configure the “EncodingMode” which can be set to “STANDARD”, “TWO_PASS” or “THREE_PASS”. In order achieve the best possible quality “THREE_PASS” is used in this example.\nThese steps are encapsulated in the \"startEncoding\" method used in the\nfull example\n.\nstartEncoding(encoding);\nThat’s it! The Per Title Encoding will be started now, the Per Title Algorithm add additional Streams to achieve high quality and bandwidth savings, and transfers them to your Output accordingly.\nPer-Title Configuration Options\nIf the fully automated way doesn't suite you completely, as you need to fullfil certain requirements, e.g. the range available to select a proper bitrate, or how small/big the steps between each representation has to be, you can\ncustomize the per title algorithm configuration\nfor each codec you are using as well.\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-an-s3-role-based-encoding-input-or-output-with-the-bitmovin-api",
    "title": "Creating an S3 Role-Based Encoding Input or Output with the Bitmovin API",
    "text": "Overview\nS3 role-based Inputs\nresp.\nS3 role-based Outputs\nare an alternative way for our services to access your AWS (Amazon Web Services) S3 bucket to be used as an Encoding Input and/or Output, or an Output for Analytics Exports.\nInstead of you providing your AWS Access/Secret key pair to our Encoding or Analytics service, we provide you with an AWS IAM (Identity and Access Management) user name, which you can grant specific access rights in your account so it can access your desired S3 bucket.\nIn this tutorial, we will\ncreate an\nS3 bucket in your AWS account\nwhich will serve as the input or output storage for your encodings.\ncreate an\nS3 role based input or output in your Bitmovin account\nusing the Bitmovin API. This step will give you the\nexternalId\nwhich is used to communicate safely with your IAM role which you create in step 3\ncreate an\nIAM role in your AWS account\n, and attach an IAM policy to it. This policy states which of your buckets can be accessed by our user, and which permissions are granted to it.\nNOTE\n: This tutorial needs to be repeated for EACH account/sub-organization you want a S3-Role-Based access for.\nRequirements\na\nBitmovin account\n. Please make sure that you run this tutorial on the same Bitmovin account (and sub-organization, if applicable) that you want to use for production. If you don't have or know your production account yet, please be aware to repeat this tutorial later on your production account.\nan\nAWS account\nto create the input and/or output bucket and the Role that allows the Bitmovin system to access that bucket.\nS3 role-based buckets can be used for segmented outputs with\nencoder version\n2.29.0\nor higher.\nCreate an AWS S3 Bucket\nIn the AWS Management Console, open the\nS3 section\n.\nClick on the\nCreate Bucket\nbutton which starts the bucket creation wizard\nIn the \"Name and Region\" panel, choose a bucket name (for example\nmy-bitmovin-bucket\n) and a Region (for example\n(EU) Ireland)\n), then press\nNext\nConfigure the\nObject Ownership\ndepending on your needs a.\nACLs enabled\n, this is\nrequired\nif PUBLIC_ACCESS is configured for Encoding outputs. Encodings that are started in our\nBitmovin Dashboard\nset the PUBLIC_ACCESS to enable preview playback.\nb. ACLs disabled. If this policy is used Encoding outputs need to be set to have the PRIVATE ACL.\nConfigure\nBlock Public Access settings for this bucket\na. The default settings will\nBlock\nall\npublic access\nb. To\nenable playback\nfor manifests and files from the bucket,\nuncheck\n🔳\nBlock\nall\npublic access.\nFinish going through the wizard and click\nCreate Bucket\nTo allow players to request content for streaming from your S3 bucket, you will also need to allow origin access with a CORS configuration. See\nHow can I configure an AWS S3 Bucket to test playback of my content?\non how to configure this for your bucket.\nYour bucket is now ready to be used.\nCreate a Bitmovin S3 role-based Input/Output\nBefore you actually create the Role in your AWS account, you need to create an S3 Role based Output (resp. Input) with the Bitmovin API.\nThe minimal required information to create a Role based S3 input or output are the following :\nthe name of your S3 bucket that you created in the previous step\nYour AWS account number\nThe name you intend to give the Role in your AWS account (e.g.\nbitmovinCustomerS3Access\n).\nNow, you have the following variables:\nbucketName\n: the name of your bucket as in 1 above\nroleArn\n: from 2 and 3 above, create the Amazon Resource Name of the Role in the format of\narn:aws:iam::YOUR-AWS-ACCOUNT-NUMBER:role/YOUR-INTENDED-ROLE-NAME\nExternal ID\nFor more control over who can assume your AWS IAM Role, an\nexternalId\n(shared secret) should be used. You will create the\nexternalId\nwith the Bitmovin API and then configure the AWS Role with it.\nThe\nexternalId\nis returned by the Bitmovin API after creating an S3-role-based Output. Enable it by selecting one of the following\nexternalIdMode\n:\nexternalIdMode\n:\nGLOBAL\n- A consistent and unique ID is used, which is used for every S3 role-based output that you create in one Bitmovin account.\nGENERATED\n- A unique but random UUID is returned.\nCUSTOM\n(\ndeprecated\n) Define a custom external ID that you can use in your AWS IAM role definition.\nexternalId\n(\ndeprecated\n): required when using\nCUSTOM\nas\nexternalIdMode\n.\nWarning:\nCUSTOM\nis now deprecated and must not be used.\nNote:\nIf you use\nexternalIdMode.GLOBAL\n, each new S3 Role-based input or output you create within the same account will return the same\nexternalId\n. As you will configure your AWS Role with this\nexternalId\n, please be aware that if you run the same code on a different Bitmovin account or even a different Bitmovin sub-organization, the Bitmovin API will return a different\nexternalId\n. This will lead AWS to deny access to the Role.\nTherefore, please create the Role-based input or output on the same Bitmovin account and sub-organization that you will use later in production.\nNote:\nIf you use\nexternalIdMode.GENERATED\n, each new S3 Role-based input or output you create will generate a new\nexternalId\n. Thus, you can only assume your AWS Role by re-using the S3 Role-based input or output whose\nexternalId\nyou used to configure the AWS Role.\nCreating the S3 Role Based Input or Output\nNow, using\nbucketName\n,\nroleArn\nand\nexternalIdMode\nfrom above, create an S3 Role based output (resp. input) with the Bitmovin API.\nSee the Java and cURL examples below.\nIn the response, you will receive the\nexternalId\nto be used in the next section when you create the Role.\nCreate an AWS IAM Role\nIn order to continue, you will have to create a\nRole\nin your AWS account.\nLogin to your AWS account.\nClick on \"Services\" near the top left.\nLook for \"Security, Identity & Compliance\" and click on \"IAM\". You are now in the\nIdentity and Access Management (IAM)\npage of your account.\nOn the left pane, click on \"Access Management\" -> \"Roles\".\nClick on \"Create Role\". The\nCreate Role\npage appears.\nThe page shows you four boxes of which you can select one for a type of trusted entity. Click on the \"Another AWS account\" box.\nIn the field \"Account ID\", enter\n630681592166\n.\nNext to \"Option\", check the \"Require external ID\" checkbox. A box opens asking you to enter an external ID.\nEnter the\nexternalId\nthat you got in the previous section.\nClick on \"Next: Permissions\"\nAssign a policy to the role by selecting it in the policy list.\n(Note: The pre-defined\nAmazonS3FullAccess\npolicy is known to be suitable but since it provides unrestricted access to your bucket, you might need to create a custom policy with fine-tuned access rights. Please review details of the permissions required for\nbuckets for Encoding Input and Output\nor\nbuckets for Analytics Exports\nbuckets\n)\nClick on \"Next: Tags\". The\nAdd Tags\npage appears, on which you optionally can assign tags to the role.\nClick \"Next: Review\". The\nReview\npage appears. Give the new role a name.\nRole name MUST match the YOUR-INTENDED-ROLE-NAME specified in Create an S3 role-based Input/Output > in\nroleArn\nClick \"Create Role\". You are now back in the\nIdentity and Access Management(IAM)-Roles\npage, and the system tells you \"The role Bitmovin has been created\". You also see the new role in the list of roles in your account.\nIf you want to learn more about Roles in AWS, please see their\ndocumentation\n.\nJSON Payload\nIf you prefer using the AWS CLI tools, you can create this role with the following JSON payload.\nJSON\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::630681592166:user/bitmovinCustomerS3Access\"\n      },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"sts:ExternalId\": \"<EXTERNAL_ID_RETURNED_BY_BITMOVIN_API>\"\n        }\n      }\n    }\n  ]\n}\n(Java) S3 Role-Based Output Example\nThis example uses our latest\nOpen API client for Java\n, which is available on Github. This example shows how to create an Output.\nCreate a new S3 Role-Based Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\n\nAclEntry aclEntry = new AclEntry();\naclEntry.setPermission(AclPermission.PRIVATE);\n\nList<AclEntry> acl = new ArrayList<>();\nacl.add(aclEntry);\n\nS3RoleBasedOutput s3RoleBasedOutput = new S3RoleBasedOutput();\ns3RoleBasedOutput.setBucketName(\"<BUCKET_NAME>\");\ns3RoleBasedOutput.setRoleArn(\"<AWS_ARN_ROLE>\");\ns3RoleBasedOutput.setExternalIdMode(ExternalIdMode.GLOBAL);\ns3RoleBasedOutput.setAcl(acl);\n\ns3RoleBasedOutput = bitmovinApi.encoding.outputs.s3RoleBased.create(s3RoleBasedOutput);\nHint: In case you chose to enable\nBlock public access\non your S3 bucket (recommended), you would have to make sure that the ACL is set to\nPRIVATE\non the output (as shown above) as well as on your Muxing configurations.\nTo create an Input is fairly similar, but you just use the\nS3RoleBasedInput\nresource and the\nbitmovinApi.encoding.inputs.s3RoleBased\nendpoint\nUse an existing S3 Role-Based Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\nS3RoleBasedOutput s3RoleBasedOutput = bitmovinApi.encoding.outputs.s3RoleBased.get(\"YOUR_S3_ROLE_BASED_OUTPUT_ID\");\n(CURL) S3 Role-Based Output Example\nCreate a new S3 Role-Based Output\nAPI reference:\ncreate a Role-Based S3 Output\n:\nShell\ncurl -X POST \\\n  https://api.bitmovin.com/v1/encoding/outputs/s3-role-based \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY' \\\n  -d '{\n    \"bucketName\": \"<BUCKET_NAME>\",\n    \"roleArn\": \"<AWS_ARN_ROLE>\",\n    \"externalIdMode\": \"<GLOBAL|GENERATED>\",\n    \"acl\": [\n        {\n            \"permission\": \"PRIVATE\"\n        }\n    ]\n}'\nGet an existing S3 Role-Based Output\nAPI reference:\nget an S3 Role-Based Output\nShell\ncurl  https://api.bitmovin.com/v1/encoding/outputs/s3-role-based/YOUR_S3_ROLE_BASED_OUTPUT_ID \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY'\nWhat's Next?\nNow that you have created S3 Role-Based Inputs and/or Outputs, you can use them in your encoding in much the same way as you would any other Input or Output.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/understanding-the-default-timestamp-offset-for-ts-muxings",
    "title": "Understanding the Default Timestamp Offset for TS Muxings",
    "text": "This is done to ensure correct audio visual sync, and to avoid negative\nDTS\n(DecodingTimeStamp), which break some decoders. Some of them apply a different start value for the audio and video\nPTS\n(PresentationTimeStamp). This can cause slight audio visual sync issues. Instead, a slight offset is applied to all streams so that no negative\nDTS\nvalues occur and all streams are aligned.\nWhy is that?\nStreams contain a lot of details, including timestamps, so a decoder knows how to handle the content properly. The\nDTS\ndecides when a frame has to be decoded, while the\nPTS\ndescribes when a frame has to be presented. This difference becomes important when using B-frames, which are frames that can have references to frames in the past, but also to frames in the future. Given that, there will be frames in the future, which a decoder needs to decode first in order to use them as reference. So they will have a smaller\nDTS\nthan their\nPTS\n. As\nDTS\nvalues can not be negative, muxers introduce a short offset for the\nPTS\nvalues to accommodate for the fact that\nDTS\ncan be smaller than the\nPTS\nvalues. As the\nPTS\nvalues are used to align the presentation time of different streams (audio and video) to each other, all streams need to start with the same\nPTS\noffset, to not introduce any sync issue.\nHow it is solved\nThe “best practise” way in order prevent these audio/video sync problems when processing such content is to set a\nPTS offset\nof 10 seconds. This is the default value used or\nTS muxings\nin our service as well as of other vendors. In our service it can be adjusted when creating a\nTS muxing\nas well. Please see the\nstartOffset\nproperty when creating a\nTS Muxing\nin the\nAPI reference\n)\nWhat about subtitles?\nWebVTT\nis a common way to provide subtitles for video content. It contains time windows when a certain subtitle has to be shown, and is therefore synchronized to the video content. The timing information in the\nWebVTT\ntrack gets converted into a timestamp information by the player. However, the calculated timestamp would be 10 seconds behind the actual point in time the subtitle would have to be shown. While many HTML5 players can compensate that, native players don't do that necessarily. In order to provide them with this information, the\nX-TIMESTAMP-MAP\ncan be used for that, as described in the\nHLS specification\n.\nSo, if\nWebVTT\nalready contains this information, it would start like that:\nWEBVTT\nX-TIMESTAMP-MAP=LOCAL:00:00:00.000,MPEGTS:900000\nThe value of\nMPEGTS\nactually describes an\nPTS offset\nof 10 seconds. The default timescale for MPEG-TS is 90,000. In order to achieve an offset of 10 seconds you multiply it by the timescale, which results in 900,000.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-speke-for-drm",
    "title": "Using SPEKE for DRM",
    "text": "Overview\nIn most of our tutorials about DRM, you will have noticed that you are expected to call specific DRM API endpoints and provide to them appropriate information in order for Bitmovin to be able to encrypt your content in the right way. For example, to generate a\nTS muxing with FairPlay\n, you need to supply a key and an initialisation vector (\niv\n), or for a\nCENC-encrypted fMP4 muxing supporting Widevine and PlayReady\nyou need to provide a\nkid\n,\nkey\nand a couple of\npssh\nparameters.\nIn your encoding workflow, you are therefore in charge of obtaining this information from your DRM provider, usually through their own APIs, and then pass that information into your encoding configuration.\nThis not only increases the complexity of your workflow, in particularly if you want to support multiple DRM systems (for example both PlayReady and Widevine), but also introduces security risks as you will probably need to store this sensitive information, if only temporarily.\nThe industry has therefore come up with a number of initiatives to try and improve this type of workflow. One of those is the Secure Packager and Encoder Key Exchange (shortened to SPEKE) standard.\nWith SPEKE, the responsibility for obtaining DRM information, in particular encryption keys, is delegated to the encoder. The encoder, based on your configuration, will make a secure request to the SPEKE server, for one or multiple DRM systems, and use the information in the response to configure and encrypt your muxings.\nℹ️\nSPEKE is an extension of another standard named Content Protection Information Exchange (CPIX), with additional functionality for some use cases not covered by CPIX. Bitmovin does not currently support CPIX directly, but depending on your DRM provider, you may find that our SPEKE implementation can work with their CPIX endpoints out of the box.\nRequirements\nEncoder Version: v2.40.0 or above\nSDK Version: v1.58.0 or above\nYou also need to have access to a SPEKE v1 server. Most DRM vendors support that protocol\n⚠️\nThis tutorial only covers SPEKE v1. Bitmovin does not currently support SPEKE v2. Depending on your DRM provider, you may however find that our SPEKE integration can work with a SPEKE v2 server, although some limitations may apply.\n📝 Always check with your DRM vendor to which extent their SPEKE implementation varies from the functionality described in this document.\nAuthentication\nThe SPEKE protocol enables multiple ways for the encoder to authenticate with the SPEKE server. The Bitmovin encoder supports the following:\nWith credentials\nYour DRM vendor will provide a username and password that you will provide in your encoding configuration.\nNote: They may have additionally required to do some IP whitelisting. If that is the case, contact your Bitmovin Technical Account Manager to ask for the necessary information to pass to your DRM provider.\nWith AWS Identity and Access Management\nIf your DRM vendor have deployed their SPEKE server in AWS, they may also support a more secure and therefore recommended role-based authentication. This will allow the Bitmovin encoding platform to request access to the SPEKE server on your behalf\nFor this, an initial setup is necessary:\nFirst, you create a role in your own AWS account, allowing the Bitmovin’s user (defined by ARN\narn:aws:iam::630681592166:user/bitmovinCustomerSpekeAccess\n) to assume that role:\nJSON\n{\n  \"Effect\": \"Allow\",\n  \"Principal\": {\n    \"AWS\": \"arn:aws:iam::630681592166:user/bitmovinCustomerSpekeAccess\"\n  },\n  \"Action\": \"sts:AssumeRole\",\n  \"Condition\": {\n    \"StringEquals\": {\n      \"sts:ExternalId\": \"{{externalId}}\"\n    }\n  }\n}\nThe\nsts:ExternalId\nis optional, but recommended. See the next section to determine what to set it to.\nThen, attach the following policy to the role, to allow it to invoke the AWS API Gateway:\nJSON\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"execute-api:Invoke\"\n      ],\n      \"Resource\": [\n        \"arn:aws:execute-api:{{region}}:*:*/*/POST/*\"\n      ]\n    }\n  ]\n}\nin which you replace\n{{region}}\nwith the region of the API gateway (for example\nus-west-2\n) in which the SPEKE server is deployed, or set it to\n*\nto give the role access to all regions.\nFinally, provide the ARN of the role generated in this way to your DRM vendor to allow them to enable users using this role to get access to the SPEKE server.\nAt runtime, the Bitmovin user\narn:aws:iam::630681592166:user/bitmovinCustomerSpekeAccess\nwill assume the role and call the SPEKE server’s API Gateway endpoint to request DRM information.\nEncoding Configuration\nWhen it comes to configuring your encoding to make use of the SPEKE protocol, you will start in much the same way as any encoding, by defining your input(s), your audio and video codec configurations, one or multiple output streams, and muxings to containerize them.\nThen, instead of applying a system-specific DRM resource on top of the (clear) muxing, you will use the endpoint specific to SPEKE resources.\nIn general, the following parameters will need to be specified in the payload sent to that endpoint:\nprovider\ncontains access details for your DRM vendor’s SPEKE server: -a\nurl\nto the server’s endpoint\nif you use credentials-based authentication, the\nusername\nand\npassword\nprovided by your SPEKE provider\nor, for role-based authentication,\nroleArn\nwhich contains the ARN of the role created earlier\nexternalId\n(and/or\nexternalIdMode\n, see below)\na\ngatewayRegion\ncorresponding to the region of the AWS API Gateway used to access the SPEKE server.\nA list of\nsystemIds\n, which identify the content protection scheme(s) you are requesting to apply to this muxing. See the Annexes below for a list of common identifiers. Note that not all\nsystemIds\nare supported for all muxing types.\ncontentId\n, a unique identifier for your content. If you do not provide one, a random identifier will be generated.\nkid\n, an optional key identifier, which will be generated if not set. A SPEKE DRM server will usually return the same keys each time it is called with the same pair of\ncontentId\nand\nkid\niv\n, a 16 byte initialisation vector, represented by a 32-character text string (eg.\n08eecef4b026deec395234d94218273d\n). This property is mandatory for some protection systems, in particular AES-128 and FairPlay, but check with your DRM vendor what is specific to their solution.\nExternal ID\nThe External ID set in the AWS IAM role and the SPEKE configuration helps in making your role-based solution more secure, as explained in the\nAWS documentation\n. You have different choices here, and the externalIdMode parameter in the SPEKE endpoint payloads is used to specify the method that applies:\nSet\nexternalIdMode\nto\nGLOBAL\nand the\nexternalId\nwe use when assuming the role will be set to the Organisation ID of the account that you perform your encodings in (which you can find\nin the Bitmovin Dashboard\n). You will therefore have to set it in your AWS role accordingly first.\nSet\nexternalIdMode\nto\nCUSTOM\nif you want to use your own string, but make sure that it is fairly unique.\nSet\nexternalIdMode\nto\nGENERATED\nto have the Bitmovin platform generate a random string when you create the SpekeDrm resource by calling the relevant endpoint. This will only be practical for simple workflows where you reuse the same DRM object for all your muxings, if only because it will require you to modify your AWS role each time you create a new resource, after retrieving that value through the Bitmovin API.\nExamples\nIn the code snippets below, we skip the aspects of the configuration that are not specific to DRM. You can consult any of our other tutorials if you need a full example; a good one for comparison and contrast with the next part of this tutorial is the one on\ncreating Widevine DRM-protected content\n.\nSingle Protection System, with credentials\nLet’s start with a simple example, in which we encode our source file into a H264 single representation, using fragmented MP4, and encrypt it with CENC for Widevine. We access the SPEKE server with simple credentials.\nJava\nSpekeDrmProvider spekeDrmProvider = new SpekeDrmProvider();\nspekeDrmProvider.setUrl(\"https://url-to-my-drm-vendor-speke-server\");\nspekeDrmProvider.setUsername(\"my-speke-username\");\nspekeDrmProvider.setPassword(\"this-is-my-secure-password\");\n\nH264VideoConfiguration h264Config = createH264VideoConfig();\nStream videoStream = createStream(encoding, input, inputFilePath, h264Config);\n\n// Like with any DRM configuration, we create a base (clear) muxing with no output\nFmp4Muxing videoMuxing = createBaseFmp4MuxingWithoutOutput(encoding, videoStream);\n\n// SPEKE config for Widevine\nSpekeDrm spekeDrm = new SpekeDrm();\nspekeDrm.setProvider(spekeDrmProvider);\nspekeDrm.setContentId(\"my-unique-content-id\");\nspekeDrm.setSystemIds(Arrays.asList(\"edef8ba9-79d6-4ace-a3c8-27dcd51d21ed\"));\n\n// We define where to output the protected muxing\nEncodingOutput encodingOutput = new EncodingOutput();\nencodingOutput.setOutputId(output.getId());\nencodingOutput.setOutputPath(\"path-to-my-output-folder\");\nspekeDrm.setOutputs(Arrays.asList(encodingOutput));\n\nbitmovinApi.encoding.encodings.muxings.fmp4.drm.speke.create(\n   encoding.getId(), fmp4Muxing.getId(), spekeDrm);\nWhen it comes to configuring your manifest, it works in much the same way as with any other DRM endpoint, and you provide the identifier of the DRM you created (spekeDrm in this example) to the endpoints to create the DashFmp4DrmRepresentation and ContentProtection elements you want added to your manifest.\nFor this simple example, you can also simply use our Default Manifest functionality to have our encoder build it all for you, as demonstrated in the tutorial on\ncreating Widevine DRM-protected content\n.\nMultiple Protection System, with AWS IAM Authentication\nMost of the times, you will probably want to encrypt your content for support on many players and platforms, which requires different DRM systems.\nWhen the protection systems are compatible (as is the case generally with Widevine and PlayReady for CENC encoding - in CTR mode), you simply need to supply all relevant system IDs in a single array:\nJava\n// Widevine and PlayReady\nspekeDrm.setSystemIds(\n   Arrays.asList(\"edef8ba9-79d6-4ace-a3c8-27dcd51d21ed\", \"9a04f079-9840-4286-ab92-e65be0885f95\")\n);\nIf you need to encrypt your content with incompatible encryption mechanisms, such as Widevine/PlayReady (in CTR mode) on the one hand and FairPlay (in CBC mode) on the other, you will need to create distinct outputs, each with its own DRM configuration. This is not specific to SPEKE, but also needs to be done when SPEKE is involved.\nLet’s take the opportunity to also use role-based authentication to access the SPEKE server:\nJava\nSpekeDrmProvider spekeDrmProvider = new SpekeDrmProvider();\nspekeDrmProvider.setUrl(\"https://url-to-my-drm-vendor-speke-server\");\nspekeDrmProvider.setRoleArn(\"arn:aws:iam::1234567890:role/my-speke-role\");\nspekeDrmProvider.setGatewayRegion(\"eu-west-1\");\n\nH264VideoConfiguration h264Config = createH264VideoConfig();\nStream videoStream = createStream(encoding, input, inputFilePath, h264Config);\n\n// A single base (clear) muxing with no output is used for both DRM outputs\nFmp4Muxing videoMuxing = createBaseFmp4MuxingWithoutOutput(encoding, videoStream);\n\n// one SPEKE config for Widevine and PlayReady\nSpekeDrm spekeDrmCenc = new SpekeDrm();\nspekeDrmCenc.setProvider(spekeDrmProvider);\nspekeDrmCenc.setContentId(\"my-unique-content-id\");\nspekeDrmCenc.setSystemIds(\n   Arrays.asList(\"edef8ba9-79d6-4ace-a3c8-27dcd51d21ed\", \"9a04f079-9840-4286-ab92-e65be0885f95\"));\n\n// another SPEKE config for FairPlay\nSpekeDrm spekeDrmFairplay = new SpekeDrm();\nspekeDrmFairplay.setProvider(spekeDrmProvider);\nspekeDrmFairplay.setContentId(\"my-unique-content-id\");\nspekeDrmFairplay.setSystemIds(\n   Arrays.asList(\"94ce86fb-07ff-4f43-adb8-93d2fa968ca2\"));\n\n// each DRM is output in a distinct folder\nEncodingOutput encodingOutput1 = new EncodingOutput();\nencodingOutput1.setOutputId(output.getId());\nencodingOutput1.setOutputPath(\"path-to-my-output-folder/cenc\");\nspekeDrmCenc.setOutputs(Arrays.asList(encodingOutput1));\n\nEncodingOutput encodingOutput2 = new EncodingOutput();\nencodingOutput2.setOutputId(output.getId());\nencodingOutput2.setOutputPath(\"path-to-my-output-folder/fairplay\");\nspekeDrmFairplay.setOutputs(Arrays.asList(encodingOutput2));\n\n// Apply both DRMs to the base muxing\nbitmovinApi.encoding.encodings.muxings.fmp4.drm.speke.create(\n   encoding.getId(), fmp4Muxing.getId(), spekeDrmCenc);\nbitmovinApi.encoding.encodings.muxings.fmp4.drm.speke.create(\n   encoding.getId(), fmp4Muxing.getId(), spekeDrmFairplay);\nYou will find a full example of such a configuration at\nGitHub - bitmovin/bitmovin-api-sdk-examples: Set of encoding workflow examples highlighting the use of the Bitmovin API SDKs\n, in which we create separate manifests for DASH (with CENC) and HLS (with FairPlay).\nKnown Limitations\nThe SPEKE v1 protocol is meant to be used only in cases where all tracks (video and audio) use a single encryption key. Support for multiple key is part of the SPEKE v2 / CPIX v2.3 protocols, but is not currently supported by the Bitmovin encoder. However, given how the configuration of DRM is done on a per-muxing basis with Bitmovin, you may be able to provide different key IDs (kid) for different muxings (or sets of muxings) to get the same result. Check with your SPEKE provider to see if they would support such a workflow.\nAuthentication through JWT tokens is not currently supported.\nSPEKE can be used for Live encodings, but key rotation is not supported\nAnnexes\nSupported System IDs\nThe mapping of system IDs to protection system technologies is defined as an industry-wide standard, partly managed by DASH-IF (for CPIX)\nSystem ID\nProtection System\nedef8ba9-79d6-4ace-a3c8-27dcd51d21ed\nWidevine\n81376844-f976-481e-a84e-cc25d39b0b33\nAES-128\n1077efec-c0b2-4d02-ace3-3c1e52e2fb4b\nClearKey (with CENC)\n94ce86fb-07ff-4f43-adb8-93d2fa968ca2\nFairPlay\n9a04f079-9840-4286-ab92-e65be0885f95\nPlayReady\nFor more information, check the DASH-IF reference at\nhttps://dashif.org/identifiers/content_protection/\n. Note that any system ID not listed in the table above may not be supported by our encoder.\nCheck also with your DRM vendor whether their SPEKE server supports all these system IDs or additional ones.\nFinally, note that the protection system you can apply will depend on the muxing type. This is not a constraint specific to SPEKE however, but the standard general consideration when applying DRM to muxings.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/retrieving-vod-encoding-information-with-the-bitmovin-api",
    "title": "Retrieving VOD Encoding Information with the Bitmovin API",
    "text": "Introduction\nIn most of the tutorials in our documentation and our public examples, you will find detailed instructions on how to configure various aspects of your encoding workflows with focus on producing output files that can be used directly for playback or for ingest into downstream systems. In this article however, we will focus on metadata about your encodings, and how to retrieve information about previously performed encodings, in particular for the purpose of auditing past performance, writing reports and to check on the health of your workflows.\nThis article also provides a few tips and tricks on how you can adjust your encoding configuration to improve your ability to retrieve specific information at a later stage.\nList of encodings\nIn most cases, and regardless of the information you are trying to obtain, you will want to obtain a list of encodings. Use the\nlist all encodings endpoint\nfor that purpose. In its most basic usage, it will return a list of the last 25 encodings that have been created in the account, in reverse order of creation (ie. with the newest coming first).\nEncoding Information returned\nIn addition to the fields that you set when creating the encoding (such as\nname\n,\ndescription\nand\nlabels\n), and an\nid\nfield with a unique identifier generated by the platform, the payload returned for each encoding in the list will also contain essential information to determine its status:\nstatus\nwith one of the following values (details about the meaning of each can be found\nin this FAQ\n):\nCREATED\nfor encodings that have been configured but not started\nQUEUED\nfor encodings that have been started, but are in a queue (either waiting for an encoding slot to be available, of for the encoder to be warmed up)\nRUNNING\nfor encodings that are being performed\nCANCELED\nfor encodings that were cancelled by the user\nFINISHED\nfor encodings that successfully completed\nERROR\nfor encodings that failed\nTRANSFER_ERROR\nfor encodings that completed, but failed in the last step when attempting to transfer the output files to the Output storage (and\nhere\nis how you go about resolving those)\nprogress\ncontains a value between 0 and 100 depending on the progress of the encoding\ntimestamps for the most important stages in the encoding:\ncreatedAt\n,\nstartedAt\n,\nqueuedAt\n,\nrunningAt\nand\nfinishedAt\n. The latter reports the time when the encoding was completed, cancelled or stopped with an error.\nNote that some encodings may also carry an\nerrorAt\n. Since release 1.50.0 of our API, this field has been deprecated, in favour of the combination of the\nfinishedAt\nproperty and\nstatus\nproperty (set to\nERROR\n). It is however still included for backward compatibility\nselectedEncoderVersion\nstates the version of the encoder used to perform the encoding. If a version tag such as\nSTABLE\nor\nBETA\nwas used when configuring the encoding (as is recommended), this field will provide the actual version used.\nselectedCloudRegion\nstates the cloud provider and region in which the encoder performed the encoding. If the encoding was configured with\nAUTO\n, this field is resolved to the actual region used.\nQuery Filters\nTo retrieve only a logical subset of encodings, you can use query parameters when calling the\nlist all encodings\nendpoint to filter the results.\nStatus filter\nIf you want to return only a list of encodings with a particular status, use the status query parameter with one of the values listed above.\nDate filters\nIf you want encodings created or started within a certain time window, you can use and combine 6 datetime-based filters, which take ISO8601-formatted values:\ncreatedAtNewerThan\nand its counterpart\ncreatedAtOlderThan\nstartedAtNewerThan\nand its counterpart\nstartedAtOlderThan\nfinishedAtNewerThan\nand its counterpart\nfinishedAtOlderThan\nSearch filter\nThe\nsearch\nfilter allows you to do a match on some of the metadata associated with the encoding, which at the time of writing this article comprises the\nname\nand\nlabels\nfields. It also allows partial matching of strings (from the start of those metadata fields), by using the\n*\nwildcard.\nTip: Naming conventions\nTo make best use of this filter, you need to ensure that you define metadata when you configure your encoding in such a way as to enable this type of metadata query down the line. Our recommendations are as follows:\nTag your encoding with one or more small labels that represents some important aspects of your encoding workflows that you may want to retrieve information about in a segmented way at a later stage, such as different encoding workflows, different targets, specific experiments you are performing, etc.\nAlternatively, or additionally, prefix the encoding name with a short string that identifies similar or additional important aspects about the encoding for later retrieval\nOther parameters and filters\nThere are other parameters that may be useful for the purpose of tuning the results of the query:\ntype\n: if you are using a mix of Live and VOD encodings in your account, you will want to set that filter to\nVOD\n.\nsort\n: allows you to define the order in which the encodings are returned, by specifying a field and a direction (\nasc\nor\ndesc\n). The most meaningful fields for this type of request are\ncreatedAt\nand\nstartedAt\n. The default applied is\ncreatedAt:desc\n.\nselectedEncoderVersion\n: to retrieve encodings performed with a specific encoder version.\nselectedCloudRegion\n: to retrieve encodings performed with a specific cloud provider and in a specific region.\nA full list of other parameters can be found in the API documentation for the\nList all encodings\nendpoint.\nPagination\nRemember that if you are planning to pull information for a large number of encodings, the results will come in batches, by default of 25 items, and you will need to paginate through the results with the\nlimit\nand `offset´ parameters. The first defines how many objects are returned in the response payload (with a maximum of 100), and the latter at what index to start returning results. Note that the payload returned for this type of query will always include a count of the total number of encodings that match the filters, as well as URLs to the next (and previous) batch of encodings for the applied filters.\nExamples\nEncodings with errors\nTo return the last 50 VOD encodings that failed, you would make a call to\nHTTP\nhttps://api.bitmovin.com/v1/encoding/encodings\n   ?type=VOD\n   &status=ERROR\n   &limit=50\nNote that with our SDKs, query and filter parameters are typically provided as properties of a specific object that is passed as a single property to the\nlist\nmethod. The response from that endpoint would be a\nPaginationResponse\npayload that contains pagination information and an array of\nitems\n.\nFor example, with the Java SDK, the query above would look like this:\nJava\nEncodingListQueryParams queryParams = new EncodingListQueryParams();\nqueryParams.setType(\"VOD\");\nqueryParams.setStatus(\"ERROR\");\nqueryParams.setLimit(50);\n\nList<Encoding> encodings = bitmovinApi.encoding.encodings.list(queryParams).getItems();\nEncodings performed on a specific day for a specific workflow\nTo return a full list of completed encodings in a specific 24 hour period in natural time order of creation of the encodings, for a labeled workflow and an experiment \"A\" (which was used as prefix to the encoding name):\nHTTP\nhttps://api.bitmovin.com/v1/encoding/encodings\n   ?sort=createdAt:asc\n   &type=VOD\n   &status=FINISHED\n   &search=labels:workflow1 AND name:ExperimentA*\n   &createdAtNewerThan=2020-09-17T10:00:00Z\n   &finishedAtOlderThan=2020-09-18T10:00:00Z\nThis payload will only return the first 25 encodings. The call to get the next 25 would be\nhttps://api.bitmovin.com/v1/encoding/encodings\n   ?sort=createdAt:asc\n   &type=VOD\n   &status=FINISHED\n   &search=labels:workflow1 AND name:ExperimentA*\n   &createdAtNewerThan=2020-09-17T10:00:00Z\n   &finishedAtOlderThan=2020-09-18T10:00:00Z\n   &limit=25\n   &offset=25\nLet’s look at it in Python this time:\nPython\npage = bitmovin_api.encoding.encodings.list(\n   query_params=EncodingListQueryParams(\n      type_=\"VOD\",\n      status=\"FINISHED\",\n      search=\"labels:workflow1 AND name:ExperimentA*\",\n      created_at_newer_than=datetime(2020, 9, 17, 10, 00, 00),\n      finished_at_older_than=datetime(2020, 9, 18, 10, 00, 00),\n      offset=25,\n      limit=25 \n   )\n)\nencodings = page.items\nDetailed status\nTo get more details about the status of your encoding, and in particular finer grained details about the various stages of the encodings, and details of warnings and errors that may have occurred, you will need to query the specific\nstatus endpoint\nfor each encoding in turn. In addition to the same information as above, additional significant information in the response payload comprises:\nsubTasks\nlist the major phases in the encoding (eg. queue, file download, file analysis, encoding, muxing, etc.) and provide for each:\nstatus and progress information\ntiming information\noptionally metadata containing statistics relevant to the phase (eg. size of the download, number of bytes or frames encoded, etc.\nmessages\nproviding detailed information about stages reached in the encoding (info, debug), as well as errors and warnings\nInput information\nIn certain cases, you may want to be able to determine the spec of the input file (or files) that were used in your encodings, for example because you want to analyse the encoding performance in relation to the type of file you ingested, or you want to get an idea of why an encoding failed with a specific file, or understand\nhow a specific stream condition was applied\n.\nWhen you configure an encoding, you provide information on how to access the input file directly or indirectly (through an\nIngestInputStream\n) when creating the (output) Stream. Since it is possible to have different streams to use different files (for example, if audio and video input files are distinct), you may need to first\nretrieve a list of streams\nfor your encoding and collect the identifier of the relevant stream(s) to go further.\nFile path and Stream selection\nDetermining the file path and input stream selection parameters used when configuring the encoding will depend on whether the input file information was provided directly in the\nStream\ncreation payload or as an\nIngestInputStream\n. The difference is explained in\nthis FAQ\n. On that basis, you therefore need to either:\nretrieve the stream details\n(which requires a stream identifier), and/or\nretrieve the ingest input streams\n(which does not)\nFile specification\nThe Bitmovin API also allows you to retrieve the details of the analysis of your input file, such as the list of streams found in the file, and their specification (for example, file size, duration, codecs, dimensions, channel format, etc.)\nTo retrieve this information, use the\nstream input details endpoint\nwith your encoding ID and stream ID. Note that the information returned is for the whole input file, and does not take into consideration the stream selection configuration of your stream. This may be an advantage if all your streams use the same input file, as a single call will return all the information you might be interested in.\nHere is an example of the type of input file analysis information returned:\nJSON\n{\n  \"data\": {\n    \"result\": {\n      \"formatName\": \"matroska,webm\",\n      \"startTime\": 0.0,\n      \"duration\": 263.013,\n      \"size\": 51107796,\n      \"bitrate\": 1554532,\n      \"tags\": {\n        \"ENCODER\": \"Lavf56.24.101\",\n        \"COMPATIBLE_BRANDS\": \"isommp42\",\n        \"MAJOR_BRAND\": \"mp42\",\n        \"MINOR_VERSION\": \"0\"\n      },\n      \"audioStreams\": [\n        {\n          \"id\": \"e7328177-ff24-41db-b213-b03ed0004c6b\",\n          \"position\": 1,\n          \"codec\": \"aac\",\n          \"sampleRate\": 44100,\n          \"channelFormat\": \"2\",\n          \"hearingImpaired\": false\n        }\n      ],\n      \"videoStreams\": [\n        {\n          \"id\": \"5c916072-2f49-4233-b25d-b8347b6d8987\",\n          \"position\": 0,\n          \"codec\": \"h264\",\n          \"fps\": \"24000/1001\",\n          \"width\": 1280,\n          \"height\": 720,\n          \"par\": 1.0\n        }\n      ]\n    }\n  }\n}\nNote:\nAt the time of writing, it is only possible to get full details about the input files when the encoding only used 1 file per (output) stream. For use cases such as\nconcatenation of files\n, the information is not available for all files. This will be added in a future version of the API.*",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/finding-and-understanding-your-encoding-ids",
    "title": "Finding and Understanding Your Encoding ID's",
    "text": "Details\nReferencing Encoding Id in Support tickets & Bitmovin Community helps Bitmovin Staff check on your Encoding job.\nYou can fetch the Encoding ID in two easy ways:\nFor VOD Encodings:\nEncoding ID in the list of Encodings\nEncoding ID from a particular encoding:\nFor Live Encodings\nThe Encoding IDs can be found in the same locations within the dashboard as those for VOD.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/90c107b143d416a47f9abbf5d19a85506fc423cf0aa423402f2b94f3eaf3b48b-encodingid2.jpg",
      "https://files.readme.io/f4b7e3e60cd842d13bfb1861b9f26fecb79bce52212e1c77542e4bc1d03ea323-encodingid.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/documentation",
    "title": "Encoding Documentation",
    "text": "Bitmovin\noffers advanced encoders designed to meet the diverse needs of video streaming.\nOur\nVOD Encoder\nis built for pre-recorded content, delivering optimized video quality and fast encoding times to enhance the on-demand viewing experience. It supports adaptive streaming formats, advanced codecs, and high scalability, ensuring top performance for various use cases.\nFor real-time video, our\nLive Encoder\nis a SaaS product that can be deployed at scale on multiple public clouds, providing high-quality streaming, charging based on usage. Both encoders are designed to offer efficiency, flexibility, and premium viewer satisfaction.\nWhether you’re streaming live events or managing on-demand libraries, Bitmovin’s encoders provide reliability, efficiency, and premium viewer experiences. Discover more at our:\nWebsite\nBlog\nGithub\nThe advantages of a flexible Encoding stack\nWe believe that video encoding should be decoupled from the underlying hardware to enable agile media teams to deploy anywhere based on their needs. We developed Bitmovin Video Encoding to be the most flexible encoding in the market that does not lock you into a specific hardware or cloud.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/aws-s3-simple-live-outputs",
    "title": "AWS S3 Simple Live Outputs",
    "text": "Overview\nThis method is available as a creation method using the UI\nLive Encoding Wizard\nand was developed to enable users to experiment with using the Live Encoder on AWS and output to S3 with relatively little effort. This can be especially helpful during testing and discovery phases, and is a short cut method for creating a Role Based S3 output.\n🚧\nThis output type can be especially helpful during testing and discovery phases. We would always recommend that users run production workflows using Access Keys or Role based S3 outputs. If in doubt, consult with your InfoSec team.\nInstead of you providing your AWS Access/Secret key pair to our Encoding or Analytics service, we provide you with an AWS IAM (Identity and Access Management) user name, which you can grant specific access rights in your account so it can access your desired S3 bucket.\nIn this tutorial, we will create an\nS3 bucket in your AWS account\nwhich will serve as the input or output storage for your encodings.\nNOTE\n: This tutorial needs to be repeated for EACH account/sub-organization you want a S3-Role-Based access for.\nSetting up these buckets consists of 3 main sections:\nS3 Bucket permissions: select or create a new S3 Bucket and ensure general security permissions match the Bitmovin requirements.\nOutput creation in the Bitmovin UI: using the creation wizard create a new output using this S3 bucket and generate the policy.\nUpdate Policy and CORS: apply the generated policy to your S3 Bucket permission and ensure CORS is configured correctly for playback.\nRequirements\na\nBitmovin account\n. Please make sure that you run this tutorial on the same Bitmovin account (and sub-organization, if applicable) that you want to use for production. If you don't have or know your production account yet, remember to repeat this tutorial later on your production account.\nan\nAWS account\nto create the input and/or output bucket and the\nRole\nthat allows the Bitmovin system to access that bucket.\nS3 role-based buckets can be used for segmented outputs with\nencoder version\n2.29.0\nor higher.\nCreate an AWS S3 Bucket\nIn the AWS Management Console, open the\nS3 section\n.\nClick on the\nCreate Bucket\nbutton which starts the bucket creation wizard\nIn the \"Name and Region\" panel, choose a bucket name (for example\nmy-bitmovin-bucket\n) and a Region (for example\n(EU) Ireland)\n), then press\nNext\nConfigure the\nObject Ownership\ndepending on your needs\na.\nACLs enabled\n, this is\nrequired\nif PUBLIC_ACCESS is configured for Encoding outputs. Encodings that are started in our\nBitmovin Dashboard\nset the PUBLIC_ACCESS to enable preview playback.\nb. ACLs disabled. If this policy is used, Encoding outputs need to be set to have the PRIVATE ACL.\nConfigure\nsettings for playback\na. The default settings will\nBlock\nall\npublic access\nb. To\nenable playback\nfor manifests and files from the bucket,\nuncheck\n🔳\nBlock\nall\npublic access.\nComplete the wizard and click click\nCreate Bucket\nKeep in mind that playing directly from an S3 bucket can be expensive, and a CDN endpoint is typically cheaper.\nDashboard wizard step\nUpdate S3 Bucket Policy & CORS\nIn order to continue, you will have to update the\nPolicies\nin your AWS account.\nMake sure you have the AWS console open and have navigated to the S3 bucket permissions where the security settings were updated.\nIf you want to learn more about\nPolicies\nin AWS, please see their\ndocumentation\nNOTE\n: You will get JSON Payload by clicking copy button.\nApplying the JSON to S3\nEdit the bucket policy and paste the JSON file from the UI form to the Bucket.\nUpdating the CORS\nAlso under the Permissions tab in the AWS console, update the CORS settings.\nYou can copy the JSON below.\nJSON\n[\n    {\n        \"AllowedHeaders\": [\n            \"Authorization\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"*\"\n        ],\n        \"ExposeHeaders\": [],\n        \"MaxAgeSeconds\": 3000\n    }\n]\nThen edit the CORS policy and paste in the AWS Bucket, as shown in the image below.\nUsing the Live Output\nThe bucket will appear in the Outputs list, and in the Wizard under AWS Role Based outputs.\nYou can confirm the bucket is created in the API by using\nLive S3 Role-based Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/97ae4b0-image.png",
      "https://files.readme.io/281e789-image.png",
      "https://files.readme.io/b629497-image.png",
      "https://files.readme.io/7ba1524-image.png",
      "https://files.readme.io/3b626ee-image.png",
      "https://files.readme.io/dbc8787-Screenshot_2024-04-06_at_11.50.28.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/deinterlacing-content-with-bitmovin-encoding",
    "title": "Deinterlacing Content with Bitmovin Encoding",
    "text": "Introduction\nMonitors of digital playback devices such as computers or smartphones display videos in\nProgressive\nmode, which means each frame is displayed as one pixel row after the other.\nOlder analog devices such as CRTs display videos in\nInterlaced\nmode. This means, each frame is divided into two\nfields\n. One field contains the odd-numbered pixel rows (1, 3, 5, ...) and the other field contains the even-numbered pixel rows (2, 4, 6, ...). When it was introduced 70 years ago (primarily for television video formats like NTSC and PAL), Interlacing reduced flicker and provided well distributed brightness on the screen. With today's digital screens this is no longer necessary.\nMany cameras and equipment still support the interlaced format and thus, many source files contain interlaced video (e.g., if your source files originate from a broadcast playout system, there is a high chance that they may still be interlaced).\nInterlaced files are labeled with an \"i\" for the frame rate, e.g.\n1080i/25 (or 1080i50)\n1080p/25 (or 1080p25)\nvideo of 1080 pixels height with 25 interlaced frames (=50 fields) per second\nvideo of 1080 pixels height with 25 progressive frames per second\nWhen encoding interlaced video into Progressive mode, the encoder must construct each frame from two fields.\nTypically however, the fields have been recorded separately, which means that two consecutive fields show the image with a time difference of double the frame rate (e.g. 1/50 second). Especially when the video contains fast movement, this results in undesirable artifacts known as \"the comb effect\" where the odd and even pixel rows are clearly visible in the frame. The result is a blurry video quality.\nInterlaced Picture\nDe-interlaced picture\nThus, to remove the comb effect and improve video quality,\nDeinterlacing\nmust be applied to the video before encoding. The Bitmovin Encoder includes a Deinterlacing filter which does exactly that.\nPlease note:\nIf possible, check with your content provider if they can provide the sources as Progressive mode. They will be of better quality than deinterlaced even with the best filter.\nThe Deinterlacing Filter\nThe Deinterlacing filter's documentation can be found here:\nCreate Deinterlace Filter\nA Java SDK example that uses the Deinterlacing filter can be found here:\nhttps://github.com/bitmovin/bitmovin-api-sdk-examples/blob/master/java/src/main/java/Filters.java\nThe example is also available in other languages.\nUsing the Deinterlacing Filter\nWe assume that you worked with the Bitmovin examples before and know how to configure an encoding. If you are new to Bitmovin encoding, please refer to our tutorials such as\nGet started with the Bitmovin API\nand closely check the examples we provide in your preferred language:\nSDKs\n.\nIn the\nFilters.java\nexample, after creating the video codec configuration\nJava\nH264VideoConfiguration h264Config = createH264VideoConfig();\nand after creating the video stream\nJava\nStream videoStream = createStream(encoding, input, inputFilePath, h264Config);\nyou need to apply the filters. You do this by creating a list of filters, adding all filters you need to this list, and applying the filter list to the video stream.\nJava\nList<Filter> filters = new ArrayList<>();\n(...)\nfilters.add(createDeinterlaceFilter());\n\ncreateStreamFilterList(encoding, videoStream, filters);\nThe example shows the mechanism in the\ncreateStreamFilterList()\nfunction. In that function, a list of\nStreamFilter\nobjects is created. Then, each filter is registered in a\nStreamFilter\nobject and added to the list. Note that you can determine the order of the filters with the\nposition\nparameter. At the end you create the filters and apply them to the stream and the encoding.\nConfiguring the Deinterlacing Filter\nThe most interesting function of the previous section is\ncreateDeinterlaceFilter()\nas it contains the configuration of the filter.\nIn the example however, this function is as simple as it can be:\nJava\nprivate static DeinterlaceFilter createDeinterlaceFilter() throws BitmovinException {\n  return bitmovinApi.encoding.filters.deinterlace.create(new DeinterlaceFilter());\n}\nHere, a default Deinterlace filter is created.\nIf you want to configure the Deinterlace filter differently than default, you can do it by altering the function:\nJava\nprivate static DeinterlaceFilter createDeinterlaceFilter() throws BitmovinException {\n    DeinterlaceFilter deinterlaceFilter = new DeinterlaceFilter();\n    (...)\n    return bitmovinApi.encoding.filters.deinterlace.create(deinterlaceFilter);\n}\nThe\n(...)\ndescribes the place where you can set optional configurations.\nThe fields you can configure are:\nmode\nframeSelectionMode\nparity\nAutoEnable\nFollowing the description of the fields in detail:\nmode\nJava\ndeinterlaceFilter.setMode(DeinterlaceMode.FRAME);\nValue\nDescription\nFRAME (default)\nOutput one frame for each frame (e.g. i50 will be encoded into p25)\nFIELD\nOutput one frame for each field (e.g. i50 will be encoded into p50)\nFRAME_NOSPATIAL\nLike FRAME, but it skips the spatial interlacing check.\nFIELD_NOSPATIAL\nLike FIELD, but it skips the spatial interlacing check.\nframeSelectionMode\nJava\ndeinterlaceFilter.setFrameSelectionMode(DeinterlaceFrameSelectionMode.ALL);\nValue\nDescription\nALL (default)\nDeinterlace all frames\nINTERLACED\nOnly deinterlace frames marked as interlaced\nparity\nJava\ndeinterlaceFilter.setParity(PictureFieldParity.AUTO);\nThis sets the picture field parity assumed for the input interlaced video. As said above, in interlaced video streams a frame comprises of two fields. One includes the odd pixel rows (1, 3, 5,...). This is called the\ntop field\n. The other includes the even pixel rows (2, 4, 6...) This is called the\nbottom field\n. When reconstructing a frame from a stream of alternating fields (top, bottom, top, bottom...), it is important to understand whether a frame starts with a top field or a bottom field.\nValue\nDescription\nTOP_FIELD_FIRST\nAssume the top field is first\nBOTTOM_FIELD_FIRST\nAssume the bottom field is first\nAUTO (default)\nEnable automatic detection of field parity. If the interlacing is unknown or the decoder does not export this information, top field first will be assumed.\nautoEnable\nThis value controls the application of the filter per se. With this comes in certain cases an alteration of some configured parameters.\nThe possible values of this field are:\nALWAYS_ON (default)\nMETADATA_BASED\nMETA_DATA_AND_CONTENT_BASED\nJava\ndeinterlaceFilter.setAutoEnable(DeinterlaceAutoEnable.ALWAYS_ON);\nThis table shows the consequences of each value\nParameter\nALWAYS_ON\nMETADATA_BASED\nMETA_DATA_AND_CONTENT_BASED\n(the filter itself)\nDeinterlacing will always be applied as configured\nDeinterlacing might be removed depending on metadata probing\nDeinterlacing might be removed depending on metadata probing and a content analysis. The content analysis is the backup solution if the meta data probing fails.\nmode\nnot touched\nnot touched\nnot touched\nframeSelectionMode\nnot touched\nnot touched\nthis is forced to\nALL\nif the content analysis detects interlaced input\nparity\nnot touched\nif parity is\nAUTO\nthen it will be overwritten with the result of the metadata probing\nif parity is\nAUTO\nthen the parity will be overwritten with the result of the metadata probing. However, if the content analysis detects interlacing then the parity will always be overwritten with the result of the analysis\nAdvice\nfor using\nautoEnable\nIf you know exactly what your input is, and you know how to set\nmode\n,\nparity\nand other settings, then set\nautoEnable\nto\nALWAYS_ON\n(default).\nIf you don't, or want to allow for variable input files, then it's safest to use\nMETA_DATA_AND_CONTENT_BASED\n. This may overwrite other settings if the source file analysis indicates a contradiction.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/4b262bb-image.png",
      "https://files.readme.io/e57aabb-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/setting-up-an-akamai-netstorage-output",
    "title": "Setting Up an Akamai NetStorage Output",
    "text": "Overview\nThe NetStorage service of Akamai provides many ways to interact with it. The best way from our perspective is to use its HTTP API interface. In this tutorial you will learn how it works and what it needs to obtain proper credentials to create an Akamai NetStorage Input/Output in our service.\nThe biggest advantage is the recilience of HTTP requests and the amount of parallel connections you can leverage compared to FTP transfers, where it is capped to 25 connections for one akamai upload account. The\nNetStorage HTTP CMS API\nhowever doesn't has such limits and is therefore well suited to handle many parallel chunked transfers in a fast and reliable manner.\nSo lets look into it how you can configure an Akamai NetStorage to be used for your encodings in our service.\nIMPORTANT\n: This tutorial assumes that you already have access to an Akamai NetStorage and Setup and a dedicated Upload Account for it. If you are missing any of it yet,\nplease read through this tutorial first\non how to create an Akamai NetStorage and Upload Account before you proceed with this tutorial.\nCreate an Bitmovin Akamai NetStorage Output\nTo create a Bitmovin\n\"Akamai NetStorage\" Output configuration\n, that can be used by our encoding service, you need to know the\nHTTP API Key\nand\nUsername\nof your upload account, as well as its\nHost\nURL.\nUse the Dashboard UI\nSelect the\nEncoding\nmenu on the left and go to\nOutputs\n.\nClick on\nCreate\nin the upper right corner of the Outputs overview\nSelect\nAkamai\nas Output type and\nEnter all required fields (please see the image below)\nClick on\nCreate\nUse an Bitmovin API SDK\nEach of our Open API SDK's implements the Bitmovin API, and make it easy to start its integration in your project or use-case. Use them to create reusable output resources to be used for your encodings:\nBitmovin API SDK for Java - Output example\nJava\nAkamaiNetStorageOutput output = new AkamaiNetStorageOutput();\noutput.setHost(AKAMAI_NETSTORAGE_HTTP_API_HOST); // e.g. xxxxx-nsu.akamaihd.net\noutput.setUsername(AKAMAI_NETSTORAGE_USERNAME);\noutput.setPassword(AKAMAI_NETSTORAGE_PASSWORD);\noutput = bitmovinApi.output.akamaiNetStorage.create(output);\nSee all available examples for each of our Bitmovin API SDK's in our\nGH Example Repository\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/e64a53f-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/setting-up-cors-for-your-google-cloud-storage-bucket",
    "title": "Setting Up CORS for Your Google Cloud Storage Bucket",
    "text": "If you already familiar with Google Cloud Services and Tools, like gcloud and/or gsutil, you can also checkout\nGoogle's documentation about CORS\n.\nLogin to your google cloud console:\nhttps://console.cloud.google.com/home\n. Click on \"Activate Google Cloud Shell\" in the upper right corner (see picture below):\nAt the bottom of your window, a shell terminal will be shown, where gcloud and gsutil are already available. Execute the command shown below. It creates a json-file which is needed to setup the cors-configuration for your bucket. This configuration will allow every domain to access your bucket using XHR-Requests in the browser:\necho '[{\"origin\": [\"*\"],\"responseHeader\": [\"Content-Type\"],\"method\": [\"GET\", \"HEAD\"],\"maxAgeSeconds\": 3600}]' > cors-config.json\nIf you want to restrict the access one or more specific domains, add their URL to the array, e.g.:\necho '[{\"origin\": [\"https://yourdomain.com\"],\"responseHeader\": [\"Content-Type\"],\"method\": [\"GET\", \"HEAD\"],\"maxAgeSeconds\": 3600}]' > cors-config.json\nReplace\nYOUR_BUCKET_NAME\nwith your actual bucket name in the following command to update the cors-settings from your bucket\ngsutil cors set cors-config.json gs://YOUR_BUCKET_NAME\nTo check if everything worked as expected, you can get the cors-settings of a bucket with the following command:\ngsutil cors get gs://YOUR_BUCKET_NAME",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/497aec6-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/vp9-presets-live",
    "title": "VP9 Presets",
    "text": "Our VOD VP9 presets can also be used for live streaming workflows. For detailed information, visit our VOD\nVP9 Presets\npage.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/adapting-automatically-to-different-source-files-using-stream-conditions",
    "title": "Adapting Automatically to Different Source Files Using Stream Conditions",
    "text": "Context\nIn an ideal world, as a content publisher, you would only be asked to work with a single type of source file, which would allow you to have a single encoding workflow configured, and complete predictability about what your output streams will look like. However, we all know this: this ideal world doesn't exist. You just have to open a newspaper if you need convincing...\nWhen it comes to video workflows, in many situations the input file might not fit the output profiles. This could cause it to waste computing resources, generate outputs that are not suitable for downstream systems such as players or content partners, or stop your encoding workflow altogether.\nThe traditional way to get around this is to create multiple versions of your workflow, with subtle differences between them to handle the differences in your input file. You would then need on an additional analysis step in the workflow before you start the encoding, to make decisions on what workflow to execute, or to create the whole configuration on the fly. Tools like\nMediaInfo\nor\nffprobe\nwill provide a lot of information and have been used by generations of video developers, but require additional logic to parse and interpret the results in ways that are meaningul when it comes to this logic.\nLuckily, with the Bitmovin encoder's smart defaults, there is a reduced need to go through all this, but when you need that additional level of control, stream conditions will give you all the tools you should need!\nPrinciples\nStream Conditions are a feature in our encoding stack that allows you to pre-configure the behaviour of your encoding based on the input file (or files) that it receives. Before instructions to create streams are passed to the encoder, the pre-defined conditions attached to it are checked against the internal results of an analysis step; if they evaluate negatively, those streams will not get encoded. Furthermore all muxings depending on this streams will also be ignored.\nThis makes it quite easy to only have one encoding configuration, which works with a variety of input files, and to adapt it over time to cover more variation in the input specs.\nBasic usage\nLet's start from a simple script that might generate a fixed bitrate ladder from a source file, with a top rendition at 4K (3280x2160). For the top rendition, the code might look something like this (with your own functions taking care of creating other resources, such as codec configurations, stream inputs, etc. in much the same way as you will typically find\nin our examples\n):\nH265VideoConfiguration videoConfig2160 = createH265VideoConfig(2160, 10_000_000L);\n\nStream stream2160 = new Stream();\nstream2160.addInputStreamsItem(videoStreamInput);\nstream2160.setCodecConfigId(videoConfig2160.getId());\nstream2160 = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream2160);\n\nMuxingStream muxingStream2160 = new MuxingStream();\nmuxingStream2160.setStreamId(stream2160.getId());\n\nMp4Muxing muxing2160 = new Mp4Muxing();\nmuxing2160.addOutputsItem(encodingOutput);\nmuxing2160.setFilename(\"video2160.mp4\");\nmuxing2160.addStreamsItem(muxingStream2160);\nmuxing2160 = bitmovinApi.encoding.encodings.muxings.mp4.create(encoding.getId(), muxing2160);\nThat's all well and good, but what if your input file only has an HD resolution (1920x1080)? You could obvioulsy have another workflow that takes care of those, but what if the only difference between them is a couple of extra renditions for those higher resolutions? Enter stream\nCondition\ns... let's modify the example to check on the file's height, by introducing the following snippet before the call to the API on line 6:\nCondition heightIs2160 = new Condition();\nheightIs2160.setAttribute(\"HEIGHT\");\nheightIs2160.setOperator(ConditionOperator.EQUAL);\nheightIs2160.setValue(\"2160\");\nstream2160.setConditions(heightIs2160);\nNow, the encoder will only create this 2160 stream and the\nvideo2160.mp4\nfile if your source file has a height of 2160 pixels\nIf you source file is of different resolution, that stream won't get created. And to make sure that the muxing associated with that stream does not cause an encoding error, we can also add the following instruction to instruct the encoder to completely drop that muxing:\nmuxing2160.setStreamConditionsMode(StreamConditionsMode.DROP_MUXING);\nThis last step is not strictly necessary however, since it is the default setting on all muxing types anyway.\nCombining conditions\nLet's go a step further. What if in addition to this, you also want to have different output bitrates for that rendition based on the input frame rate? If the frame rate is anything up to 30 fps, you want a lower bitrate than for any source above that. Easy enough! You can combine multiple conditions through logical operators AND or OR, by using conjunctions such as\nAndConjunction\nand\nOrConjunction\nresources. So, here we'll create 2 configurations and 2 streams, one for each set of condition:\nH265VideoConfiguration videoConfig2160_10Mbps = createH265VideoConfig(2160, 10_000_000L);\nStream stream2160_10Mbps = new Stream();\nstream2160_10Mbps.addInputStreamsItem(videoStreamInput);\nstream2160_10Mbps.setCodecConfigId(videoConfig2160_10Mbps.getId());\n\nH265VideoConfiguration videoConfig2160_16Mbps = createH265VideoConfig(2160, 16_000_000L);\nStream stream2160_16Mbps = new Stream();\nstream2160_16Mbps.addInputStreamsItem(videoStreamInput);\nstream2160_16Mbps.setCodecConfigId(videoConfig2160_16Mbps.getId());\n\nCondition heightIs2160 = new Condition();\nheightIs2160.setAttribute(\"HEIGHT\");\nheightIs2160.setOperator(ConditionOperator.EQUAL);\nheightIs2160.setValue(\"2160\");\n\nCondition fpsMin30 = new Condition();\nfpsMin30.setAttribute(\"FPS\");\nfpsMin30.setOperator(ConditionOperator.GREATER_THAN);\nfpsMin30.setValue(\"30\");\nAndConjunction heightIs2160AndFpsUpTo30 = new AndConjunction();\nheightIs2160AndFpsUpTo30.setConditions(Arrays.asList(heightIs2160, fpsMin30));\n\nCondition fpsMax30 = new Condition();\nfpsMax30.setAttribute(\"FPS\");\nfpsMax30.setOperator(ConditionOperator.LESS_THAN_OR_EQUAL);\nfpsMax30.setValue(\"30\");\nAndConjunction heightIs2160AndFpsAbove30 = new AndConjunction();\nheightIs2160AndFpsAbove30.setConditions(Arrays.asList(heightIs2160, fpsMax30));\n\nstream2160_10Mbps.setConditions(heightIs2160AndFpsUpTo30);\nstream2160_16Mbps.setConditions(heightIs2160AndFpsAbove30);\n\nstream2160_10Mbps = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream2160_10Mbps);\nstream2160_16Mbps = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream2160_16Mbps);\nBeyond that, the world is your oyster. You can create more conditions, and even nest multiple conjunctions.\nStream conditions mode\nWe've seen above how to use\nDROP_MUXING\nas\nMuxing.streamConditionsMode\nto ensure that the muxing will not be generated if any of the streams it should contain fails to meet the conditions.\nThere is another option,\nDROP_STREAM\n, which, as the name should make clear, will not drop the muxing, but will attempt to create it without that stream.\nA good use case for this is when handling audio sources. Let's imagine that we want our encoding to be able to handle source files that normally contain a stereo audio track, but may also contain a 5.1 track in addition to it. We want our encoding outputs to reflect that setup, but we don't want to fail if the surround track is missing.\nHere's such a setup. This time we use Python\n(if only because it's a little more compact than Java and this article is getting long...)\nPython\nconfig_video = bitmovin_api.encoding.configurations.video.h264.create(\n    h264_video_configuration=H264VideoConfiguration(bitrate=5_000_000, preset_configuration=PresetConfiguration.VOD_STANDARD))\nconfig_audio_stereo = bitmovin_api.encoding.configurations.audio.aac.create(\n    aac_audio_configuration=AacAudioConfiguration(bitrate=128000, channel_layout=AacChannelLayout.CL_STEREO))\nconfig_audio_surround = bitmovin_api.encoding.configurations.audio.ac3.create(\n    ac3_audio_configuration=Ac3AudioConfiguration(bitrate=384000, channel_layout=Ac3ChannelLayout.CL_5_1))\n\nstream_input_video = StreamInput(input_id=input.id, input_path=input_path,\n                                 selection_mode=StreamSelectionMode.VIDEO_RELATIVE, position=0)\nstream_input_audio_0 = StreamInput(input_id=input.id, input_path=input_path,\n                                   selection_mode=StreamSelectionMode.AUDIO_RELATIVE, position=0)\nstream_input_audio_1 = StreamInput(input_id=input.id, input_path=input_path,\n                                   selection_mode=StreamSelectionMode.AUDIO_RELATIVE, position=1)\n\ninput_stream_exists = Condition(attribute=\"INPUTSTREAM\", operator=ConditionOperator.EQUAL, value=\"true\")\n\nstream_video = bitmovin_api.encoding.encodings.streams.create(\n    encoding_id=encoding.id,\n    stream=Stream(codec_config_id=config_video.id,\n                  input_streams=[stream_input_video]))\n\nstream_audio_stereo = bitmovin_api.encoding.encodings.streams.create(\n    encoding_id=encoding.id,\n    stream=Stream(codec_config_id=config_audio_stereo.id,\n                  input_streams=[stream_input_audio_0],\n                  conditions=input_stream_exists))\nstream_audio_surround = bitmovin_api.encoding.encodings.streams.create(\n    encoding_id=encoding.id,\n    stream=Stream(codec_config_id=config_audio_surround.id,\n                  input_streams=[stream_input_audio_1],\n                  conditions=input_stream_exists))\n\nmuxing = bitmovin_api.encoding.encodings.muxings.mp4.create(\n    encoding_id=encoding.id,\n    mp4_muxing=Mp4Muxing(filename=\"output.mp4\",\n                         outputs=[EncodingOutput(output_id=output.id, output_path=output_path)],\n                         streams=[stream_video, stream_audio_stereo, stream_audio_surround],\n                         stream_conditions_mode=StreamConditionsMode.DROP_STREAM))\nNotice how we use a single type of condition to test whether any of the streams we expect in the source is indeed present. With the last line, we state that should either of those be missing, we should still create a muxing with just those streams that are possible to create. A side effect of this setup is that even if the source file has no audio at all, the muxing will succeed and only contain a video stream. It's up to you whether that's a good thing or not (and to tune the code to your desires).\nI hear you ask: \"But what if our files had tracks containing either stereo, surround or both, and we wanted to only output a single audio output stream, with a preference for 5.1 if it's available in the source?\". We've got you covered! After small alterations to the code above, lines 15 and following become:\nPython\nhas_1_audio_stream = Condition(attribute=\"AUDIOSTREAMCOUNT\", operator=ConditionOperator.EQUAL, value=\"1\")\nhas_2_audio_streams = Condition(attribute=\"AUDIOSTREAMCOUNT\", operator=ConditionOperator.GREATER_THAN_OR_EQUAL, value=\"2\")\nis_stereo = Condition(attribute=\"CHANNELFORMAT\", operator=ConditionOperator.EQUAL, value=\"2\")\nis_surround = Condition(attribute=\"CHANNELFORMAT\", operator=ConditionOperator.EQUAL, value=\"6\")\n\nstream_video = bitmovin_api.encoding.encodings.streams.create(\n    encoding_id=encoding.id,\n    stream=Stream(codec_config_id=config_video.id,\n                  input_streams=[stream_input_video]))\n\nstream_audio_stereo_from_track_0 = bitmovin_api.encoding.encodings.streams.create(\n    encoding_id=encoding.id,\n    stream=Stream(codec_config_id=config_audio_stereo.id,\n                  input_streams=[stream_input_audio_0],\n                  conditions=AndConjunction(conditions=[has_1_audio_stream, is_stereo])))\nstream_audio_surround_from_track_0 = bitmovin_api.encoding.encodings.streams.create(\n    encoding_id=encoding.id,\n    stream=Stream(codec_config_id=config_audio_surround.id,\n                  input_streams=[stream_input_audio_0],\n                  conditions=AndConjunction(conditions=[has_1_audio_stream, is_surround])))\nstream_audio_surround_from_track_1 = bitmovin_api.encoding.encodings.streams.create(\n    encoding_id=encoding.id,\n    stream=Stream(codec_config_id=config_audio_surround.id,\n                  input_streams=[stream_input_audio_1],\n                  conditions=has_2_audio_streams))\n\nmuxing = bitmovin_api.encoding.encodings.muxings.mp4.create(\n    encoding_id=encoding.id,\n    mp4_muxing=Mp4Muxing(filename=\"output.mp4\",\n                         outputs=[EncodingOutput(output_id=output.id, output_path=output_path)],\n                         streams=[stream_video,\n                                  stream_audio_stereo_from_track_0,\n                                  stream_audio_surround_from_track_0,\n                                  stream_audio_surround_from_track_1],\n                         stream_conditions_mode=StreamConditionsMode.DROP_STREAM))\nSummary and additional advantages\nHopefully, you have a good idea of what's possible at this stage. Beyond allowing your workflows to cater for more use cases when it comes to the input files, it is also worth nothing other advantages that may be gained by using stream conditions:\nSimplify your workflow: no need for an extra analysis process in your workflow - the conditions will be evaluated during the encoding process which will only take a few milliseconds\nSpeed up the encoding: no extra time spent encoding streams you don't need\nSimplify maintenance: you can just adapt your encoding script instead of needing multiple versions\nReduce encoding costs: streams are only encoded when they match the conditions\nReduce storage and CDN costs: streams that are not encoded require no storage (duh!)\nAvoid upscaling and quality loss\nAvoid unnecessary audio resampling\nAvoid encoding errors\nYou can also use stream conditions together with most of the features provided by the Bitmovin encoder, such as algorithms like per-title and 3-pass.\nFinally, a word of caution. You should only really use stream conditions when they are warranted. If all you want to achieve is to have the output of your encoding match settings from the input, such as frame rate, resolution, audio layout, it is better to not specify those on your codec configuration. By default, unless specified, the Bitmovin encoder will apply smart defaults!\nReference\nA complete reference of all attributes and values available when using conditions can be found in\nthis article",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/automating-video-editing-with-bitmovin-encoding-api",
    "title": "Automating Video Editing with Bitmovin Encoding API",
    "text": "Overview\nWith the emergence of online streaming platforms, distributing broadcast content on online platforms has become common. However, due to copyright issues or the need to customize content for a specific audience, it is often necessary to edit parts of the content before distributing it on the internet. In this tutorial, we will explore how to use the Bitmovin Encoding API to automate the video editing workflow with its powerful trimming and concatenation methods, enabling efficient and flexible video editing.\nRequirements\nBitmovin Encoder Version: 2.142.0 or higher\nVideo Editing Workflow with Bitmovin API\nIn this section, we will delve into the procedure of automating the video editing process using the Bitmovin Encoding API. The Bitmovin API provides various trimming and concatenation methods that enable you to create a flexible workflow. You can effortlessly trim the video and audio sections as per your requirement, making it easy to comply with copyright laws or customize content for your intended audience.\nWith the recent improvements to the Bitmovin Encoder, it has become highly flexible in terms of video concatenation processing. For instance, it is now possible to replace only certain parts of the video that may pose copyright issues or substitute the audio with separately recorded material while preserving the other side of the input stream.\nLet's take a closer look at the process for replacing a video section and explore the workflow when using the Bitmovin API in more detail. The flowchart below illustrates the processing steps.\nStep 1: Defining Input Using IngestInputStream\nTo begin, you need to define your input streams using the\nIngestInputStream\nclass. This class allows you to specify the input source's stream position to extract a desired stream from your input file. By defining the input streams, you can access the video and audio tracks separately, facilitating their manipulation.\nPython\nmain_video_ingest_input_stream =IngestInputStream(\n    input_id=input.id,\n    input_path=input_path,\n    selection_mode=StreamSelectionMode.POSITION_ABSOLUTE,\n    position=0)\nalt_video_ingest_input_stream = IngestInputStream(\n    input_id=input.id,\n    input_path=input_path,\n    selection_mode=StreamSelectionMode.POSITION_ABSOLUTE,\n    position=0)\nmain_audio_ingest_input_stream = IngestInputStream(\n    input_id=input.id,\n    input_path=input_path,\n    selection_mode=StreamSelectionMode.POSITION_ABSOLUTE,\n    position=1)\nStep 2: Trimming your input content\nOnce you have defined your input streams, you can use the\nTimeBasedTrimmingInputStream\nclass to extract the desired video section. This method enables you to specify the start\noffset\nand the\nduration\ntimes of the content you want to extract.\nIn the following example, we provide a definition that replaces the 30-40 second video segment of a 60-second input file with the 0-10 second section of the\nalt_video_input_stream\n.\nAs there is no trimming required for the audio section, it is defined in its entirety from start to finish. The parameter duration\nnull\nrepresents the duration until the end of the stream in the trimming class.\nPython\n# For video stream\npart1_video_input_stream=TimeBasedTrimmingInputStream(\n    input_stream_id=main_video_ingest_input_stream.id,\n    offset=0,\n    duration=30\n)\nalt_video_input_stream=TimeBasedTrimmingInputStream(\n    input_stream_id=alt_video_ingest_input_stream.id,\n    offset=0,\n    duration=10\n)\npart2_video_input_stream=TimeBasedTrimmingInputStream(\n    input_stream_id=main_video_ingest_input_stream.id,\n    offset=40,\n    duration=None\n)\n# For audio stream\naudio_input_stream=TimeBasedTrimmingInputStream(\n    input_stream_id=main_audio_ingest_input_stream.id,\n    offset=0,\n    duration=None\n)\nStep 3: Generate video and audio stream with ConcatenationInputStream\nAfter extracting your desired video and audio section, you can replace it with a new video using the\nConcatenationInputStream\nmethod. This method allows you to concatenate multiple video sections. You can also specify the position of each video part within the stream to define the order.\nAs for the audio stream, since it is simply copied from the input, we define the entire input as the source and also define it in the\nConcatenationInputStream\nclass.\nPython\n# For video stream \npart1_video_concatenation_input_configuration = ConcatenationInputConfiguration(\n    input_stream_id=part1_video_input_stream.id,\n    position=0,\n    is_main=True)\nalt_video_concatenation_input_configuration = ConcatenationInputConfiguration(\n    input_stream_id=alt_video_input_stream.id,\n    position=1,\n    is_main=None)\npart2_video_concatenation_input_configuration = ConcatenationInputConfiguration(\n    input_stream_id=part2_video_input_stream.id,\n    position=2,\n    is_main=None)\nconcatenation_input_stream=ConcatenationInputStream(\n    concatenation=[part1_video_concatenation_input_configuration,\n                   alt_video_concatenation_input_configuration,\n                   part2_video_concatenation_input_configuration]\n)\n# For audio stream \naudio_concatenation_input_configuration = ConcatenationInputConfiguration(\n    input_stream_id=audio_input_stream.id,\n    position=0,\n    is_main=True)\nFinally, you will need to output your edited content. This process enables you to specify the output codec and muxing format of the edited video, as well as the output file destination. By defining the output stream, you can generate a final edited video that meets your requirements. These steps are common in other workflows as well, so we will omit further details in this tutorial.\nConclusion\nAs illustrated above, with proper preparation of input files and desired editing points, the Bitmovin API can be utilized to automate the entire process of generating output for online streaming.\nAutomating your video editing workflow can save time and reduce the need for human intervention. By utilizing the trimming and concatenation features of Bitmovin Encoding API, video or audio segments can be substituted effortlessly, simplifying the video editing process without compromising the requirement for your target audience. By following the steps outlined in this tutorial, you can prepare an organized content and streamline your video editing process.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/8928c7f-image.png",
      "https://files.readme.io/161e68c-image.png",
      "https://files.readme.io/a7c068a-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/akamai-live-outputs",
    "title": "Akamai Live Outputs",
    "text": "Akamai MSL4 Live Outputs\nAkamai NetStorage Live Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rest-api-services-100-1490",
    "title": "REST API Services 1.0.0 - 1.49.0",
    "text": "1.49.0\nAdded\nAdded additional timestamp properties and\nprogess\ninformation to Encoding object.\nIntroduced polymorphic details endpoints for following resources: codec configs, muxings, inputs, outputs, filters, DRMs.\nChanged\nHLS VOD manifests now by default include the framerate in the master manifest. (requires a correct streamId being set on the HLS streaminfo).\nHLS VOD manifests now include PLAYLIST-TYPE:VOD.\nEnhance error messages for the creation of GCE Accounts.\nFixed\nDefault\nDASH\nand\nHLS\nfailed when any output path contains characters within round brackets (e.g.\nmyfolder(test)\n).\nGo API SDK\nFixed issue that prevented users from creating stream conditions.\nFixed issue that prevented the user from creating resources with properties which were empty strings.\n1.48.0\nAdded\nAdd List of VoD SMOOTH Manifests to StartEncodingRequest for VoD encodings\nSupport for XDCAM output with the following new codecs / muxings:\nH262 / MPEG2 video codec configuration\n, with mandatory preset\nXDCAM_HD_422\nPCM audio codec configuration\nMXF muxing\n1.47.0\nChanged\nMoved the HLS characteristics from the group level to the group element level to facilitate separate characteristics for separate group elements.\n1.46.1\nFixed\nFixed an issue where encodings with configured fallback regions were started on the fallback region even though the preferred region was healthy.\n1.46.0\nAdded\nOCR processing of DVB subtitles is now supported for live encodings with an output using the WebVTT codec and a muxing of either ChunkedText or fMP4\nChunkedTextRepresentation\nfor Segmented WebVtt in DASH Manifests in combination with DVB subtitles\nDvbSubtitleInputStream\nto allow specifying the Dvb subtitle input stream\nSupport for Segmented WebVtt in HLS Manifests in combination with DVB subtitles\nPython API SDK\n:\nWe now also publish our client to\nPyPi.org\nAdded the\nGCE region endpoint\nto specify the network and the subnet for GCE-Connect.\nFixed\nWhen using Input/Output with S3 Role-Based or GCS Service Credentials bucket the closest encoding region is now chosen depending on the region of the bucket\n1.45.0\nAdded\nAdded\nDolby Atmos\nsupport\nInput as ADM or DAMF Dolby Master files\nOutput to fMP4 in DASH/HLS\nSupport for Widevine, PlayReady and FairPlay DRM\nAdded the possibility to have a fixed IP range for GCP-Connect on port 22.\nIf different segment lengths are configured for the same stream a warning is generated which says which segment length is used.\n1.44.0\nAdded\nFor video encodings a display aspect ratio (DAR) for the output can be specified. If the DAR does not match with the width and height, then the sample aspect ratio (SAR) will be adjusted accordingly.\nAuto restart of live encoding on internal encoder error can be enabled via the\nAutoRestartConfiguration\non\nlive encoding start\n.\nAdded the concept of\nfallback regions\n. In case it is not possible to start the encoding on the primary region we retry with the given fallback regions. The failure previously manifested as a\nScheduling failed\nerror.\nAdded support for passthrough of\nDTS audio codecs\n.\nCodec configuration now fails if the following values are set differently for a Dolby Vision stream:\nColorSpace =\nUNSPECIFIED\nColorPrimaries =\nUNSPECIFIED\nColorTransfer =\nUNSPECIFIED\nColorRange =\nJPEG\nMasterDisplay =\nG(13250,34500)B(7500,3000)R(34000,16000)WP(15635,16450)L(10000000,1)\nIngest Input Stream\n,\nDolby Vision Metadata Ingest Stream\n,\nFile input stream\n,\nDVB Teletext input stream\n,\nCEA 608 Input Stream\n, and\nCEA 708 Input Stream\nnow support\ninputPath\nlonger than 255 characters.\nFixed\nFixed an issue where HLS\ncharacteristics\nfield on subtitles was not written into the master manifest file when using\nVTT Media\nIf the input file contained metadata tags with 4 byte chars then the\nStream Input\nendpoint never returned data. This was fixed and the 4 byte chars are replaced with �.\nAdded additional validation for manifest paths.\n1.43.0\nAdded\nCustomers using their own AWS account can now opt-in, so that all HTTP communication on port 9090 and 9999 comes from one specific IP address. This enables a more rigid network security policy where, per default, every IP addressed but one is blocked for HTTP connections.\nEnabled restart of live encodings if the encoder is unresponsive due to an instance failure (hardware, software, network issue etc.). This will lead to the live encoding being started on an instance with a new IP address.\nChanged\nAllow the restart of a finished or error live encoding.\nFurther improved Database queries to increase performance for\nlist calls of configurations\n,\nlist calls of outputs\n,\nlist calls of inputs\nand\nlist calls of filters\nFixed\nEnabled retries for m3u8 manifest uploads when the upload runs into an\ninternal google error\nFixed a display issue with minutes per codec not matching overall minutes\nFixed potential out of memory errors when creating lots of manifests simultaneously\nFixed an issue where HLS characteristics field on subtitles was not written into the master manifest file\n1.42.0\nAdded\nAdded\nstatus\nproperty for\nmanifest listing\ncalls to the OpenAPI Clients.\nFixed\nEncodings with HTTP(s) inputs and servers that don't support HEAD requests might have failed\n1.41.0\nAdded\nImplemented earlier feedback if a specified file doesn't exist on an ftp or http server.\nIf a not supported\npresetConfiguration\nis selected then the request will now fail. This is now enforced for\nh265\nand\nvp9\nas well.\nChanged\nImproved Database queries to increase performance for\nlist calls of configurations\nImproved Database queries to increase performance for\nlist calls of filters\nImproved Database queries to increase performance for\nlist calls of outputs\nImproved Database queries to increase performance for\nlist calls of inputs\nFixed\nFixed incorrect date-time formatting of LiveEncodingStatsEvent of the\nLive Statistics Endpoint\nand\nLive Statistics Events Endpoint\nFor the Live Input Stream Changed Email Notification a fail fast was added if the conditions are not set.\n1.40.0\nAdded\nAdded support for\nVOD_SPEED\n,\nVOD_STANDARD\nand\nVOD_HIGH_QUALITY\npresets for\nVP9 codec configuration\n.\nChanged\nPHP API SDK\n:\nUpdated\nGetting Started Guide\nto use our new API SDK.\nFixed\nFor HLS a fail fast was added if the uri of a VTT Media starts with a\n/\n.\nOn DASH manifest creation including Progressive WebM muxings the width and height attributes are now set on Representation level instead of AdaptationSet level.\nImproved\nencoding stop call\nto handle cases in which the encoder is unresponsive.\nList all Codec Configurations\nendpoint:\nH264 and H265 configurations now include the formerly missing property\ncolorConfig\n.\nThe call does not fail anymore if the response includes a WebVTT configuration.\n1.39.0\nAdded\nAdded the\ninformation\nendpoint for fMP4 muxings.\nAdded\nList DRMs\nendpoint for Progressive WebM muxings\nPHP API SDK\n:\nAfter steady improvements and thorough testing over the last few months, we decided it's time to remove the alpha tag. There will be no further breaking changes.\nCreated\n10 examples\nshowcasing the usage of our new PHP API SDK.\nFixed\nLive2Vod manifests can now be successfully created, after the livestream has been restarted\nJava API SDK\n:\nBitmovinException now also contains error information (error code & message) if the log level is not set to\nFULL\n1.38.0\nAdded\nAdded\ndaily statistics endpoint\nfor live encodings\nAdded\nsourceChannels\nto\nAudioMixInputStreamChannel\nfor\naudio mix input streams\nto allow to mix and merge channels\nAdded the possibility to select subtitle streams via\nselectionMode\nSUBTITLE_RELATIVE\n. See\nstreams\nAdded Support for Service Account based GCS\ninputs\nand\nouputs\nAdded H.264 preset VOD_QUALITY\nChanged\nA maximum of 5 webhooks per webhook type and resource can be configured\nFixed\nThe\nignoredBy\nproperty will be set correctly for text muxing streams\n1.37.0\nAdded\nImplemented secure transfer support for Azure Blob Storage\nImplement Dash Segment Timeline for VOD\nC# API SDK\n:\nAfter steady improvements and thorough testing over the last few months, we decided it's time to remove the alpha tag. There will be no further breaking changes.\nCreated\n10 examples\nshowcasing the usage of our new C# API SDK.\nFixed\nFixed an internal 500 error which was caused by an invalid search parameter when querying the\nGET encoding\nendpoint\n1.36.0\nAdded\nAdded support for\nSegment List\nfor MP4 Dash On-Demand Manifests\nAdded support for\nAES-128 Encryption for Progressive TS\nmuxings\nAdded support for\nRole properties\non Dash On-Demand manifests\nPHP API SDK\n:\nExtended error messages to show all relevant details at a glance\nAdded\nConsoleLogger\nand better request / response logging\nEnabled configuration of\ntenantOrgId\nto be able to connect to the API as a tenant of an organization.\nFixed\nPHP API SDK\n:\nEmpty objects in request bodies were serialized as empty arrays, which resulted in\n400 - Bad Request\nresponses in the past.\n1.35.0\nAdded\nAdded an input file check for S3, Azure and GCP before starting the encoding\nSupport for\nprogressive WebM DASH manifests\nAdded condition based\nDASH default manifest\ncreation. For example to create a manifest that only contains video streams up to a specific resolution. This can be used with the new v2 version of the default manifest. V2 is considered experimental and still subject to change. Breaking changes will be announced in the release notes.\n1.34.0\nAdded\nDash\ndefault manifest V2\ngroups video representations with the same encryption into the same adaption set.\nCEA 608/708 subtitle passthrough for\nH265 codec configurations\nvia property\ncea608708SubtitleConfig\n.\nAdded\nsshPort\nand\nfaspPort\nto\nAspera inputs\nto be able to configure non-standard ports for session intialization and data transfer.\nChanged\nAdjusted\nH265 codec configuration\npreset\nVOD_SPEED\nto better fit in between\nVOD_STANDARD\nand\nVOD_HIGHSPEED\n.\nFixed\nFixed the handling of an edge case, that resulted in encodings failing with the error message\nScheduling failed\n.\n1.33.0\nAdded\nPython API SDK\n:\nAfter steady improvements and thorough testing over the last few months, we decided it's time to remove the alpha tag. There will be no further breaking changes.\nCreated\n10 examples\nshowcasing the usage of our new Python API SDK.\nFixed\nImprove DASH Manifests compatibility for Vtt Representations with certain players where\nmediaPresentationDuration\nwas previously set to\n0\nFixed invalid return types in the http response when calling the\nSPEKE Get endpoint\n1.32.0\nAdded\nChanged default watermark of trial encodings.\nFixed\nFixed bug which allowed failed encodings to be started again.\nFixed a bug which prevented status updates for encodings with an encoder version older than v2.2.0.\nResolved problem which prevented encoding big-sized input files to multiple progressive muxings.\n1.31.0\nAdded\nAdded\nImage Overlay\nfor live encodings\nSupport concatenation for multiple resolution inputs\nSpecify\nAspectMode\nwhen adding a concatenated input stream\nFixed\nA 500 error has been returned when adding an invalid DRM configuration to a muxing instead of client error\n1.30.0\nAdded\nImproved validation messages for invalid Dolby Vision configurations, like configuring Dolby Vision metadata on an audio stream.\nAdded\nsegmentsMuxed\nto\nWebmMuxing\nwhich contains the number of segments that have been encoded.\nFixed\nDefault HLS manifest creation does not fail anymore, when SPEKE config contains incompatible DRM systems.\nHLS manifest creation does not fail anymore for big inputs and huge configurations (~20 streams and more).\nFixed the duration number format of very short HLS segments.\n1.29.0\nChanged\nDuration\nStream Input Details\nwas incorrectly set as Integer instead of Double in documentation.\nFixed\nConverted SCC to WebVTT now succeeds when creating HLS manifests\n1.28.0\nAdded\nAdded new\nH264 codec configuration presets\ntargeted for Live encodings:\nLIVE_VERYLOW_LATENCY\n,\nLIVE_ULTRAHIGH_QUALITY\n,\nLIVE_LOWER_LATENCY\n,\nLIVE_STANDARD\n,\nLIVE_VERYHIGH_QUALITY\nand\nLIVE_ULTRAHIGH_QUALITY\nImproved resilience against\nDocker Hub registry outages\n.\nChanged\nTweaked two\nH264 codec configuration presets\ntargeted for Live encodings:\nLIVE_LOW_LATENCY\nand\nLIVE_HIGH_QUALITY\nFixed\nSmooth client manifest audio track names did not match the track names used in the server manifest in some cases.\nFixed very verbose error messages in dashboard for specific muxing errors.\n1.27.0\nAdded\nDolby Vision - necessary metadata can be provided via\nsidecar file\nor by pointing to an\nembedded input stream\n.\n1.26.0\nAdded\nAdded possibility to add (black) padding sequences (\nconcatenation -> paddingBefore / paddingAfter\n) between input streams inside a\nconcatenation input stream\nJS/TS API SDK\n:\nCreated\n10 examples\nshowcasing the usage of our new JS/TS API SDK\n1.25.0\nAdded\nAdded integration for Encoder Key Exchange\nSPEKE\nfor the muxings:\nWebM\n,\nFMP4\n,\nCMAF\n,\nTS\n,\nprogressive TS\n,\nMP4\nAdded support for segmented\nWebVTT\nfor the\nHLS\nmanifest.\nChanged\nIf a\npresetConfiguration\nwas set it will be returned when the resource is fetched:\nh264\n,\nh265\n,\nVP9\nFixed\nFixed a bug which caused the\nstatistics label call\nto fail.\nFixed a bug that produced invalid files if a TS muxing with HEVC was used.\n1.24.0\nAdded\nCustomers can now run encodings in their\nown Google Compute Engine (GCE) account\n.\nAdded\nConform filter\nwhich allows for small changes of the video frame rate where the playback speed of video as well as audio will be adapted instead of frame interpolation.\nPossibility to\nadd a video language tag\nvia the stream metadata (currently supported for MP4 muxings).\nJS/TS API SDK\n:\nAfter steady improvements and thorough testing over the last few months, we decided it's time to remove the alpha tag. There will be no further breaking changes.\nPHP API SDK\n:\nThis is the first version of our PHP API SDK and new versions will be included in future releases. We would be happy to get early feedback.\nChanged\nIf upload of a DASH manifest fails for one output, we now do not skip other configured outputs.\nImproved the API documentation for the\nScale filter\n.\nC# API SDK\n:\nImproved error messages to give more detail what went wrong.\nJS/TS API SDK\n:\nImproved logger interface\nImproved error messages to give more detail what went wrong.\nFixed\nFixed generation of\ndefault Smooth manifests\nfor Per-Title workflows.\nC# API SDK\n:\nUnknown enum values are now deserialized to null instead of causing an exception.\nJava API SDK\n:\nFixed rare case where a network issue could lead to throwing a\nFeignException\ninstead of a\nBitmovinException\n.\nPython API SDK\n:\nFixed serialization of enum lists.\n1.23.0\nAdded\nAdded dynamic scaling of\nwatermarks\nwith new properties\nwidth\nand\nheight\nFail fast if more than one codec is specified for Per-Title encodings\nFail fast if CencDrm is used together with Fair Play and\nivSize\nis set to\nEIGHT_BYTES\nAdded option to exclude region for\nWebVTT\nconversions (\nignoreRegion\n)\nAdded option to auto detect interlaced content when using\ndeinterlace filter\n(\nautoEnable\n)\nAdded SBR\nsignaling\nto\nHE-AAC-V1\nand\nHE-AAC-V2\nAdd\ncutoffFrequency\nfor\nAAC\n,\nAC3\n,\nEAC3\nChanged\nJS/TS API SDK\n:\nThe discriminator property of polymorphic models is now an enum.\nModel constructors can now also be called without parameters.\nFixed\nDefault DASH manifest\ncreation may fail when using WebM, DRM and Per-Title\nDefault HLS manifest\nmay have failed when only manifest name is set\n1.22.0\nAdded\nImproved ramp-up phase for huge encoding batches\nJava API SDK\n:\nCreated\n10 examples\nshowcasing the usage of our new Java API SDK\nJS/TS API SDK\n:\nAll methods are now compatible with plain objects as well as class instances, which makes it easier to use our client in combination with\nRedux\nPython API SDK\nExtended error messages to show all relevant details at a glance\nFixed\nSupport for watermark image URLs which have more than 255 characters\nFixed issue where a PSSH value inside a PlayReady DRM configuration was not written to the smooth streaming manifest\nFixed problem when using a default HLS manifest in common with a per title encoding including HLSv3 muxings\nJS/TS API SDK\n:\nFixed (de-)serialization of polymorphic models, discriminator is now part of the typed models and of the serialized request body\n1.21.1\nFixed\nFixed an issue where the codec configuration level\nencodingMode\nwas ignored for certain Per-Title encodings\n1.21.0\nAdded\nWebVTT\n: Add optional\nhh\nfor timestamps less than 1 hour (\nappendOptionalZeroHour\n)\nFixed\nWhen using DASH On-Demand manifests and CC to VTT conversion, the resulting manifest might have an invalid\nmediaPresentationDuration\nFixed an issue where Webhook might not be fired for Manifest Finished event\nWhen using DASH On-Demand with multiple video representations the adaptation id might be not compliant\n1.20.0\nAdded\nImproved the\nsearch capabilities of encodings\nand allow to search for labels\nReduced delay for allowlisting new domains for a player license to max. 30 seconds\nUsed\nencoding mode\nis now set in property\nselectedEncodingMode\n. This is especially useful when starting an encoding with encoding mode\nSTANDARD\nUsed\nencoder version\nis now set in property\nselectedEncoderVersion\n. This is especially useful when starting an encoding with encoder version\nBETA\nor\nSTABLE\nAdded property\npriority\nto order\nSmooth Tracks\nin the manifest\nJava API SDK\n:\nConfiguration option\nheaders\nenables customization of HTTP headers sent with every request\nChanged\nJS/TS API SDK\n:\nExtended error messages to show all relevant details at a glance\nRenamed config option\nadditionalHeaders\nto\nheaders\nto be consistent with other API SDKs\nFixed\nMultiple video representations have not been added to the DASH manifest when using DASH.ON_DEMAND with progressive MP4\nWhen using\nHLS_BYTE_RANGES\nMP4 progressive muxing, the targetduration from audio and video playlist might have been off by 1 second\nJava API SDK\n:\nUnknown enum values received from our API will now be deserialized to\nnull\ninstead of throwing an exception\n1.19.0\nAdded\nAllow searching for\nencodings\nby name (exact, startsWith)\nMade\nexternalId\nfor S3 role-based\ninputs\nand\noutputs\noptional\nAdded fail-fast when trying to\ncreate a DASH-Manifest fMP4 Representation\nwithout specifying the\ntype\nReduced delay for allowlisting new domains for a player license to max. 1 minute\nPrevent rare occurences of encodings staying in QUEUED for an unexpected long time\nReduced manifest creation errors because of eventual consistency problems\nJS/TS API SDK\n:\nAdded exports for models\nImproved type support when constructing models\nAdditional headers can be set during configuration\nAdded lambda syntax for query parameters\nPython API SDK\n:\nAdded Python 2 compatibility\nAdded code documentation to improve usability of the client, especially when using Python IDEs\nChanged\nPython API SDK\n:\nChanged package name from\nbitmovin\nto\nbitmovin_api_sdk\nto prevent naming conflicts when using the old and new client simultaneously\nForce keyword arguments rather than positional arguments when instantiating models\nFixed\nFixed missing closed caption accessibility tags on adaptation sets for VoD DASH manifests\nFixed default DASH manifest creation to only include supported muxing types\nDefault value of an\nInput\nStreams\nselectionMode\nproperty is now set to\nAUTO\nFixed error which caused that Live Encodings could not be started with\nUDP Multicast inputs\n1.18.0\nAdded\nCEA-\n608\n/\n708\nto\nSidecar\nWebVTT\nSidecar TTML\nto\nSidecar\nWebVTT\nAdded\nvariableMuxRate\n,\ninitialPresentationTimeStamp\nand\ninitialProgramClockReference\nto\nBroadcast TS muxing\nAdded generation of\nBIF\noutput\nAdded additional properties for\nprogramNumber\n,\npmt\n, and\npcr\nto\nTS muxing\nFixed\nFixed serialization of date fields in the OpenAPI clients\nHandle response of empty body objects gracefully in all OpenAPI clients\nDotNet API SDK\n: enum values are now null by default\n1.17.2\nFixed\nJavaScript Client\n: Fixed TypeScript to ES5\nenum\ntranspilation\n1.17.1\nFixed\nGo Client\n: Fixed serialization issue for empty body POST requests\n1.17.0\nAdded\nAdded parameter\ninterval\nto the\nThumbnails endpoint\nto create thumbnails every x seconds\nAdded validation for DRM and\nMP4\nwith\nHLS_BYTE_RANGES_AND_IFRAME_PLAYLIST\nmanifest type\nAdded\nwebhooks for manifest\nFixed\nManifests that have been created using the\nDefault Manifest endpoint\ncan now be successfully\nretrieved\nby searching for\nencodingId\n1.16.0\nAdded\nAdded endpoint to return all error codes and descriptions of the API\nFRAME-RATE\nand\nVIDEO-RANGE\ncan be added to an\nHLS stream\nby enabling\nforceFrameRateAttribute\n/\nforceVideoRangeAttribute\nAdded additional codec settings to\nVP9\n:\nerrorResiliencyEnabled\n,\nclientBufferSize\n,\nclientInitialBufferSize\n,\nbiasPct\n,\ncpuUsed\n,\nautomaticAltRefFramesEnabled\n,\ntargetLevel\n,\nrowMultiThreadingEnabled\nFixed\nDefault manifests can now be used during the start of an encoding to create preview manifests\nSetters for the SDK clients have been missing documentation\n1.15.0\nAdded\nAdded support for\nRedundant SRT Input\n(\nbackupSrtInputs\n)\nAdded single pass\nEBU-R128 Audio Normalization\nAdded\nopacity option to watermarks\n(\nopacity\n)\nAdded support for AWS region\nEU_WEST_2\nAdded\npresets for H265\n(\npresetConfiguration\n)\nImproved output validation for Akamai MSL by checking if at least one manifest is on the same output as the muxings\nFixed\nTHREE_PASS\nencodings with big input files might fail due an out of disk error\n1.14.1\nAdded\nProperty\nmode\nto the Dash manifest\nAdd fMP4 representation\ncall, which gives you more control over where segment templates are generated\n1.14.0\nAdded\nFail-fast when Per-Title minBitrateStepSize is <= 1.0\nSetup role-based S3\ninputs\nand\noutputs\nwith our API\nReordered elements (QualityLevel and c elements) in our Smooth manifest to ensure playback on additional devices\nFlag\nwriteDurationPerSample\nin\nfMP4 muxing\nto enable writing the duration per sample into the sample entry in the Track Fragment Run Box\nEndpoint that returns\nerror definitions\nof our encoding product\nEndpoint for (segmented)\nWebVTT sidecar files\n1.13.1\nFixed\nHLS manifest can now be created containing both, Progressive TS and MP4 muxings\n1.13.0\nAdded\nObject detection\nfor encodings, that uses our new machine-learning technology\nCMAF muxing\nfor Live and VoD workflows\nGet PSNR values per segment with our\nnew QC Api\nAdded ability to\ntransfer sidecar files\n(like WEBVTT subtitles)\nNew Output\nAkamai Media Services Live\n(MSL)\n1.12.0\nAdded\nIntroduced codec configuration presets for\nH264\n,\nH265\nand\nVP9\nAdded new Google regions:\nASIA_EAST_2\n,\nASIA_NORTHEAST_1\n,\nASIA_SOUTH_1\n,\nASIA_SOUTHEAST_1\n,\nAUSTRALIA_SOUTHEAST_1\n,\nEUROPE_NORTH_1\n,\nEUROPE_WEST_2\n,\nEUROPE_WEST_4\n,\nNORTHAMERICA_NORTHEAST_1\n,\nSOUTHAMERICA_EAST_1\n,\nUS_WEST_2\n,\nUS_EAST_4\nAdded character encoding to\nSRT burn-in subtitles\nEnabled HMAC signatures on Webhooks\nFeature parity between webhooks and email notifications\nFail-fast when\nWatermark filter image\nis not set\nFixed failing Per-Title encodings for trial users, caused by the auto-created Watermark filter\n1.11.0\nAdded\nAdded\n{rand_chars}\nplaceholder for\nsegmentNamingTemplate\nand\ninitSegmentNameTemplate\nto generate a random character sequence. This is especially useful when restarting a live stream and having storage / CDN caching enabled.\n1.10.0\nAdded\nAdded\nsampleAspectRatioNumerator\nand\nsampleAspectRatioDenominator\nto Scale Filter\n1.9.0\nAdded\nImplemented a route to create default manifest for encodings\nAdded support to cancel queued encodings\nImproved error messages when putting video filters on audio streams\nAdded fail-fast when adding manifest without configured output\nAdded fail-fast for configuring multiple streams with MP4 muxing and DASH_ON_DEMAND manifest type is set\nImproved response time for manifest queried by encoding id\nAdded fail-fast for H264+FairPlay+fMP4 as this combination could lead to unencrypted output\n1.8.0\nAdded\nAdded validation for unspported thumbnail file extensions\nAdded validation for Broadcast TS muxings to check if the muxrate is below the sum of bitrates of the stream\n1.7.0\nFixed\nHLS manifest creation might fail when using stream conditions where not all streams are ignored\n1.6.0\nAdded\nAdded support to add streams or thumbnails in parallel to a stream\nAdded an endpoint to get the HLS media type\nAdded fail fast when the\nrate\nof a video configuration is set to 0 (0 fps)\nChanged\nFixed an issue where the stream condition mode of a Broadcast TS muxing was not correctly propagated to the encoder\nFixed an issue where the statistic endpoint returns null dates for live encodings\nFixed an issue where the daily statistics endpoint returns an empty result when no start and end date was set\n1.5.0\nAdded\nAdded support to add streams or thumbnails in parallel to a stream\n1.4.0\nAdded\nAdded validation that audio filters may only be applied to audio streams\nAdded validation that if AV1 is used in progressive WebM muxing, no other streams are allowed in that muxing\nSupport for filtering when listing muxings for stream mode\nFixed\nFixed an issue where the start encoding call fails when the PSSH of DRM was longer than 255 characters.\n1.3.0\nAdded\nAdded additional routes for live statistics to query events and streams separately\nAllow retrieval of encoding start information even when the encoding is not in\nRUNNING\n, but also in\nQUEUED\n,\nFINISHED\nor\nERROR\nstate\n1.2.0\nAdded\nAdded an endpoint for prewarming deployments on on-premise Kubernetes clusters\n1.1.0\nAdded\nSupport for CencDRM PSSH data greater than 255 characters\nImproved error message when adding invalid codec configurations\nRetrieve start encoding configuration after an encoding has been started\n1.0.0\nAdded\nCheck S3 and GCS access rights when creating the input / output\nImproved error message when adding invalid codec configuration settings",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/editing-codec-configurations",
    "title": "Editing Codec Configurations",
    "text": "No, codec configurations and other resources can't be edited. Therefore, you would have to create a new codec configuration and delete the previous one accordingly.\nHINT:\nYou can apply\nStream Conditions\nto a stream and its attached codec configuration, in order to either use or ignore a specific stream during the encoding process, if the condition is fulfilled based on the result of the input file analysis. This can be used to prevent upscaling or ignore certain streams and their audio codec configuration, if there is no audio track available in the input file. More details are available in our blog post\nhere\n.\nExamples:\nJava API SDK -\nExample\nJavascript API SDK -\nExample\nThese and other examples for all our Bitmovin API SDK's can be found in our\npublic Github example repository!",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-bitmovin-cloud-connect-with-gcp",
    "title": "Using Bitmovin Cloud Connect with GCP",
    "text": "This document explains how to set up Bitmovin Encoding on Google Cloud Platform (GCP) infrastructure so that the Bitmovin platform can run encoders using the Google Compute Engine (GCE) API.\nThe instructions in this document for the\nREST API Services\napply to live encoding and file-based encoding. For a complete list of formats and input types, see the\nBitmovin website\n.\nPrerequisites\n🚧\nActivation required\nThis feature requires a commercial agreement and needs to be specifically activated for a Bitmovin account, it is not available by default. You will not be able to complete the configuration below without this activation.\nA Bitmovin account enabled for usage of Cloud Connect\nIf you want to use Cloud Connect with a sub organization, this sub organization must be enabled for usage of Cloud Connect\nA\nGoogle Cloud account\nA\nGoogle Cloud project\nwith enabled\nCompute Engine API\nIt is recommended to have a dedicated project for encoding with Bitmovin\nConfigure your GCP account\nIn this section, you will create a separate resource group that will be used by the Bitmovin platform when interacting with your GCP infrastructure. Also, we will create the appropriate infrastructure setup to enable Encoding jobs.\n📘\nOptional: Configure with Terraform\nIf you prefer infrastructure-as-code, Bitmovin AWS Cloud Connect is also configurable using Terraform scripts:\nBitmovin Terraform Cloud Connect\nCreate a service account\nThe service account is used by the Bitmovin platform to get access to your GCP account and create compute resources to run parts of the the encoding service within it, including starting and stopping GCE instances.\nOpen the \"IAM & Admin >\nService Accounts\n\" settings in the Google Cloud console:\nhttps://console.cloud.google.com/iam-admin/serviceaccounts\nSelect the project you want to use for encoding with Bitmovin, then click the \"\nCreate service account\n\" button\nIn \"Service account details\" (Step 1), provide a \"\nService account ID\n\", then click the \"\nCreate and continue\n\" button\nIn \"Grant this service account access to project\" (Step 2), select the \"\nCompute Instance Admin (v1)\n\" role, then click the \"\nContinue\n\" button\n📘\nOptional: Additional Permissions for reserving Static IP Addresses\nIf you are planning to use the\nStatic IP Addresses Feature\nof the Encoder you also need to add the \"\nCompute Network Admin\n\" role.\nIn \"Grant users access to this service account\" (Step 3), click the \"\nDone\n\" button\nClick on the \"\nEmail\n\" or your newly created service account\nIn the \"\nKeys\n\" tab, select \"\nAdd key\n\" and click \"\nCreate new key\n\"\nSelect \"\nJSON\n\" and click the \"\nCreate\n\" button\nThis will download a JSON private key file containing your private key and other information, required in later steps\nConfigure a VPC (optional)\nBy default, the Bitmovin platform assumes that you are running instances in an\nauto mode VPC\nnamed \"default\".\nIf you prefer creating a specific custom mode VPC for your project, make a note of the VPC name and subnet(s) that you create. Follow the Google \"\nCreate and manage VPC networks\n\" docs for your custom configuration.\nCustom VPCs are supported from Encoder v2.51.0 and above\nConfigure and create firewall rules\nFirewall rules enable network access into your VPC and to the GCP infrastructure in particular.\nOpen the \"Network Security >\nFirewall policies\n\" settings in the Google Cloud console:\nhttps://console.cloud.google.com/networking/firewalls/list\nUnder the \"\nVPC firewall rules\n\" section, make sure the existing firewall rules are configured according to the following tables by clicking on the respective role, then click the \"\nEdit\n\" button:\nKey\nValue\nName\ndefault-allow-internal\nDescription\nFor communication between the session manager VM instance and its instance manager VM instances\nSource filter\nIPv4 ranges: 10.0.0.0/8 (or add all VPC Subnets)\nProtocols and ports\nTCP: 0-65535\nUDP: 0-65535\nOther: icmp\nKey\nValue\nName\ndefault-allow-ssh\nDescription\nFor incoming commands from the Bitmovin API to control the encoding\nSource filter\nIPv4 ranges: 104.199.97.13/32, 35.205.157.162/32\nProtocols and ports\nTCP: 22\nAdditionally, create new rules for each of the following tables by clicking the \"\nCreate firewall rule\n\" button:\nKey\nValue\nName\nencoder-service\nDescription\nFor communication with the service that manages the encoding\nSource filter\nIPv4 ranges: 104.199.97.13/32, 35.205.157.162/32\nProtocols and ports\nTCP: 9999\nKey\nValue\nName\nencoder-service-https\nDescription\nFor HTTPS communication with the service that manages the encoding\nSource filter\nIPv4 ranges: 104.199.97.13/32, 35.205.157.162/32\nProtocols and ports\nTCP: 9443\n📘\nLive encodings\nAdditional firewall rules are required if you are encoding live streams transported over RTMP, SRT or Zixi.\nAdditional inbound rules for RTMP live streams\nKey\nValue\nName\nrtmp-listener\nDescription\nFor RTMP live streams\nSource filter\nIPv4 ranges: 0.0.0.0/0 (or the specific set of addresses where streams will originate from)\nProtocols and ports\nTCP: 1935\nKey\nValue\nName\nrtmps-listener\nDescription\nFor RTMPS live streams\nSource filter\nIPv4 ranges: 0.0.0.0/0 (or the specific set of addresses where streams will originate from)\nProtocols and ports\nTCP: 443\nAdditional inbound rules for SRT live streams\nKey\nValue\nName\nsrt-listener\nDescription\nFor SRT live streams\nSource filter\nIPv4 ranges: 0.0.0.0/0 (or the specific set of addresses where streams will originate from)\nProtocols and ports\nTCP: 2088\nUDP: 2088, 2089, 2090, 2091\nAdditional inbound rules for Zixi live streams\nKey\nValue\nName\nzixi-listener\nDescription\nFor Zixi live streams\nSource filter\nIPv4 ranges: 0.0.0.0/0 (or the specific set of addresses where streams will originate from)\nProtocols and ports\nTCP: 4444\nConfigure your Bitmovin account\nBefore you continue, make sure you have collected the following information from your GCP account:\nFrom the JSON private key file\nProject ID\nClient email\nPrivate key\nFrom your (custom) VPC:\nhttps://console.cloud.google.com/networking/networks/list\nNetwork ID\n- requires the full GCP network path e.g.\n/global/networks/your-custom-vpc\nSubnet ID\n- requires the full GCP subnet path e.g.\n/regions/your-region/subnetworks/your-custom-subnet\nLink your GCP project\nTo enable your Bitmovin account to run encodings in your GCP project, you need to link it with Infrastructure and Region Settings objects.\nOpen the\nBitmovin Dashboard\n:\nhttps://dashboard.bitmovin.com/\nGo to \"VOD/LIVE Encoding >\nCloud Connect\n\"\nClick the \"\nAdd infrastructure account\n\" button, select the \"\nGoogle Cloud Platform\n\" option, click the \"\nNext\n\" button\nProvide a \"\nName\n\" of your choice for your project, fill in the\nService Account Email\n,\nProject ID\nand\nPrivate Key\n, click the \"\nNext\n\" button\nNote:\nYou can also upload/select the JSON private key file, which you created earlier via the GCP console or the Terraform script.\nSelect the appropriate \"\nCloud Region\n\", fill in the\nNetwork\nID and\nSubnet ID\n, click the \"\nNext\n\" button\nGetting access to MIs\nThe Bitmovin platform uses private machine images (MIs) from which to create encoder instances in GCE. After you have created all infrastructure objects in the account (or accounts) that you need, ask your Bitmovin technical contact to whitelist access to the MIs for your specific GCP service account.\nYou will need to provide:\nthe\nClient email\nof your GCP service account, and\nthe\nOrganization ID\nof your Bitmovin account (or email address)\nRun encoding jobs in GCP\nAfter configuration has been completed, you will be able to run encoding jobs in your own GCP project. To do so, use the Bitmovin API client SDKs to submit encoding jobs, in the same way as you would do for encodings running in the Bitmovin Managed Cloud service. The only difference is that you need to specify the new infrastructure instead of public cloud regions.\nHere is a Python snippet demonstrating how to link your encoding to your infrastructure.\nPython\n# ID of the Infrastructure object\n    infra_id = ‘<infrastructure_id>’ \n    \n    # GCP region of the GCP-connect setup\n    infra_region = CloudRegion.GOOGLE_EUROPE_WEST_1\n    infrastructure = InfrastructureSettings(infrastructure_id=infra_id, \n                            cloud_region=infra_region)\n    \n    encoding = Encoding(name='gcp connect encoding',\n        cloud_region=CloudRegion.EXTERNAL,\n        infrastructure=infrastructure,\n        encoder_version='STABLE')\nSub Organizations\nIf you have set up your infrastructure in a sub organization, you must tell the Bitmovin API that you want to run the encoding in that sub organization. Thus, in addition to the code snippet above, make sure to set the\ntenant_org_id\nalongside the\napi_key\nin the\nbitmovin_api\nobject:\nPython\n# ID of the sub organisation you added the infrastructure to\n    organisation_id = '<sub_organisation_id>'\n\n    bitmovin_api = BitmovinApi(api_key=config_provider.get_bitmovin_api_key(), \n                       tenant_org_id=organisation_id,\n                       logger=BitmovinApiLogger())\nResource Quotas\nIf you want to run several encodings in parallel, then the default quota limits may not be sufficient. In that case, you will have to request limit increases for the following quotas in your regions, through the\nQuotas page\nin the Cloud Console.\nFor the limits to request we will be using these variables:\nVariable name\nExplanation\n(maximum number of encodings)\nThe maximum number of parallel encodings the infrastructure must be able to run. Typically this is the number of encoding slots assigned to the Bitmovin account or sub-org associated with the infrastructure.\n(maximum number of instances per encoding)\nThe number of instances used by one encoding.\nThis number varies depending on the input file size and the number and data rate of the encoder representations. However, we recommend to use 60 as the maximum number of instances per encoding when getting started and to increase this limit if it proves insufficient.\nFor Dolby Vision encodings and conversions, starting from Encoder\nv2.208.0\n, the maximum number of instances have been increased to\n- 100 instances if the\nencodingMode\nconfigured is\nSTANDARD\n(which currently maps to\nTWO_PASS\n),\nSINGLE_PASS\nor\nTWO_PASS\n- 200 instances if the\nencodingMode\nconfigured is\nTHREE_PASS\nUsing the variables above, please request the following limits:\nQuota name\nLimit to request\nIn-use IP addresses\n(maximum number of encodings) * (maximum number of instances per encoding)\nCPUs\n(maximum number of encodings) * 8\nPreemptible CPUs\n(maximum number of encodings) * (maximum number of instances per encoding) * 8\nPersistent Disk SSD (TB)\n(maximum number of encodings) * 0.5 + ((number of instances) * (number of encodings)) * 0.05\nVM instances\n(maximum number of encodings) * (maximum number of instances per encoding)\nThe values above assume 8-core instances. If you believe that your use case requires instances with a different number of cores, this number may need to be increased after discussion with your Bitmovin team.\nThe maximum number of instances needed depends on the maximum number of parallel encodings multiplied by the maximum number of instances for one encoding. The number of instances used by one encoding varies depending on the input file size and the number and data rate of the encoder representations.\nGenerally, it is a good idea to multiply the expected limit calculated for your current situation by 2, to have some margin in case you need to ramp up.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/acf3393-image.png",
      "https://files.readme.io/fc26b6c-image.png",
      "https://files.readme.io/e07fc1b-image.png",
      "https://files.readme.io/e84ba4e-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/srt-api-configuration",
    "title": "SRT API Configuration",
    "text": "Introduction\nThe Bitmovin cloud encoding service is a powerful tool for live streaming, and our API makes it easy to implement. This tutorial concentrates on feeds contributed with the SRT protocol.\nSRT is a transport protocol, based on UDP, that enables the secure, reliable transport of data across unpredictable networks, such as the Internet. While any data type can be transferred via SRT, it is particularly optimized for audio/video streaming. Initially developed by Haivision Systems Inc., the SRT protocol was released as open source in April 2017 in partnership with Wowza Media Systems Inc.\nThere are basically 4 steps involved when it comes to using SRT with our live streaming service in the cloud.\n1. Ingest SRT Stream to our Live Encoder\nUsually a mezzanine or \"contribution\" encoder that is processing the live signal will transcode this signal to a high quality mezzanine format and ingest it at the SRT ingest point in our live encoder.\nIn this tutorial, the contribution encoder will act as an SRT Caller, while our live encoder will act as an SRT Listener which waits for a connection from the Caller.\nIf your contribution encoder supports SRT Caller mode, you stream its live stream directly to our live encoder. If your contribution encoder supports only standard protocols such as RTMP, you can send its RTMP output live stream to a Wowza Media Server which will convert the stream into SRT and send it to our live encoder.\n2. Encoding of the Input Stream to MPEG-DASH and HLS\nYou can define multiple output resolutions and bitrates for MPEG-DASH and HLS, define if you want to encode to H.264 (AVC) or H.265 (HEVC). There are literally no limits in defining what output you want from our live encoder, e.g. it can easily handle multiple 4k 60FPS streams encoded to HEVC.\nIn this tutorial, we will create a simple adaptive stream with three video renditions (H.264, 1080p/720p/480p) and one audio rendition (AAC, 128kbps). The stream will be playable using the DASH or HLS streaming protocols.\n3. Direct Output to Your Storage\nOur live encoder writes the encoded files directly to your AWS S3 or GCS bucket. You could use this output storage as origin for your CDN or for multiple CDNs. This works well with GCS as Google offers a CDN Interconnect for several CDNs such as Akamai, Fastly, Level3, HighWinds, CloudFlare, Limelight, and Verizon. AWS S3 could also easily be connected with their Cloudfront CDN and provides you an out-of-the-box solution in one cloud.\nIn this tutorial, we will write the encoded files to an AWS S3 bucket of yours, from where you will be able to stream it using DASH or HLS.\n4. Playback on any Device, or Browser\nIf you are using the Bitmovin HTML5 player your content will playback on any device or browser. However, you are not limited to use our player and can also choose among others as described in our recent blog post on encoding for multiple HTML5 Players. You can also use the other Bitmovin Player SDKs if you want to build native apps on iOS, Android, Roku and other devices.\nIn this tutorial, we will play back the DASH and HLS versions of the encoded live stream in our online demo player.\nPreparation\nTo run the example in this tutorial, you need a\nBitmovin API Key\nand credentials for an\nAmazon AWS S3 bucket\n.\nStarting With The API\nNow, let’s see how we can start a live stream that will generate MPEG-DASH and HLS streams from an SRT input.\nThe following example shows the setup of an SRT live stream, and will be utilizing our\nOpen API SDK for Java\n. The\nfull example\ncan be found in our\nOpen API Example Repository\non Github.\nSetting up Your Properties file\nFirst we need to setup the Bitmovin API client with your API key and S3 bucket credentials. For this, create a directory named\n.bitmovin\nin your user’s home directory. Then, create a text file named\nexamples.properties\nin the\n.bitmovin\ndirectory, with the following content:\nBITMOVIN_API_KEY=<YOUR_API_KEY>\nS3_OUTPUT_BUCKET_NAME=<YOUR_S3_BUCKET_NAME>\nS3_OUTPUT_ACCESS_KEY=<YOUR_S3_ACCESS_KEY>\nS3_OUTPUT_SECRET_KEY=<YOUR_S3_SECRET_KEY>\nS3_OUTPUT_BASE_PATH=/output/encoding_test/doc/srt\nUsing ConfigProvider to Initialize the API\nThe OpenAPI client includes a\nconfigProvider\nobject which has access to your\nexamples.properties\nfile and thus, to your Bitmovin and S3 credentials. Before we can do anything, we must set up the\nconfigProvider\n.\nNext, we set up\nbitmovinApi\n, the basic object accessing the Bitmovin API. It gets the API key from\nconfigProvider\n.\nJava\nconfigProvider = new ConfigProvider(args);\nbitmovinApi =\n    BitmovinApi.builder()\n        .withApiKey(configProvider.getBitmovinApiKey())\n        .withLogger(\n             new Slf4jLogger(), Level.BASIC) \n        .build();\nYou will find this code in the\nmain()\nmethod of the\nSrtLiveEncoding\nclass. This is the method where the main workflow resides. It is also the method you later will run in order to start the encoding.\nCreate an Encoding\nCreate an\nencoding\nobject with the simple line\nJava\nEncoding encoding =\n    createEncoding(\"Live Encoding Test with SRT Input Listener Mode\", \"Live encoding with HLS and DASH manifest\");\nThis initializes a default encoding which is sufficient for a start.\nPlease note:\nFor production, to guarantee a stable connection between the contribution encoder and our live encoder, it is crucial to carefully select the cloud and region in which your live encoder will run to reduce latencies.\nOur encoder uses Western Europe as the default region. You can set cloud regions by adding attributes to the encoding object in the\ncreateEncoding()\nmethod.\nCreate the Live SRT Input\nBitmovin's live encoding service supports SRT ingestion in both Caller and Listener modes. The code below initializes your encoding as SRT Listener, listening at port\n2088/udp\n, ready to receive a stream from your contribution encoder or Wowza server acting as an SRT Caller.\nJava\nSrtInput input = createSrtInput(SrtMode.LISTENER, 2088, null);\nIf you are interested in the details, please read on. Otherwise you may skip the rest of the section and continue with\nCreate an Output\n.\nDetails\nThe code above calls a method named\ncreateSrtInput()\nwhich looks like this:\nJava\nprivate static SrtInput createSrtInput(SrtMode mode, int port, String host) throws BitmovinException {\n    SrtInput srtInput = new SrtInput();\n    srtInput.setMode(mode);\n    srtInput.setPort(port);\n    srtInput.setHost(host);\n    SrtInput input = bitmovinApi.encoding.inputs.srt.create(srtInput);\n    return input;\n  }\nThe method accepts three parameters, which are explained here:\nSrtMode mode\nmust be either\nSrtMode.LISTENER\nor\nSrtMode.CALLER\n. This determines in which SRT Mode the Bitmovin live encoder will run.\nIt is also possible to configure the latency (in milliseconds) for the SrtInput (e.g.:\nsrtInput.setLatency(1000);\n). The default value is 120 ms.\nIf you experience picture artifacts in the output frames, SRT packet loss could be a cause. Increasing this value can help mitigate this issue.\nIf the Bitmovin live encoder is running in Listener mode\nAs a Listener, Bitmovin will wait for a contribution encoder to connect as a Caller. In this case, the other two parameters have the following meaning:\nint port\nis the UDP port at which the Bitmovin live encoder listens for connections from the contribution encoder.\nPlease note:\nCurrently, port 2088/udp is the only supported port for SRT Listener inputs. Therefore please make sure your contribution encoder calls the Bitmovin encoder on that port, and your firewalls allow outgoing traffic on that port.\nString host\nmust be\nnull\n. This is because the Bitmovin live encoder does not need to know the IP address of the calling contribution encoder upfront.\nPlease note:\nThe IP address of the Bitmovin live encoder (where it listens to incoming SRT calls) will only be determined after you start the encoding, as you will later see. The Bitmovin live encoder's address is chosen by Bitmovin and will typically be different for every new encoding, even if you stop and start the encoding to change parameters. You cannot configure the Bitmovin encoder to assume a certain IP address.\nIf Bitmovin live encoder is running in Caller mode\nAs a Caller, Bitmovin live encoder will reach out for the contribution encoder which must run in Listener mode. In this case, the other two parameters have the following meaning:\nint port\nis the UDP port at which the Bitmovin encoder tries to reach the contribution encoder. Please make sure you configure your contribution encoder to listen at this port.\nString host\nis the contribution encoder's IP address, in string format.\nPlease note:\nIn this tutorial, we are using Bitmovin live enocder in Listener mode.\nCreate an Output\nIn this tutorial we are using an AWS S3 bucket as output location of our live encoder. However, it's a simple change to use an Google Cloud Storage bucket as output instead if you prefer.\nThe\noutput\nobject gets the credentials for the bucket from the\nconfigProvider\n.\nJava\nOutput output = createS3Output(\n                    configProvider.getS3OutputBucketName(),\n                    configProvider.getS3OutputAccessKey(),\n                    configProvider.getS3OutputSecretKey());\nAdd Video and Audio Codec Configurations\nIn this section, you will determine the formats into which your live stream will be encoded.\nThe encoding, or transcoding, will happen in the following steps:\nSelect the video and audio tracks of the input stream\nConfigure the video resp. audio renditions into which the input tracks will be encoded\nSelecting the video and audio tracks from the input stream\nAn input stream can have multiple video and multiple audio tracks, e.g for different angles and languages. For this tutorial, we assume that the input stream (coming from the contribution encoder) only contains one video and one audio track.\nFor this simple but widely used track layout it is sufficient to tell Bitmovin encoder to select the first available track as source for the video renditions, and the second available track as source for the audio renditions. The first available track has the number\n0\nand the second has the number\n1\n. We will meet these numbers later in the process.\nConfigure Video and Audio Renditions\nA rendition is a way to encode content. For adaptive streaming we typically use multiple video renditions - high quality for fast networks and low bandwidth for slow networks. We only use one audio rendition because audio bandwidth is low compared to video and can be used for both fast and slow network conditions.\nA codec configuration contains the configuration for a video rendition or an audio rendition. We will link the video input track to three codec configurations for three different video renditions. We will link the audio input track to one codec configuration.\nThe following code defines a set of three H264 video profiles. The parameters are the\nname\n, the\nbandwidth\n(in bps), the\nheight\n, and the\noutput path\n. The last parameter is always\n0\nto select the first track of the input stream, which is the video track.\nJava\nprivate static List<VideoConfig> videoProfile =\n  Arrays.asList(\n    new VideoConfig(\"480p\", 800_000L, 480, \"/video/480p\", 0),\n    new VideoConfig(\"720p\", 1_200_000L, 720, \"/video/720p\", 0),\n    new VideoConfig(\"1080p\", 3_000_000L, 1080, \"/video/1080p\", 0));\nThe code below transforms the video profiles into codec configurations of type\nH264VideoConfiguration\n. This will encode the input stream's video part using the H.264 codec. Of course it is possible to use other codecs as well, such as VP9 or H.265/HEVC.\nJava\nfor (VideoConfig videoConfig : videoProfile) {\n      H264VideoConfiguration h264Configuration =\n          createH264VideoConfig(videoConfig.height, videoConfig.bitRate);\n      Stream stream =\n          createStream(encoding, input, h264Configuration, videoConfig.inputStreamPosition);\n      \n      createFmp4Muxing(encoding, stream, output, videoConfig.outputPath);\n    }\nHere the lines mean the following:\nJava\nH264VideoConfiguration h264Configuration =\n          createH264VideoConfig(videoConfig.height, videoConfig.bitRate)\nThis line creates a\nh264Configuration\nobject which uses the\nheight\nand the\nbitrate\nof the video profile. Because the aspect ratio of the source is kept, with\nheight\nyou can control the resolution of the rendition and with\nbitRate\nthe bandwith and hence, quality.\nJava\nStream stream =\n           createStream(encoding, input, h264Configuration, videoConfig.inputStreamPosition);\nThis line links the\nh264Configuration\nobject to the\ninput\nstream's track which is depicted by the video profile's\ninputStreamPosition\n(Remember that this is\n0\nfor the video track). The line basically defines a task to \"encode the input stream's video track with the given H.264 configuration\". Also, this task is linked to the\nencoding\nobject we created earlier, making it part of the encoding. The whole encoded video rendition is referenced as a\nstream\n.\nJava\ncreateFmp4Muxing(encoding, stream, output, videoConfig.outputPath);\nThis line is explained in the next section of this tutorial.\nFor audio it works pretty much the same way as for video as you can see in this code sample:\nFirst, the predefined audio profile:\nJava\nprivate static List<AudioConfig> audioProfile =\n     Collections.singletonList(new AudioConfig(\"128kbit\", 128_000L, \"/audio/128kb\", 1));\nThen, the code that creates an AAC audio configuration out of the profile and links it to the audio track of the input stream\nJava\nfor (AudioConfig audioConfig : audioProfile) {\n      AacAudioConfiguration aacConfig = createAacAudioConfig(audioConfig.bitrate);\n      Stream audioStream =\n          createStream(encoding, input, aacConfig, audioConfig.inputStreamPosition);\n      \n      createFmp4Muxing(encoding, audioStream, output, audioConfig.outputPath);\n    }\nAgain, the line\ncreateFmp4Muxing(..)\nis explained in the next section.\nCreate Muxings for MPEG-DASH and HLS\nIn the previous section we have explained how the input stream is encoded, but in order to write down the encoded data it must be packaged into a container format.\nAs the content will be used for MPEG-DASH and HLS streaming, we choose a container format that supports both, named\nFragmented MP4\nor\nfMP4\n. This format cuts the stream in multiple small, numbered files called\nSegments\n.\nThe following line of code, which was already seen in the previous section, defines an\nfMP4\ncontainer format. Because writing an encoded stream into a container is also known as \"multiplexing\", or short\nmuxing\n, container formats are also known as\nmuxings\n.\ncreateFmp4Muxing(encoding, stream, output, videoConfig.outputPath);\nThis line takes an encoded video rendition (\nstream\n) from an\nencoding\n, and packages it into the\nfMP4\nformat. Then it writes the\nfMP4\nsegments to the\noutput\n, which is the S3 bucket we configured earlier. The segments are written into the\noutputPath\ndefined in the video profile.\nDefine the MPEG-DASH and HLS Live Manifests\nSo far we have defined how to encode the input streams and package the encoded streams into\nfMP4\nsegements which will reside on your output bucket.\nIn order to playback content using MPEG-DASH or HLS, we also need to have\nmanifests\n. A manifest is a text or XML file which describes the adaptive output stream so a player always knows which segments to retrieve and playback next. Manifests tell the player which renditions are available and where to get them. Typically a stream has a set of interlinked manifests. HLS and MPEG-DASH manifests differ in format. But both formats eventually retrieve and playback the\nfMP4\nsegments created by the Bitmovin live encoder.\nWith the Bitmovin API you have full control over creating manifests, e.g. create multiple manifests with different renditions.\nWhen creating the Live MPEG-DASH or Live HLS manifest (or manifests), you also specify where it is output, with path location and filename for the manifest.\nJava\nDashManifest dashManifest = createDashManifest(output, \"/\", encoding);\nHlsManifest hlsManifest = createHlsManifest(output, \"/\", encoding);\n\nLiveDashManifest liveDashManifest = new LiveDashManifest();\nliveDashManifest.setManifestId(dashManifest.getId());\nliveDashManifest.setTimeshift(300D);\nliveDashManifest.setLiveEdgeOffset(90D);\n\nLiveHlsManifest liveHlsManifest = new LiveHlsManifest();\nliveHlsManifest.setManifestId(hlsManifest.getId());\nliveHlsManifest.setTimeshift(300D);\nYou also have the option to set specific live options, such as the timeshift parameter that defines how many seconds a user will be able to seek back in time. In the example below we allow the users to seek back 5 minutes. For MPEG-DASH you can also define how far away from the real live signal the player will start to playback the segments. If you are not aiming for low latency live streams, choose a value between 60 and 120 seconds to give the player enough room for buffering.\nStart the Live Encoding\nNow that we have defined every aspect of the live encoding, we can finally start the live stream using both created manifests in the start call.\nFor this, we define a new\nStartLiveEncodingRequest\nobject named\nstartRequest\nand add both manifests to it:\nJava\nStartLiveEncodingRequest startRequest = new StartLiveEncodingRequest();\nstartRequest.addDashManifestsItem(liveDashManifest);\nstartRequest.addHlsManifestsItem(liveHlsManifest);\nAutomatic shutdown\nOptionally an automatic shutdown can be defined for the live encoding. The automatic shutdown can be defined via the\nLiveAutoShutdownConfiguration\n. There are two possible settings. The\nbytesReadTimeoutSeconds\ndefines how long the live stream will run after an input loss and the\nstreamTimeoutMinutes\ndefines the time after which the live stream will be automatically shut down.\nJava\nLiveAutoShutdownConfiguration shutdownConfig = new LiveAutoShutdownConfiguration();\nshutdownConfig.setStreamTimeoutMinutes(240L);\nshutdownConfig.setBytesReadTimeoutSeconds(600L);\nstartRequest.setAutoShutdownConfiguration(shutdownConfig);\nStarting the Example\nThe\nstartLiveEncodingAndWaitUntilRunning()\nmethod with both the configured\nencoding\nand the\nstartRequest\nwill start the encoding.\nJava\nstartLiveEncodingAndWaitUntilRunning(encoding, startRequest);\nTo start the example and actually run a live encoding with SRT input, run the\nmain()\nmethod of the\nSrtLiveEncoding\nclass in the\nSrtLiveEncoding.java\nfile.\nRetrieve SRT Ingest Point Information\nAfter starting the live encoding, you need to wait for it to be ready before you can ingest your SRT stream. This may take a few minutes.\nOnce it is ready you can easily query the live stream details and (for example) print it to the console:\nJava\nLiveEncoding liveEncoding = waitForLiveEncodingDetails(encoding);\n logger.info(\n         \"Live encoding is up and ready for ingest. SRT URL: srt://{}:{}\",\n         liveEncoding.getEncoderIp(),\n         input.getPort());\nThe SRT URL will look like this:\nsrt://123.45.67.89:2088\nIf Bitmovin is running in Listener Mode\nThe SRT URL is the URL that the Bitmovin live encoder is listening on. Your contribution encoder needs to call the SRT URL and send the SRT stream to it.\nIf Bitmovin is running in Caller Mode\nThe SRT URL is the URL that your contribution encoder needs to listen on, waiting for calls from the Bitmovin live encoder.\nPlease note\nthat in this tutorial we are running the Bitmovin live encoder in Listener mode.\nAfter retrieving the SRT URL, please start your contribution encoder and send the live stream via SRT to the Bitmovin live encoder.\nPlayback the Live Stream\nThe live stream can be played back with a compatible player. Bitmovin has a test player page:\nhttps://bitmovin.com/demos/stream-test\nIn this page, enter your DASH or HLS URL in order to play back your live stream.\nProvided you use your S3 bucket and the settings in this example, the URLs will look like this:\nDASH\nHTTP\nhttps\\://\\<your\\_bucket\\_name>.s3-\\<cloud\\_region>.amazonaws.com/output/encoding\\_test/doc/srt/SrtLiveEncoding/stream.mpd\nHLS\nhttps\\://\\<your\\_bucket\\_name>.s3-\\<cloud\\_region>.amazonaws.com/output/encoding\\_test/doc/srt/SrtLiveEncoding/master.m3u8\nShutdown the Live Encoding\nBecause of this mechanism to provide a continuous output, the live encoding won't stop automatically when the input feed stops. You therefore have to shut the encoding down at the end of your event, to avoid generating unnecessary costs in your account. The following code shows how to stop a live encoding:\nJava\nlogger.info(\"Shutting down live encoding!\");\nbitmovinApi.encoding.encodings.live.stop(encoding.getId());\nwaitUntilEncodingIsInState(encoding, Status.FINISHED);\nYou can also use the Bitmovin dashboard to stop the encoding. The controls are shown in the Live Encoding details.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/2972523-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/thumbnail-generation-support-for-vod-encoding",
    "title": "Thumbnail Generation Support for VOD Encoding",
    "text": "Compatibility\nThe table below summarizes the thumbnails types supported by Bitmovin Encoding.\nThumbnail Type\nProtocol\nThumbnail container\nImage Format\nSingle image\nNA\nNone (Images only)\njpeg\n/\npng\nSprites\nHLS / DASH\nWebVTT\njpeg\n/\npng\nSprites\nDASH\nImage Adaptation Sets\njpeg\n/\npng\nRoku BIF\nHLS/DASH\nBase Index Frames (BIF)\njpeg\nI-frame\nHLS\nI-frame only playlist\nVideo frame\n⚠️\nCurrently, thumbnail generation is supported only for VoD use case, not for live encoding.\nOverview\nThumbnails are still images that enable preview of videos. Depending on the platform, protocol and thumbnails types, thumbnails images can be used for several use cases, for instance:\nThumbnails as\nsingle images\n- for example for feeding CMS (Content Management Systems) with imagery.\nThumbnails for\ntrick play / trick mode\n- usually showing preview images while users scrubs through the seek bar or while doing fast-forward operation during a playback session.\nBitmovin encoding supports the generation of a wide range of thumbnails types to support those use cases, and the following sections will provide details of the different types we support with the Bitmovin Encoding solution\nℹ️\nTo check what thumbnails types we support with Bitmovin Player see\nPlayer - Thumbnail Preview Support\n.\nThumbnails as single images\nThe Bitmovin Encoding solution supports the generation of\njpeg\n/\npng\nsingle images from the input video asset. Within the Bitmovin API, we know these single images as\nThumbnails\n.\nThe Bitmovin encoding API supports several configuration options for single image thumbnails, such as setting image size, image interval generation, file name pattern, etc. For more information check out the\nBitmovin API for adding Thumbnails to an Encoding\n.\nThumbnails for trick play\nWebVTT Sprites\nThe Bitmovin encoding solution supports the generation of\njpeg\n/\npng\nSprites and its respective WebVTT file. A\nSprite\nis a\njpeg\nor\npng\nfile that contains smaller images stitched together into a single one that is referenced within a WebVTT file. Each WebVTT file has the proper mapping information for referencing each Sprite and its respective images within a specific time range for playback.\nWhen creating a Sprite, the Bitmovin API allows to set the number of images per sprite, the Sprite dimensions - rows and columns, image interval, image size, etc. For more information check out the\nBitmovin API for adding Sprites to an Encoding\n.\nSome players (including the Bitmovin web player) can be configured to use such thumbnails, without need for them to be added to the manifests.\nImages Adaptation Sets for DASH\nThe Bitmovin encoding solution supports the generation of\njpeg\n/\npng\nSprites based on the\nDASH-IF IOP specification\n-section 6.2.6. According to the DASH specification, thumbnails are added as a new\nAdaptationSet\nwith\n@contentType=\"image\"\n. In this way, each Image Adaptation Set provides the timing and the proper addressing mechanism just as for other audio/video\nAdaptationSet\n, allowing mapping of images (or sections in the sprite) to the specific time ranges for playback. Within the Bitmovin API, we refer to this mechanism as\nImageAdaptationSet\n.\nFor more information check out the\nBitmovin API for adding ImageAdaptationSet to DASH manifests\n, which relies on Sprites having been created first - as explained in the previous section.\nI-frame only playlist for HLS\nThe Bitmovin encoding solution supports the generation of video I-frames as defined in the standard HLS specification -\nrfc8216\n. According to this spec, video I-frames are used to render images for operations such as trick play and seek.\nFor more information check out the\nBitmovin API for adding I-Frames to HLS manifests\n.\nBase Index Frames (BIF)\nBitmovin encoding supports the generation of BIF files - a\nRoku specification that allows trick mode support\n. This format is not used by other playback platforms. For more information checkout the\nAPI for creating BIF files\n.\n⚠️\nIn addition to BIF, Roku specifications also allows to use\nHLS Image Media Playlist\nfor trick mode playback, however HLS Image Media Playlist are not currently supported with our encoder. Please inform your account manager if this is a format you require.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoding-templates-for-live-standby-pools",
    "title": "Encoding Templates for Live Standby Pools",
    "text": "Creating and Testing a valid Encoding Template\nThe Live Standby Pools require an\nEncoding Template\nto be assigned to them as the basis for all Live Encodings that they will create.\nCreating the Template\nIn Live Encoding -> Encoding Templates, we have some examples to get started, or they can be created from scratch.\nEither by using a template to get started or creating from scratch, you can use your own text editor program or the Editor in the Dashboard.\nAs this Encoding Template will be used to create multiple Live Encodings placeholder names with regular expressions for certain settings are required to ensure certain values are unique, such as:\nAuthentication for inputs such as RTMP Input streamkeys = for now give this a real value such as\nabc123\nshown above.\nEncoding name =\n\"{poolName}_%number%\"\n# will be replaced pool name\nEncoding labels = \"standby-pool-id:{poolId}\" # will be replaced with poolId\nSegment names =\n\"seg%number%\"\nor\n\"seg%number%_{segment_rand_chars:10}.ts\"\n# will add a number with or without random characters to the suffice of the segment file name.\nOnce you're happy with the Encoding Template, give it a temporary name for now, for example add TESTING as a suffix.\nSave It\n.\nIt will then be visible in the Custom template tab, where all customers can store up to 24 templates at a time.\n❗️\nTemplates are imutable\nOnce saved, a template can not be edited. If changes are required then another copy needs to be made, saved and run.\nTesting the template\nOpen the template in the editor and press\nRun the Encoding\nOnce running, you'll notice that certain names for pools appear in the formula - such as the Encoding Name.\nThis is expected when they are Run \"manually\".\nProviding that the Live Encoding starts correctly, connect to the ingest point and confirm video and audio playback is as expected either using the Player in our Dashboard or your own via your CDN endpoint.\nIf you are happy, go back to Encoding Templates and edit the file, changing any input authentication to one compatible with a pool.\nAuthentication for inputs such as RTMP Input streamkey =\n\"{uuid}\"\n# will be replaced with a UUID\nYou can now delete the previous template used for testing. Your production ready template is now ready to use with a new Live Standby Pool.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/92ef1a9c8c45ecdf667358a4f8a25e4b2a63190e07dde7c36acad7ca5fa22de1-Screenshot_2024-11-29_at_15.47.40.png",
      "https://files.readme.io/3dd002a57a4f8b5b8d3a70fcfd2f800c525ec9b476bf721ce465042144a7575a-Screenshot_2024-11-29_at_16.27.47.png",
      "https://files.readme.io/2e729e59c5671275f4005f1abb698b2fdc5ccfdc782caed9ba8239a72437c816-Screenshot_2024-11-29_at_16.15.57.png",
      "https://files.readme.io/8a18f65ca82a148db5e954a2524016360385199a3e11c2e42a9d5acc7a8389d8-Screenshot_2024-11-29_at_16.36.27.png",
      "https://files.readme.io/ab9069c34deb875d1aa0e4f6abd13f20721355b21c7e4451b1117002af60b7fc-Screenshot_2024-11-29_at_16.34.09.png",
      "https://files.readme.io/4170beef76af12e3012bd7350974f7c344dca5b4f58972e8efd9a2d4a4fc9dca-Screenshot_2024-11-29_at_16.41.10.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/managing-your-organization-team-access",
    "title": "Managing Your Organization & Team Access",
    "text": "Managing your organization\nWhen you create a Bitmovin Account via the\nBitmovin Dashboard\nan\norganization is also created\nautomatically and you become the\nowner\nof the organization.\nYou can access your\norganization details & settings\nvia your profile:\nYou will find your\norganization details & ID\nin your\nOrganization Overview\n.\nTerminology\nBitmovin Account\n: When you sign up or get invited to an Organization via the Bitmovin Dashboard, an account is created for you. Each account is associated with an\nOrganization\n.\nOrganization:\nAn Organization holds all the resources you can create in the Dashboard or through the Bitmovin API (Encodings, Inputs, Outputs, Configurations, Manifests, etc.), as well as the subscriptions for each of our services. Furthermore, one or more\nUsers\ncan be part of an\nOrganization\n.\nUser\n(Owner, You): A\nUser\nis by default the owner of the Organization of its own Bitmovin Account.\nGroup:\nA\nGroup\nhas specific\nPermissions\nassigned to it to control which actions its members can perform in an\nOrganization\n.\nUser:\nA\nUser\ncan become a member of other\nOrganizations\nby being added to one or more of its\nGroups\n.\nUsers\nthat are added to other\nOrganizations\nare also referred to as\nTenant-Users\n.\nPermissions:\nPermissions\ncan be assigned to\nGroups\nto control what resources its members are allowed to\nView\nor\nManage\ninside an\nOrganization\n.\nManaging your team\nVia your\norganization's team settings\nyou can invite additional users to the Bitmovin Dashboard and also manage their access by adding them to groups and defining group permissions.\nInviting team members\nInvite your team members by clicking the\n+ Add\nbutton next to \"Users\":\nEnter the email(s) of the user(s) you want to add to your organization and specify the groups you want to add them to and eventually click\nSave\n:\nThe users you have invited will receive an email and once they've created their account they can select your organization in the top menu bar and switch to it.\nIf the account already exists, they can just log in and select the correct organization in the top menu bar.\nManaging groups\nBy default, there are three predefined groups available:\nGroup\nDescription\nDevops\nAllowed to\nview\nEncoding, Player, and Analytics resources\nAccount Admins\nAllowed to\nview and manage\naccount settings\nProduct Admins\nAllowed to\nview and manage\nEncoding, Player, and Analytics resources\nCreating new groups\nIf none of the predefined groups fits your needs, you can create a custom group with custom permissions. In order to create a custom group, go to your\nTeam settings\n, and click on the\n+ Add\nbutton next to Groups:\n💡\nTip\nWe recommend to use a descriptive name and description, so it is easy to see what set of permissions are configured for this group. By default, if no permissions are added to a group everything will be denied. So, by adding permissions you can allow specific actions that users of this group can perform.\nGroup permissions\nTo manage group permissions, go to your\nTeam settings\nand click on the\nEdit\nbutton next to the group name and description.\nPermission levels & available policies\nThe following permission levels are available:\nPermission level\nDescription\nView\nAllows to view the resources only (API SDK: only\nGET\nRequests)\nManage\nAllows to create/edit/delete resources (API SDK: only\nPOST/PUT/DELETE/PATCH\nRequests)\nAvailable Policies\nEncoding - Control access to all encoding related resources and its APIs\n(\nVoD/Live/PerTitle Encodings and Object Detection\n,\nConfigurations\n,\nInputs\n,\nOutputs\n,\nFilters\n,\nManifests\n,\nInfrastructure\n,\nStatistics\n)\nNotifications - Controls access to email- and Webhook notifications for encoding events and its APIs\n(\nEmail\n,\nWebhook\n)\nPlayer - Controls access to all player related resources and its APIs\n(\nLicenses\n,\nStatistics\n)\nAnalytics - Controls access to all analytics related resources and its APIs\n(\nLicenses\n,\nQueries\n,\nMetrics\n)\nAccount - Controls access to Bitmovin Account related areas\n(\nAccount Settings\n,\nOrganization Settings\n)\nBilling - Controls access to all billing and subscription related areas\n(\nSubscriptions\n,\nPayment Methods\n,\nBilling Information Details\n)\nUsing an API SDK with different Organizations\nWhen using one of our API SDKs, you provide the API Key of your own Bitmovin Account to authenticate your requests and execute them within your Organization:\nJava\nBitmovinApi bitmovinApi = BitmovinApi.builder()\n        .withApiKey(\"<API_KEY>\")\n        .build();\nIf you want to execute API calls in a\ndifferent organization\n, you also have to provide the Organization ID when initialising the\nBitmovinApi\nobject:\nJava diff\nBitmovinApi bitmovinApi = BitmovinApi.builder()\n    .withApiKey(\"YOUR_API_KEY_HERE\")\n+   .withTenantOrgId(\"YOUR_ORGANISATION_ID_HERE\")\n    .build();\n💡\nNote\nDepending on your permissions, you might not be able to execute all actions. In this case, the group you're in needs additional permissions via\nTeam Settings\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/47296638d4524cd7b8e4a370a6756de2118a8a3daaaeecfceeed16a3ae5f2a1d-Screenshot_2024-10-17_at_3.17.38_PM.png",
      "https://files.readme.io/3ac0f9fd77fb469cdb82be97a7edac332d168789a5cf6b1f689595738ae579af-Screenshot_2024-10-17_at_4.00.43_PM.png",
      "https://files.readme.io/31f10aa896956192beea3c6405e944a1bceb9a7c46546112c3c7bd6076f01e90-Screenshot_2024-10-17_at_3.46.26_PM.png",
      "https://files.readme.io/6f13e37c6a92d7fc6f75a010ec34e221afdc741fd16c11f02127b0c84159ffb3-Screenshot_2024-10-17_at_3.47.13_PM.png",
      "https://files.readme.io/a8843d09db5e1b77893aeef6332bcb98bd8294aa4499e13e11e25990d8da5c27-Screenshot_2024-10-17_at_3.47.51_PM.png",
      "https://files.readme.io/c81f7f08064e54e1dd31c6c97f954e53e6d9b7603959660db8e3f6aaa279ce94-Screenshot_2024-10-17_at_3.35.08_PM.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/sign-up-through-aws-marketplace",
    "title": "Sign Up Through AWS Marketplace",
    "text": "When subscribing to the Bitmovin Video Encoding Services through the\nAWS Marketplace\n, a new Bitmovin Account is registered using the AWS Account Email Address. For this step to be successful, the following requirements do have to met:\nThe email address of your AWS account is not yet associated with an Bitmovin account at\nhttps://bitmovin.com/dashboard\n✅\nI haven't registered an Bitmovin Account using my AWS Account Email yet:\nYou're good to go! Visit the AWS Marketplace to sign up for our professional video encoding services\n❓\nI already registered an Bitmovin Account using my AWS Account Email. What can I do now?:\nPlease login to your Bitmovin Account at\nhttps://bitmovin.com/dashboard\n, and\nCreate a Support Ticket\nstating that you intend to subscribe through the AWS Marketplace. Our Support Team will get back to you assist you with the setup 👍",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-fairplay-drm-protected-content",
    "title": "Creating Fairplay DRM Protected Content",
    "text": "Overview\nFairplay is the Apple DRM and was initially used only in the iTunes store to protect AAC encoded audio files but got soon also adopted for Apple’s video products that are now part of the iTunes store. Fairplay is specifically designed for Apple HTTP Live Streaming (HLS) with Apple playback devices such as iPhone, iPad, Apple TV and Mac OS X. Fairplay is also used as Content Decryption Module (CDM) of the Safari browser. This enables HTML5 native playback of DRM encrypted Fairplay streams without plugins in Safari.\nFairplay DRM Requirements\nYou need a Fairplay certificate from Apple in order to be able to use Fairplay DRM. This certificate is needed in order to use DRM provider to control the playback of your content. To get one, you have to request a FPS Deployment package. More details about Fairplay are available at\nhttps://developer.apple.com/streaming/fps/\n.\nFairPlay DRM Playback is supported on Apple devices only. Please see our\nPlayer DRM support\noverview for more details.\nAbout this example\nThe code snippets shown here are based on the\nfull example\ncalled\nCencDrmContentProtection.java\n, using our\nBitmovin SDK for Java\n.\nHint:\nIf you haven't created any encodings with our Service yet, its recommended to start with our quick start guide called \"\nGet Started with the Bitmovin API\n\" first, before you continue :)\nEncoding with DRM Configuration\nEncryption details\nTo encrypt your content so it can be used with FairPlay DRM, an\nencryption key\n(referred to as\nDRM_KEY\nin the example) is required. All other values are optional, however sometimes required by specific DRM vendors, therefore have to be set (\nDRM_FAIRPLAY_IV\n,\nDRM_FAIRPLAY_URI\n, ...).\nThese values of the Fairplay DRM configuration are expected in the following format:\nDRM_KEY: (required) You need to provide a key that will be used to encrypt the content (16 byte encryption key, represented as 32 hexadecimal characters)\nDRM_FAIRPLAY_IV: The initialization vector is optional. If it is not provided we will generate one for you. (16 byte initialization vector, represented as 32 hexadecimal characters)\nDRM_FAIRPLAY_URI: If provided, this URI will be used for license acquisition, (e.g. skd://userspecifc?custom=information)\nHINT:\nSome DRM providers provide you with a dedicated service to create and safely store Encryption Keys, so you don't have to create and manage them by yourself. These values, along with the Apple certificate, are required to generate a proper playback license using DRM solution providers like Irdeto, EZDRM, ExpressPlay, Axinom, BuyDRM, etc. to control playback permissions on the client side.\nLearn more\n.\nMuxing DRM Configuration\nWhen you create an encoding, you define\nStreams\nwhich point to the video track of the input file, and couple them with an\nCodec Configuration\nof your choice. These\nStreams\nare then used by\nMuxings\nwhich determine the desired output format of your content.\nSince iOS 10, respectively macOS Sierra/Safari 10, you can use the output of\nfMP4 Muxings\nfor HLS with Fairplay DRM. Older iOS devices still require MPEG-TS, therefore you have to use\nTS Muxings\nto support those. In order to configure a\nMuxing\nto create DRM encrypted content, you add a\nCencDRM Configuration\n(\nAPI reference\n) to it. Its a general DRM configuration object, where you can provide the required\nkey\nto encrypt the content, along with DRM solution specific details.\nThe example creates two\nStreams\n- one for the video-\nStream\n(H264) and one for the audio-\nStream\n(AAC). For each of the\nStreams\nand\nfMP4 Muxing\nis created, without providing a output destination.\nJava SDK Example - create Encoding\n(\nLine in Example\n)\nJava\nEncoding encoding =\n    createEncoding(\"fMP4 muxing with CENC DRM\", \"Example with CENC DRM content protection\");\n\nHttpInput input = createHttpInput(configProvider.getHttpInputHost());\nOutput output =\n    createS3Output(\n        configProvider.getS3OutputBucketName(),\n        configProvider.getS3OutputAccessKey(),\n        configProvider.getS3OutputSecretKey());\n\nH264VideoConfiguration h264Config = createH264VideoConfig();\nAacAudioConfiguration aacConfig = createAacAudioConfig();\n\nStream videoStream =\n    createStream(encoding, input, configProvider.getHttpInputFilePath(), h264Config);\nStream audioStream =\n    createStream(encoding, input, configProvider.getHttpInputFilePath(), aacConfig);\n    \nFmp4Muxing videoMuxing = createFmp4Muxing(encoding, videoStream);\nFmp4Muxing audioMuxing = createFmp4Muxing(encoding, audioStream);\n...\nIn typical DRM Encoding workflows the\nOutput\nconfiguration is part of The DRM Config\nonly\n. It is added to the muxing by the\ncreateDrmConfig\nmethod. So, it defines which output shall be used to store the encrypted content.\nJava SDK Example - Add DRM Config to Muxing\n(\nLine in Example\n)\nJava\ncreateDrmConfig(encoding, videoMuxing, output, \"video\");\ncreateDrmConfig(encoding, audioMuxing, output, \"audio\");\nIMPORTANT:\nYou can still provide an\nOutput\nconfig to the fMP4 Muxing as well, but it would push the\nunencrypted version of your encoded content\nto this\nOutput\ndestination. So, if you want to create encrypted content only, provide an\nOutput\nconfiguration exclusively as part of your DRM Config.\nJava SDK Example - createDrmConfig() Method\n(\nLine in Example\n)\nJava\nprivate static CencDrm createDrmConfig(\n    Encoding encoding, Muxing muxing, Output output, String outputPath) throws BitmovinException {\n  CencDrm cencDrm = new CencDrm();\n  cencDrm.addOutputsItem(buildEncodingOutput(output, outputPath));\n  cencDrm.setKey(configProvider.getDrmKey());\n  cencDrm.setKid(configProvider.getDrmWidevineKid());\n  \n  ...\n  \n  CencFairPlay cencFairPlay = new CencFairPlay();\n  cencFairPlay.setIv(configProvider.getDrmFairplayIv());\n  cencFairPlay.setUri(configProvider.getDrmFairplayUri());\n  cencDrm.setFairPlay(cencFairPlay);\n  \n  return bitmovinApi.encoding.encodings.muxings.fmp4.drm.cenc.create(\n      encoding.getId(), muxing.getId(), cencDrm);\n}\nThat's it, so we can start this encoding now with\nexecuteEncoding()\nand create HLS and MPD manifests for it, after the encoding is finished.\nJava SDK Example - Start Encoding and create HLS and MPD Manifests\n(\nLine in Example\n)\nJava\nexecuteEncoding(encoding);\n\ngenerateDashManifest(encoding, output, \"/\");\ngenerateHlsManifest(encoding, output, \"/\");\ngenerateDashManifest\nand\ngenerateHlsManifest\nare using the DefaultManifest API call for MPEG-DASH (\nExample\n) and HLS (\nExample\n). These methods are easy to configure and enable to you create a simple manifest that for each streaming format that reference all the contents that were created and can be used by the respective Streaming Format.\nWhile your MPD, will only show the DRM protected Video- and Audio track protected with Widevine DRM, your HLS manifest holds the details to play this content with Widevine or Fairplay DRM.\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoder-21000-21490",
    "title": "Encoder 2.100.0 - 2.149.0",
    "text": "2.149.0\nReleased 2023-01-10\nChanged\nUsage of conform filter in combination with relaxed concatenation setup (enabled in\n2.142.0\n) is not supported!\nFixed\nA bug that led to incorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast had been added for the impacted encoder versions: 2.137.0 - 2.148.0.\nFor 3-pass H.265 encodings, some resolutions could lead to encoding failures. This is fixed now.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.148.0\nReleased 2022-12-20\nFixed\nWhen configuring segmented muxings for Generic S3 output always signature V2 was used. Now setting Generic S3 with signature V4 will correctly use signature V4.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.147.0\nReleased 2022-12-13\nFixed\nApplying the\nconform\nfilter on encodings that\nconcatenate\ninputs with different framerates could have led to missing frames when AAC was configured for audio.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.146.0\nReleased 2022-12-06\nChanged\nInternal stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.145.0\nReleased 2022-11-29\nAdded\nReduced the startup time for Live Encodings by appr. 25%\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.144.0\nReleased 2022-11-15\nFixed\nEncodings in two scenarios could have failed:\nAn audio-mix input stream was used with a non audio-mix input stream\nThe audio mix filter was used and the input file was in a ts container or interlaced\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.143.0\nReleased 2022-11-08\nFixed\nImproved resilience of Azure cloud encodings that were failing due to intermittent internal server errors.\nIn very rare cases three-pass encodings could have randomly crashed.\nFixed a bug that prevented manifests for live encodings from being created if a Speke DRM was used.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.142.0\nReleased 2022-10-25\nAdded\nConcatenations with\nconcatenationInputStreams\ncontaining only\ntimebasedTrimmingInputStreams\nwhich again have only\nIngestInputStreams\nas inputs got more flexible. It is now possible to\nconfigure different number of inputs per ConcatenationInputStream\nconfigure different trimming settings for the trimmings that have the same position in all concatenationInputStreams.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.141.0\nReleased 2022-10-18\nAdded\nEncodings of input assets with conflicting framerate information will now be reported with a warning.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.140.0\nReleased 2022-10-10\nChanged\nInternal stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.139.0\nReleased 2022-10-04\nAdded\nOptimized start-up of Live Encodings leading to reduced start-up times up to 40 %.\nFixed\nTargetQualityCrf\nis now correctly applied in AV1 encodings using per title. This leads to a slight improvement of the per title results for AV1 when using the default value for targetQualityCrf.\nFix for full range (jpeg range) YUV input files where the output is also configured as YUV full range. In this case an incorrect transform was applied to the limited range (mpeg range).\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.138.0\nReleased 2022-09-27\nAdded\nHLG output which is backward compatible with SDR is now supported: Using\nenableHlgSignaling\nin addition to setting the ARIB STD-B67 color transfer function will perform an ARIB STD-B67 conversion but signal it as BT.2020 10 bit. The ARIB STD-B67 conversion will be signaled in the SEI messages.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.137.0\nReleased 2022-09-20\nFixed\nFixed that inputs with variable video fps that also have a stream with VP8, VP9, AV1, or Opus codec the Encoding would fail.\nConverting Dolby Vision inputs to HDR10 or SDR when combined with an fps change resulted in abrupt jumps in brightness in the outputs.\nCombining Dolby Vision output filters (Dolby Vision, HDR10, SDR) in a single encoding is not supported. A fail-fast for such configs was added.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nIncorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast is in place. Please use version 2.149.0 or higher for this use-case.\n2.136.0\nReleased 2022-09-13\nAdded\nInternal stability improvements.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.135.0\nReleased 2022-09-06\nFixed\nFixed failing long duration encodes with many audio streams.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.134.0\nReleased 2022-08-30\nFixed\nFixed a regression where the Dolby Vision to SDR and Dolby Vision to HDR10 workflows were producing invalid output.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.133.0\nReleased 2022-08-22\nRemoved\nExperimental feature of object detection\nFixed\nWhen validating\nconcatenation\nthe position property of the ConcatenationInputConfiguration was not respected. This led to fail-fasts on valid concatenation configurations.\nKnown Issues\nConversions from Dolby Vision mezzanine files to SDR or HDR10 output create assets with incorrect colors\nS3 role-based output for segmented muxings: No upload verification available.\n2.132.0\nReleased 2022-08-16\nFixed\nEncodings containing 0-Duration-\nTrimming(s)\non\nvideo concatenation configurations\ncould have stalled.\nImproved stability of Dolby Vision encodings with long duration and a high number of renditions.\nFixed an internal issue which caused 3-pass encodings to fail in rare cases.\nKnown Issues\nConversions from Dolby Vision mezzanine files to SDR or HDR10 output create assets with incorrect colors\nS3 role-based output for segmented muxings: No upload verification available.\n2.131.0\nReleased 2022-08-09\nAdded\nReworked RTMP Live ingest stream handling, improving stability, modularity and testability.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.130.0\nReleased 2022-08-02\nAdded\ninput color primaries\nand the\ninput color transfer\nfor an encoding can now be overridden (\nH262\n,\nH264\n,\nH265\n,\nVP8\n,\nVP9\n,\nAV1\n)\nFixed\nWhen using different input files for video and audio streams, stream conditions of audio streams are now evaluated against the correct input file.\nEnhanced stability of the license validation in order to remove misleading error logs around API key verification.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.129.0\nReleased 2022-07-19\nFixed\nImproved stability of Dolby Vision encodings with many output renditions and muxings\nDASH manifests V2: ContentProtections were being written twice into the same AdaptationSet under particular conditions.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.128.0\nReleased 2022-07-12\nAdded\nImproved stability of three pass encodings with a long running time (due to input duration, high number of renditions, very complex configurations etc.)\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.127.0\nReleased 2022-07-05\nAdded\nPer Title encodings with short input files and trimming configured have been optimized leading to faster turnaround times\nFixed\nUsing the\naudio mix filter\ncould have caused audio outputs to be swapped to wrong output folders\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.126.0\nReleased 2022-06-28\nFixed\nFixed a bug that could cause long queue times for live encodings under some circumstances.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.125.0\nReleased 2022-06-21\nFixed\nWhen concatenating videos with different frame rates single frames could have been dropped or duplicated incorrectly.\nOptimized the internal caching for S3 role-based output to prevent 503 Throttling errors by the cloud provider for segmented output when running many encodings in parallel.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.124.0\nReleased 2022-06-14\nChanged\nInternal stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.123.0\nReleased 2022-06-08\nFixed\nIn DASH Manifest V2 via\nstart encoding call\n, fixed the calculation of the\npresentationTimeOffset\nDASH attribute for Segment Template and Segment Timeline Representations in DASH ManifestGenerator.V2 when\nstartSegmentNumber\nor\nstartKeyframeId\nis set in order to keep audio and video in sync.\nDolby visions encodings\nconfigured with MP4 output would fail for some assets with an internal error during video transcoding.\nDolby visions encodings\nconfigured with the progressive muxing output could generate non-compliant Dolby Vision streams.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.122.0\nReleased 2022-05-31\nChanged\nReduced turnaround times for 3-pass encodings up to 10%.\nFixed\nFixed segment paths of manifests for live encodings that are started via the Simple Encoding API.\nDolby Vision encodings\nconfigured with increasing frame rate change for the output (DV to SDR or HDR10) failed earlier. This is fixed now.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.121.0\nReleased 2022-05-24\nFixed\nFixed a bug for AV1 fMP4 Muxing with CENC encryption where ivSizes were always set to 8 bytes no matter the configuration. Now the\ndefault\nof 16 bytes and individually configured values are correctly applied.\nEncodings will fail early if a\ntext filter\nis set with a\ndrop-frame timecode\nand the corresponding video stream frame rate is not 29.97, 30, 59.94 or 60 FPS, conforming to the SMPTE standard.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.120.0\nReleased 2022-05-10\nAdded\nStream Conditions\ncan now also be applied to\nLive Encodings\n.\nFixed\nOption\nappendOptionalZeroHour\nwas fixed for\nWebVTT\nAdjusted internal threshold for emitting warnings when duration of the output is is not matching the expected duration to 0.3 seconds\nWhenever the\ninput ingest stream\nwas reused for multiple audio stream outputs in\nconcatenation workflow\nit lead to erroneous mixed-up audio stream outputs. This is fixed now.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.119.0\nReleased 2022-05-03\nFixed\nAudioMixInputStream failed to extract specified L and R channel when mapping each channel to a different stream\nSelectionMode for inputStreams\nnow correctly selects the desired InputStream for Live encodings\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.118.0\nReleased 2022-04-26\nAdded\nHLS manifest creation with ManifestGenerator.V2\nnow fails with appropriate error messages in the following cases:\nIf an empty streamId is given for a\nStream\n/\nMediaInfo\n, but there are more than one streams present in the muxing.\nIf a streamId is given for a\nStream\n/\nMediaInfo\nthat is not contained in the muxing whose id is given.\nFixed\nFixed an issue with updating the manifests for live encodings. Reading delays for cloud buckets could have caused stalls in updating the live manifests.\nFixed an issue with negative video time stamps for input files with aac audio in mxf containers that could lead to stalling when RESYNC_AT_START_AND_END (default behavior) is enabled.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.117.0\nReleased 2022-04-12\nChanged\nFor live encodings with RTMP/ZIXI/SRT input:\nVideo configurations do not explicitly need to set\nrate\n(output fps) and both\nwidth\nand\nheight\n(only one of that has to be set)\nAudio configurations do not explicitly need to set\nrate\n(output audio sampling rate)\nThese properties are now detected automatically and set according to the input when not provided\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.116.0\nReleased 2022-04-05\nAdded\nDolby Vision mezzanine assets can now be converted to h.264 SDR output\nAdded a warning message in case the signaled duration of an input file differs from the duration that could be parsed from the input.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.115.0\nReleased 2022-03-29\nFixed\nIdentified and fixed a very rare issue which could potentially lead to less parallelity processing batch jobs when the\nAudio Mix\nfeature was used\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.114.0\nReleased 2022-03-22\nFixed\nThe internal retry mechanism for handling throttling errors on AWS output was significantly improved. Encodings with a big number of renditions and muxings are significantly less likely to fail from throttling errors.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.113.0\nReleased 2022-03-15\nFixed\nFixed a bug which potentially caused A/V to run out of sync for inputs with variable fps\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.112.0\nReleased 2022-03-08\nFixed\nGeneral stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.111.0\nReleased 2022-02-28\nAdded\nReduced turnaround times for Dolby Vision to SDR encodings up to 70%\nFixed\nFixed long turnaround times for specific interlaced inputs with wrong signaling of field order.\nFixed bug in resource allocation for Dolby Vision encodings which caused encodings to fail in AWS\nTime-based Trimming\nconfigurations with a negative duration caused encodings to stall. Such configs are now disallowed and will result in a 400 Bad Request response during creation.\nA configuration with concatenation and trimming will now fail if an input stream is too short and doesn't contain enough data to cover trimming segments for another stream in the corresponding concatenation segment. This configuration previously lead to output being out of AVSync.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.110.0\nReleased 2022-02-22\nAdded\nAV1 encodings\nsupport now the following codec presets:\nVOD_QUALITY - Higher quality\nVOD_STANDARD - Default quality and speed\nVOD_SPEED - Faster encoding\nFixed\nEncodings configured with\nDolby Vision\nand\nFmp4Muxing\nin combination with fractional frame rate (example: 23.976, 29.97 and 59.94 fps) input could have lead to out of sync output(s).\nSubtitle segments were not split for KeyFrames during 3-pass encodings.\n3-pass encodings could have failed on GCE for specific assets\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.109.0\nReleased 2022-02-08\nAdded\nPer-Title\nis now available for\nAV1\nencodings to create the optimal bitrate ladder for each asset\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.108.0\nReleased 2022-02-01\nAdded\nManifest Generator V2 is now available for single-encoding SMOOTH manifests by specifying\nmanifestGenerator=V2\nin the SMOOTH Start-Manifest Request. The minimum encoder version for that feature is\n1.108.0\n. Differences to the LEGACY generator include:\nV2 now correctly sets the \"QualityLevels\" attribute in the StreamIndex elements to the count of QualityLevel elements present.\nV2 now correctly sets the \"Index\" attribute in the QualityLevel elements by increasing the number with each element.\nFixed\nImproved handling of intermittent networking issues during encoding to prevent stalling\nFragment default values are now set in each\ntraf\nbox of the segments and not in the\ntrex\nbox of the init file for\nfMP4 muxings\nfor Dolby Digital and Dolby Digital Plus audio codecs which caused playback problems on Roku streaming devices.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.107.0\nReleased 2022-01-25\nAdded\nCloud Region\nAWS_EU_NORTH_1\n(Stockholm) is now available for encoder version\n2.107.0\nand newer.\nFixed\nIf the quality metadata (\ncreateQualityMetaData\n) was activated for a DASH stream and the file extension of the init file was modified to be something other then \"mp4\", the mimeType of the init file was set to \"application/octet-stream\" instead of \"video/mp4\".\nFixed an bug in the V2 HLS manifest generation with\nProgressiveTs\nand\nTsMuxings\nwhich contain a video and an audio stream which lead to incorrect codec strings in the master playlist.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.106.0\nReleased 2022-01-18\nFixed\nSprites\nwith\ncreationMode=INTERVAL_START\nare now always generated, even if the input duration is shorter than the chosen Sprite's\ndistance\n. For example, distance is set to 10s but video is only 8s long, this will result in a sprite with two images, the first and the last frame.\nBugfix for a rare case when multiple input and output audio streams are used with concatenation.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.105.0\nReleased 2022-01-11\nAdded\nMP4\nand\nfMP4 muxings\ncan now be used for\nAV1 encodings\n. The following combinations are now supported with AV1:\nDASH manifests\nfMP4 with Widevine and PlayReady\nMP4, fragmented MP4, WebM, and ProgressiveWebM with Widevine\nMP4 with support for muxing video and audio together\nAV1\nturnaround times are now up to 30% faster\nAdded PTSAlignMode to\nfMP4 muxings\n. Setting it to\nALIGN_ZERO_NEGATIVE_CTO\nwill shift the first composition time offset (CTO) to 0. If B-frames are used, some CTOs will be negative. Therefore, TrackRun (trun) version 1 atoms are being used. This can only be set for h.264 and h.265 streams.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.104.0\nReleased 2021-12-21\nAdded\n3-pass can now be used for AV1 encodings to achieve optimal bitrate distribution\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.103.1\nReleased 2022-11-14\nAdded\nHLG output which is backward compatible with SDR is now supported: Using\nenableHlgSignaling\nin addition to setting the ARIB STD-B67 color transfer function will perform an ARIB STD-B67 conversion but signal it as BT.2020 10 bit. The ARIB STD-B67 conversion will be signaled in the SEI messages.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.103.0\nReleased 2021-12-14\nFixed\nFixed a bug where HEVC encodings with long input files could fail.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.102.0\nReleased 2021-12-07\nChanged\nImproved the generation of\nsprites\n,\nthumbnails\nand\nBIFs\nwhen using the Per-Title algorithm. Now, these additional resources are generated only for the rendition which has the highest resolution and bitrate to prevent duplicate generation.\nFixed\nFixed a bug in the manifestGenerator\nV2\nthat caused manifests with\nSPEKE DRMs\nto fail when creating a manifest via the\nstartEncoding call\nand configuring the V2 ManifestGenerator.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.101.0\nReleased 2021-11-30\nFixed\n3-pass encodings could have violated the\nHLS specification 1.30\nwhere \"Max-Bitrate should be no more then 200% of AverageBitrate\" when the last segment was only a few frames long. The Max-Bitrate calculation was now adapted to respect that very short segments typically have a very big bitrate value.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.100.0\nReleased 2021-11-23\nAdded\nSupport for adding DASH\nLabel\nelements to\nVideo\n-,\nAudio\n-\nSubtitle\n- and\nImage\nAdaptation Sets.\nChanged\nAV1 encodings:\nSpeed - Encodings run now\n5x faster\n.\nQuality - Encodings now show appr.\n30% higher quality\nwhen compared to HEVC/VP9 encodings.\nCost - Encodings are now significantly more cost effective. See our\nEncoding Minute Calculation Methodology\nfor detailed pricing information.\nCueIdentifierPolicy default for WebVttConfigurations for inputs with FileInputStreamType TTML and SRT: The default is now INCLUDE_IDENTIFIERS instead of OMIT_IDENTIFIERS.\nAdded\nstartOffset\nto TextMuxing and ChunkedTextMuxing. This is used for setting the MPEGTS value for the RFC 8216 (HLS Specification) X-TIMESTAMP-MAP for WebVtt outputs (e.g.:\nX-TIMESTAMP-MAP=MPEGTS:900000,LOCAL:00:00:00.000\n).\nWebVttConfigurations with CueIdentifierPolicy INCLUDE_IDENTIFIERS will now generate CueIds starting from\n1\ninstead of\n0\nif the FileInputStreamType is WEBVTT.\nFixed\nStreams with WebVttConfigurations will now be trimmed with the same offset and duration as the video stream of the same encoding if the FileInputStreamType is WEBVTT (This is the same behavior as TTML and SRT FileInputStreamTypes already used to have).\nFixed a bug which caused output subtitle cues' timestamps to be shifted by the value of the first video PTS, if FileInputStream is used for the subtitle stream\nKeyFrames are now placed correctly for streams with WebVttConfigurations\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-primetime-drm-protected-content",
    "title": "Creating PrimeTime DRM Protected Content",
    "text": "Overview\nAdobe PrimeTime\nis the successor of Adobe Access and was officially launched on the National Association of Broadcasters (NAB) show in 2013. PrimeTime is a DECE approved\nHollywood grade DRM\nthat is also approved by the\nUltraViolet standard\n. Adobe’s DRM offers a fine grained policy management system that allows to allowlist applications, devices, domains etc. and it also has support for key and license rotation. The Mozilla Firefox browser supports PrimeTime as Content Decryption Module (CDM) and therefore PrimeTime is natively supported through HTML5 in the Firefox browser.\nWe are currently working on PrimeTime support for our encoding service. Join our Newsletter to stay up to date with the latest updates!",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/downloading-the-encoded-video-directly-to-a-computer",
    "title": "Downloading the Encoded Video Directly to a Computer",
    "text": "It is not possible to do this directly. Our encoding service writes the encoded content directly to the output storage you specified in its configuration. From this storage, it is possible to download it to a local device.\nIn general, we highly recommend using our encoding service with a cloud storage like Google GCS and Amazon S3, as they perform very well in terms of bandwidth availability and scalability, and our encoding service can write to them directly. This enables you to keep the overall turn around time of your encoding as low as possible, so you can provide your content to your users. However, other output storage types like (S)FTP, Akamai NetStorage, and so on can also be used with our encoding service.\nThese tools are typically used to download content from a cloud storage or FTP server:\nThe web interface of your cloud storage provider\nhttps://console.aws.amazon.com/s3\nhttps://console.cloud.google.com/storage\ngsutil (CLI tool from Google, to manage your google cloud services)\naws-cli (CLI tool from AWS, to manage your AWS resources)\nS3Browser (GUI to browse/download files from your S3 bucket)\nDragonDisk (GUI for GCS/AWS S3 to browse/download files)\nFileZilla ((S)FTP Client)\nWinSCP ...",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-progressive-mov-mp4-and-ts-outputs",
    "title": "Creating Progressive MOV, MP4 and TS Outputs",
    "text": "Generating progressive outputs with the Bitmovin Encoding API is possible, and can simply be performed by using the according progressive MP4 / MOV / TS muxing type, instead of fMP4/TS muxings that are typically used for generating HLS or MPEG-DASH content. The main difference is, that instead of providing one stream to the muxing, you provide two - a video and a audio stream. You can add more than one as well.\nHINT: While creating MP4 outputs with multiple audio tracks with our service is possible as well, common web browsers don't support it to select a specific audio track, and therefore only play the first one available.\nOpen API SDK for\nJava example\nto create progressive muxings:\nJava\nHttpInput input = createHttpInput(configProvider.getHttpInputHost());\nString inputFilePath = configProvider.getHttpInputFilePath();\n\nOutput output =\n    createS3Output(\n        configProvider.getS3OutputBucketName(),\n        configProvider.getS3OutputAccessKey(),\n        configProvider.getS3OutputSecretKey());\n\n// Add an H.264 video stream to the encoding\nH264VideoConfiguration h264Config = createH264VideoConfig();\nStream h264VideoStream = createStream(encoding, input, inputFilePath, h264Config);\n\n// Create an MP4 muxing with the H.264 and AAC streams\ncreateMp4Muxing(\n    encoding,\n    output,\n    \"mp4-h264-aac\",\n    Arrays.asList(h264VideoStream, aacAudioStream),\n    \"video.mp4\");\n...\ncreateMp4Muxing() method:\n(\nExample\n)\nJava\nprivate static Mp4Muxing createMp4Muxing(\n      Encoding encoding, Output output, String outputPath, List<Stream> streams, String fileName)\n      throws BitmovinException {\n    Mp4Muxing muxing = new Mp4Muxing();\n    muxing.addOutputsItem(buildEncodingOutput(output, outputPath));\n    muxing.setFilename(fileName);\n    \n    for (Stream stream : streams) {\n      MuxingStream muxingStream = new MuxingStream();\n      muxingStream.setStreamId(stream.getId());\n      muxing.addStreamsItem(muxingStream);\n    }\n    \n    return bitmovinApi.encoding.encodings.muxings.mp4.create(encoding.getId(), muxing);\n  }\ncreateProgressiveTsMuxing() method:\nIn order to create a progressive TS muxing all you need to do is to add these code snippets to the example from before as well.\nJava\n...\n// Create a progressive TS muxing with the H.264 and AAC streams\ncreateProgressiveTsMuxing(\n    encoding,\n    output,\n    \"progressivets-h264-aac\",\n    Arrays.asList(h264VideoStream, aacAudioStream),\n    \"video.ts\");\nJava\nprivate static ProgressiveTsMuxing createProgressiveTsMuxing(\n    Encoding encoding, Output output, String outputPath, List<Stream> streams, String fileName)\n    throws BitmovinException {\n  ProgressiveTsMuxing muxing = new ProgressiveTsMuxing();\n  muxing.addOutputsItem(buildEncodingOutput(output, outputPath));\n  muxing.setFilename(fileName);\n  \n  for (Stream stream : streams) {\n    MuxingStream muxingStream = new MuxingStream();\n    muxingStream.setStreamId(stream.getId());\n    muxing.addStreamsItem(muxingStream);\n  }\n  \n  return bitmovinApi.encoding.encodings.muxings.progressiveTs.create(encoding.getId(), muxing);\n}\nSee the\nfull version\nfor this and many other examples our\nPublic Github Example Repository\nfor each of our Open API SDK's.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/choosing-the-right-encoder-version",
    "title": "Choosing the Right Encoder Version",
    "text": "We always recommend to make use of the special tags\nSTABLE\nand\nBETA\nwhen configuring your encoding workflows, as this allows you to automatically make use of the latest improvements in performance and stability and benefit from the latest bug fixes.\nFor your production workflow, we recommend using\nSTABLE\n.\nFor your development and staging workflows, it is good practice to run encodes using\nBETA\nto ensure that brand new versions still perform your encodings as expected, before that version becomes the next\nSTABLE\n.\nTo check what versions are currently tagged as\nSTABLE\nor\nBETA\n, and what features and fixes they contain, please go to\nhttps://bitmovin.com/docs/encoding/releases/encoder\n.\nFor more details about the encoder lifecycle, check\nhttps://developer.bitmovin.com/encoding/docs/bitmovin-encoder-lifecycle\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rest-api-services-12000-12490",
    "title": "REST API Services 1.200.0 - 1.225.0",
    "text": "1.225.0\nReleased 2025-04-01\nChanged\nInternal stability improvements\n1.224.0\nReleased 2025-03-18\nAdded\nCache-control settings are now applied to HLS and DASH manifests upon upload, if specified in the encoding. For LIVE encodings, the start call now supports setting cache-control settings for AWS and Google Cloud providers.\n1.223.0\nReleased 2025-03-11\nChanged\nInternal stability improvements\n1.222.0\nReleased 2025-03-06\nAdded\nIt is now easily possible to identify whether a manifest was generated using the legacy generator or V2. The information is available directly in the dashboard (Encoding → Manifest → Select a Manifest and check the 'Generator' field). Use of V2 is recommended.\n1.221.0\nReleased 2025-02-25\nFixed\nWhen using\nSCTE-35 triggers\nin 3-pass encodings, the\nCUE tags\nwere missing in the manifests.\n1.220.0\nReleased 2025-02-18\nChanged\nInternal stability improvements\n1.219.0\nReleased 2025-02-11\nChanged\nInternal stability improvements\n1.218.0\nReleased 2025-02-04\nAdded\nNow it is possible to choose between Long and Short format of timestamp fields, when creating DASH manifests\nLong format: PXXYXXMXXDTXXHXXMXX.XXXS\nShort format: PTXXHXXMXX.XXXS\n1.217.0\nReleased 2025-01-22\nAdded\ncodecMaxBitrateFactor\nand\ncodecBufsizeFactor\nare added to\nStreamPertitleSettings\n, to enable different\nhrd\nparameters per stream when using Per-Title.\nThe values calculated with these factors will override any other\nmaxBitrate\nrespectively\nbufsize\nvalue.\n1.216.0\nReleased 2025-01-14\nChanged\nInternal stability improvements\n1.215.0\nReleased 2024-12-17\nChanged\nInternal stability improvements\n1.214.0\nReleased 2024-12-10\nAdded\nWhen configured to be dynamic,\nprewarmed encoder pools\nnow have a maximum size of 10 instances. This will be enforced the first time the pool adapts its usage.\n1.213.0\nReleased 2024-12-03\nAdded\nImproved error feedback when\nEncoding Template\nexecution fails.\nAccelerated mode is now supported in the following Azure regions:\nEUROPE_WEST\n,\nUS_EAST2\n,\nCANADA_CENTRAL\n,\nUS_EAST\n,\nUS_WEST\n,\nJAPAN_EAST\n,\nGERMANY_WESTCENTRAL\n,\nEUROPE_NORTH\n,\nUS_WEST2\n,\nAUSTRALIA_EAST\n.\n1.212.0\nReleased 2024-11-19\nChanged\nThe\nLiveEdgeOffset\nproperty for\nHlsManifests\nin Live Encoding now supports values smaller than the segment length, allowing settings down to 0.\n1.211.0\nReleased 2024-11-12\nAdded\nEncoding Templates can now be managed with these new API endpoints:\nstore\n,\nlist\n,\ndetails\nand\ndelete\nEncoding Templates now support\ncustomData\nfields for any resource\nUsing\ndolbyVisionInputStream\nas an input for a\ntimeBasedTrimmingInputStream\nis supported now\n1.210.0\nReleased 2024-10-29\nChanged\nInternal stability improvements\n1.209.0\nReleased 2024-10-22\nAdded\nImplemented rollover behaviour for\nTimecodeTrackTrimming\nfor which the end time code of the input exceeds the 24h format. E.g., a configured trimming start time code of 00:00:00.000 would be interpreted as 24:00:00.000 for an input with a start time code of 23:59:45.000 and an end time code of 24:10:00.000.\n1.208.0\nReleased 2024-10-15\nAdded\nAdded seamless support for integrating\nAkamai Object Storage\nas both an Input source (for VOD) and an Output destination (for both VOD and LIVE products). Check out Bitmovin\ndashboard\nfor an easy setup or read through our\ndocumentation\n.\nFixed\nFixed an issue where encodings could potentially run out of disk space when processing\nDolbyVisionInputStreams\nwith input files larger than approximately 500GB. This update ensures smooth processing for large input files.\n1.207.0\nReleased 2024-10-02\nAdded\n\"\nresolutionScaleFactor\n\" is now available on all\npertitleStartConfigurations\n. With this parameter it is possible to influence the Per-Title results of the lower part of the Per-Title ladder. For negative values, Per-Title will start off with smaller resolutions. For positive values, Per-Title will start of with higher resolutions.\n1.206.1\nReleased 2024-09-26\nChanged\nThe\nmaxrate\nand\nbufsize\nrequirement on the\nH265/HEVC Codec Configuration\nin case of Per-Title + Dolby Vision output has been relaxed and is now not required to be set for Per-Title. Instead\ncodecMaxBitrateFactor\nand\ncodecBufsizeFactor\nare now required to be set to the\nh265PertitleStartConfiguration\nin case of  Per-Title + Dolby Vision output. With this, the combination of  Per-Title + Dolby Vision output +\nhrd\nparameter (needed for Dolby Vision) is now working as intended.\n1.206.0\nReleased 2024-09-24\nChanged\nInternal stability improvements\n1.205.0\nReleased 2024-09-17\nChanged\nImproved reporting in case an encoding fails to provide more information about the failure. Specifically:\nerrors when initializing the hardware encoding context\nwrong or missing permissions when uploading output to S3 storage\nmissing start code in input video streams preventing decoding\nIt is now a requirement for Dolby Vision encodings to set\nmaxBitrate\nand\nbufsize\non the\nH265/HEVC Codec Configuration\n1.204.0\nReleased 2024-09-10\nAdded\nAdded support for HLS manifest creation with WebVtt muxed in FMP4.\nFixed\nFixed wrong AVERAGE-BANDWIDTH and BANDWIDTH attributes for HLS I-frame playlists ( EXT-X-I-FRAME-STREAM-INF ) generated with manifest generator V2.\n1.203.0\nReleased 2024-08-27\nFixed\nFix for calculation of Live Units vs Encoding minutes\nusage statistics\n.\n1.202.0\nReleased 2024-08-13\nChanged\nStarting from\n2024-08-21 07:00:00 UTC\n, Encoder versions >=\n2.2.0\nand <\n2.103.0\nwill remap to STABLE\nFixed\nWhen using the\nResetLiveManifestTimeShift\ncall manifest playlists potentially started with different segment numbers.\n1.201.0\nReleased 2024-08-06\nAdded\nBitmovin Java API SDK now uses ApacheHttpClient as default HTTP client that enables support for PATCH requests.\n1.200.0\nReleased 2024-07-23\nAdded\nThe following progressive Muxings are now supported for Live Encodings (the files are generated and uploaded after the Live Encoding stopped):\nMP3\n,\nMOV\n,\nProgressive WebM\n,\nProgressive TS\n, and\nBroadcast TS",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-simple-s3-output-in-the-dashboard",
    "title": "Using Simple S3 Output in the Dashboard",
    "text": "Overview\nS3 role-based Inputs\nresp.\nS3 role-based Outputs\nare an alternative way for our services to access your AWS (Amazon Web Services) S3 bucket to be used as an Encoding Input and/or Output, or an Output for Analytics Exports.\nInstead of you providing your AWS Access/Secret key pair to our Encoding or Analytics service, we provide you with an AWS IAM (Identity and Access Management) user name, which you can grant specific access rights in your account so it can access your desired S3 bucket.\nIn this tutorial, we will create an\nS3 bucket in your AWS account\nwhich will serve as the input or output storage for your encodings.\nNOTE\n: This tutorial needs to be repeated for EACH account/sub-organization you want a S3-Role-Based access for.\nSetting up these buckets consists of 3 main sections:\nS3 Bucket permissions: select or create a new S3 Bucket and ensure general security permissions match the Bitmovin requirements.\nOutput creation in the Bitmovin UI: using the creation wizard create a new output using this S3 bucket and generate the policy.\nUpdate Policy and CORS: apply the generated policy to your S3 Bucket permission and ensure CORS is configured correctly for playback.\nRequirements\na\nBitmovin account\n. Please make sure that you run this tutorial on the same Bitmovin account (and sub-organization, if applicable) that you want to use for production. If you don't have or know your production account yet, remember to repeat this tutorial later on your production account.\nan\nAWS account\nto create the input and/or output bucket and the\nRole\nthat allows the Bitmovin system to access that bucket.\nS3 role-based buckets can be used for segmented outputs with\nencoder version\n2.29.0\nor higher.\nCreate an AWS S3 Bucket\nIn the AWS Management Console, open the\nS3 section\n.\nClick on the\nCreate Bucket\nbutton which starts the bucket creation wizard\nIn the \"Name and Region\" panel, choose a bucket name (for example\nmy-bitmovin-bucket\n) and a Region (for example\n(EU) Ireland)\n), then press\nNext\nConfigure the\nObject Ownership\ndepending on your needs\na.\nACLs enabled\n, this is\nrequired\nif PUBLIC_ACCESS is configured for Encoding outputs. Encodings that are started in our\nBitmovin Dashboard\nset the PUBLIC_ACCESS to enable preview playback.\nb. ACLs disabled. If this policy is used, Encoding outputs need to be set to have the PRIVATE ACL.\nConfigure\nsettings for playback\na. The default settings will\nBlock\nall\npublic access\nb. To\nenable playback\nfor manifests and files from the bucket,\nuncheck\n🔳\nBlock\nall\npublic access.\nComplete the wizard and click click\nCreate Bucket\nKeep in mind that playing directly from an S3 bucket can be expensive, and a CDN endpoint is typically cheaper.\nDashboard wizard step\nUpdate S3 Bucket Policy & CORS\nIn order to continue, you will have to update the\nPolicies\nin your AWS account.\nMake sure you have the AWS console open and have navigated to the S3 bucket permissions where the security settings were updated.\nIf you want to learn more about\nPolicies\nin AWS, please see their\ndocumentation\nNOTE\n: You will get JSON Payload by clicking copy button.\nApplying the JSON to S3\nEdit the bucket policy and paste the JSON file from the UI form to the Bucket.\nUpdating the CORS\nAlso under the Permissions tab in the AWS console, update the CORS settings.\nYou can copy the JSON below.\nJSON\n[\n    {\n        \"AllowedHeaders\": [\n            \"Authorization\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"*\"\n        ],\n        \"ExposeHeaders\": [],\n        \"MaxAgeSeconds\": 3000\n    }\n]\nThen edit the CORS policy and paste in the AWS Bucket, as shown in the image below.\nOnce created, these new S3 outputs will be available as Amazon S3 Role Based outputs.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/97ae4b0-image.png",
      "https://files.readme.io/281e789-image.png",
      "https://files.readme.io/b629497-image.png",
      "https://files.readme.io/7ba1524-image.png",
      "https://files.readme.io/3b626ee-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/aws-s3-access-key-live-outputs",
    "title": "AWS S3 Access Key Live Outputs",
    "text": "Overview\nDepending on the security policy your organisation has in place, you may be asked to provide Bitmovin with access to an S3 bucket with an Access and Secret Key pair. This guide will explain how to generate the keys, add permissions to the S3 bucket and then create and configure the output using the Bitmovin Dashboard or API.\nThis tutorial will walk you through the steps to: create an S3 bucket, configure it to your needs, generate access and secret key pair, attach it to an AWS IAM (Identity and Access Management) User with appropriate permissions to access the bucket and then create a Live Output resource in Bitmovin.\nCreate an AWS S3 Bucket\nIn the AWS Management Console, open the\nS3 section\n.\nClick on the\nCreate Bucket\nbutton which starts the bucket creation wizard\nIn the \"Name and Region\" panel, choose a bucket name (for example\nmy-bitmovin-bucket\n) and a Region (for example\n(EU) Ireland)\n), then press\nNext\nConfigure the\nObject Ownership\ndepending on your needs a.\nACLs enabled\n, this is\nrequired\nif PUBLIC_ACCESS is configured for Encoding outputs. Encodings that are started in our\nBitmovin Dashboard\nset the PUBLIC_ACCESS to enable preview playback.\nb. ACLs disabled. If this policy is used Encoding outputs need to be set to have the PRIVATE ACL.\nConfigure\nBlock Public Access settings for this bucket\na. The default settings will\nBlock\nall\npublic access\nb. To\nenable playback\nfor manifests and files from the bucket,\nuncheck\n🔳\nBlock\nall\npublic access.\nFinish going through the wizard and click\nCreate Bucket\nTo allow players to request content for streaming from your S3 bucket, you will also need to allow origin access with a CORS configuration. See\nHow can I configure an AWS S3 Bucket to test playback of my content?\non how to configure this for your bucket.\nYour bucket is now ready to be used.\nCreate an AWS IAM User\nNext we need to create a user that will be accessing the bucket. We continue working with the AWS Console\nOpen the\nIdentity and Access Management (IAM) console\nOn the left pane, click on \"Access Management\" -> \"Users\".\nClick on\nAdd User\n. The\nAdd User\npage appears.\nStep 1: add your desired User name.\nStep 2: Permissions\na. Make sure the user has the right permissions to for Amazon S3\nb. The easiest way to achieve this is AmazonS3FullAccess\nc. In “\nPermission options”\nselect “\nAttach policies directly”\nd. In\n“Permission policies”\nfilter and select AmazonS3FullAccess\n(Note: The pre-defined AmazonS3FullAccess policy is known to be suitable but since it provides unrestricted access to your bucket, you might need to create a custom policy with fine-tuned access rights. Please review details of the permissions required for\nbuckets for Encoding Input and Output\nbuckets and create a specific IAM Policy and associate it with this user as needed\n)\nGo through the next screens - those settings are optional and do not affect the configuration.\nOn the last screen, press\nCreate User\nSelect the newly created user in the\nUsers\noverview\nSelect the\n“Security credentials”\ntab\nScroll down to\n“Access keys”\nand hit the\n“Create access key”\nbutton\nFollow the\n“Create access key”\nsteps\na. For\n“Access key best practices & alternatives”\nselect\n“Application running outside AWS”\nb. You need to securely store\nAccess Key\nand\nSecret Access Key.\nYou need to store them somewhere securely as you will need them later. Note that once you've left this screen, you will not be able to retrieve the Secret Access Key anymore and will need to generate a new one.\nIf you want to learn more about Users in AWS, please see their\ndocumentation\n.\nCreate an S3 Output\nS3 input and output resources can be created via the Bitmovin API or in the Bitmovin Dashboard. The minimal required information to create an S3 input or output are the following :\nbucketName\n: the name of your S3 bucket\naccessKey\nand\nsecretKey\n: the Access Key ID and Secret Access Key obtained earlier.\nUsing the Bitmovin Dashboard\nIn the\nBitmovin Dashboard\nnavigate to Live Encoding ->\nOutputs\nPress\n+ Create\nand select\nAWS S3\nand a form will be provided where you can enter a Bitmovin name, that can be freely chosen will be seen by users in the dashboard when selecting an output, as well as the details required from AWS. Optionally you can also provide a description, for other users or admins to describe the function of the live output.\nRemember to select the correct cloud correctly.\nOnce your happy press\nCreate\nand the bucket will be saved.\nUsing the Bitmovin API\nThis example uses our latest\nOpen API client for Java\n, which is available on Github.\n(Java) S3 Output Example\nCreate a new S3 Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\n\nAclEntry aclEntry = new AclEntry();\naclEntry.setPermission(AclPermission.PRIVATE);\n\nList<AclEntry> acl = new ArrayList<>();\nacl.add(aclEntry);\n\nS3Output s3Output = new S3Output();\ns3Output.setBucketName(\"<BUCKET_NAME>\");\ns3Output.setAccessKey(\"<AWS_ACCESS_KEY>\");\ns3Output.setSecretKey(\"<AWS_SECRET_KEY>\");\ns3Output.setAcl(acl);\n\ns3RoleBasedOutput = bitmovinApi.encoding.outputs.s3.create(s3Output);\nHint: In case you chose to enable\nBlock public access\non your S3 bucket (recommended), you would have to make sure that the ACL is set to\nPRIVATE\non the output (as shown above) as well as on your Muxing configurations.\nTo create an Input is fairly similar, but you just use the\nS3Input\nresource and the\nbitmovinApi.encoding.inputs.s3\nendpoint\nUse an existing S3 Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\n\nS3Output s3Output = bitmovinApi.encoding.outputs.s3.get(\"YOUR_S3_OUTPUT_ID\");\n(CURL) S3 Output Example\nCreate a new S3 Output\nAPI reference:\ncreate an S3 Output\n:\nShell\ncurl -X POST \\\n  https://api.bitmovin.com/v1/encoding/outputs/s3 \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY' \\\n  -d '{\n    \"bucketName\": \"<BUCKET_NAME>\",\n    \"accessKey\": \"<AWS_ACCESS_KEY>\",\n    \"secretKey\": \"<AWS_SECRET_KEY>\",\n    \"acl\": [\n        {\n            \"permission\": \"PRIVATE\"\n        }\n    ]\n}'\nGet an existing S3 Output\nAPI reference:\nget an S3 Output\nShell\ncurl  https://api.bitmovin.com/v1/encoding/outputs/s3/YOUR_OUTPUT_ID \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY'\nUsing the Live Output\nThe bucket will appear in the Outputs list, and in the Wizard under AWS S3.\nYou can confirm the bucket is created in the API by using\nList S3 Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/132bb24-Screenshot_2024-04-05_at_18.41.26.png",
      "https://files.readme.io/f31250e-Screenshot_2024-04-06_at_11.28.57.png",
      "https://files.readme.io/55b15d6-Screenshot_2024-04-06_at_11.32.01.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/understanding-why-segment-duration-differs-from-the-defined-target",
    "title": "Understanding Why Segment Duration Differs from the Defined Target",
    "text": "When inspecting your streams (ie: HLS) you may notice that, despite a specific\n#EXT-X-TARGETDURATION\nbeing defined during the muxing, the video or audio segments have a different\n#EXTINF\nactual duration. For example:\n#EXTM3U #EXT-X-PLAYLIST-TYPE:VOD #EXT-X-VERSION:5 #EXT-X-MEDIA-SEQUENCE:0 #EXT-X-TARGETDURATION:4\n#EXTINF:3.989333333333333,\naudio_segment_0.ts\n#EXTINF:4.010666666666666,\naudio_segment_1.ts\n#EXTINF:3.989333333333333,\naudio_segment_2.ts\n#EXTINF:4.010666666666666,\naudio_segment_3.ts\nFor audio, this is totally normal and expected. To find the reasons behind this behaviour, we need to understand how segments are created:\nFor\nvideo\n, segments are created and closed upon keyframes, so if our target segment duration is 4 seconds (for example), we'll need to ensure there's a keyframe on the stream every 1, 2 or 4 seconds, as these are exact divisors of 4 and will guarantee that chunks are created/closed on such boundary. Thus, the actual duration of the video segments will be exactly 4 seconds, matching the defined target.\nFor\naudio\n, keyframes are not involved and the segments being created get filled with as many audio frames as needed to obtain a segment with the desired target duration.\nAudio frames have a fixed duration that depends on the specific codec and sample rate, so assuming AAC at 48KHz (for example), each audio frame within the stream will have a fixed duration of 21.333 miliseconds (1024 audio frames per sample / 48000 samples per second = 0.02133333333 seconds). Going back to our example, this value is not an exact divisor of the target duration (4s), so in order to keep the generated segments around the target, some of them will contain 187 audio frames (187 x 0.02133333333 seconds = 3.989333333333333 seconds) and others will contain 188 audio frames (188 x 0.02133333333 seconds = 4.010666666666666 seconds), which results on the\n#EXTINF\nvalues observed when inspecting the stream.\nThis alternance of shorter/longer audio segments targetting 4 seconds ensures that sync is kept across the stream and that the difference between audio segments will never be higher than one audio frame.\nIn workflows that demand perfect alignment between audio and video segments (ie: Server Side Ad Insertion), the keyframe interval chosen for the video will need to be an exact multiple of the audio frame duration, to ensure that both audio and video segments can be created/closed at the same exact boundaries. A useful tool for calculating the optimal keyframe intervals (GOP) in such cases is\nthis one",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-inputs",
    "title": "Live Inputs",
    "text": "Bitmovin supports a range of input protocols, and the following articles provide guidance on how to configure them.\nBitmovin offers main and backup inputs as standard for all users and all live encodings for RTMP and SRT inputs at no additional cost.\nRTMP\nSRT\nZixi\nFAQ\nWhat restrictions are there for input signals frame size and frame rate?\n✅ Input signals can be provided in any aspect ratio\nDo you support Variable framerates in an input\n✅ Yes, from version 2.210.0. We now support contribution devices that output variable frame rates, the live encoder will output a constant bitrate based on the average frame rates conforming to a common standard of 24, 25, 29.97, 30, 50. There are some limits to the level of variable frame rates that we support; for example, if we receive variable framerates in excess of 60fps, the encoding could become unstable and result in an error.\nDo you support audio-only inputs?\n❌ No, At this time, we only support input feeds that have a video stream and an audio stream or just a video stream. An audio-only input feed will not be successful.\nDo you support different input codecs?\n✅ Yes, we support the following video and audio codecs from an input contribution feed.\nVideo\nMPEG-2\nMPEG-4\nH.264/AVC\nH.265/HEVC\nVP9\nAudio\nAAC\nVorbis\nCan you support Multiple Bitrate inputs?\n❌, No we only support inputs with single bitrate renditions.\nCan I use one live encoding to provide an output and switch between multiple inputs?\n✅ Yes, It is possible to connect and disconnect different devices from the same Live Encoder; however, the format, codecs, and configuration of frame size and frame rate\nmust\nremain consistent.\nThe Live Encoder is fixed to the settings for the first input that connects to it, and that can not be changed without creating a new live encoding.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/srt-inputs",
    "title": "SRT Inputs",
    "text": "Bitmovin supports Secure Reliable Transport inputs, and is proudly a platinum member of the\nSRT Alliance\ntaking part in interoperability tests.\n📘\nWe offer support for Main and Backup SRT inputs to each Live Encoder as standard.\nIn this guide we'll explain how to configure inputs using both the API and UI.\nBitmovin supports both Caller and Listener mode and AES-128 and AES-256 encrypted inputs.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/akamai-netstorage-for-live-outputs",
    "title": "Akamai NetStorage for Live Outputs",
    "text": "Overview\nThe NetStorage service of Akamai provides many ways to interact with it. The best way from our perspective is to use its HTTP API interface. In this tutorial you will learn how it works and what it needs to obtain proper credentials to create an Akamai NetStorage Input/Output in our service.\n🚧\nWhile it is possible to configure and use Akamai NetStorage to work with the Live Encoder, for the best performance we would advise using\nAkamai MSL4\nfor Live Enocding\nIMPORTANT\n: This tutorial assumes that you already have access to an Akamai NetStorage and Setup and a dedicated Upload Account for it. If you are missing any of it yet,\nplease read through this tutorial first\non how to create an Akamai NetStorage and Upload Account before you proceed with this tutorial.\nCreate an Bitmovin Akamai NetStorage Output\nTo create a Bitmovin\n\"Akamai NetStorage\" Output configuration\n, that can be used by our encoding service, you need to know the\nHTTP API Key\nand\nUsername\nof your upload account, as well as its\nHost\nURL.\nUse the Dashboard UI\nSelect the\nLive Encoding\nmenu on the left and go to\nOutputs\n.\nClick on\nCreate\nin the upper right corner of the Outputs overview\nSelect\nAkamai\nas Output type and\nEnter all required fields (please see the image below)\nClick on\nCreate\nWhen you are finished press\nCreate\nand the output will be saved.\nUse a Bitmovin API SDK\nEach of our Open API SDK's implements the Bitmovin API, and make it easy to start its integration in your project or use-case. Use them to create reusable output resources to be used for your encodings:\nBitmovin API SDK for Java - Output example\nJava\nAkamaiNetStorageOutput output = new AkamaiNetStorageOutput();\noutput.setHost(AKAMAI_NETSTORAGE_HTTP_API_HOST); // e.g. xxxxx-nsu.akamaihd.net\noutput.setUsername(AKAMAI_NETSTORAGE_USERNAME);\noutput.setPassword(AKAMAI_NETSTORAGE_PASSWORD);\noutput = bitmovinApi.output.akamaiNetStorage.create(output);\nSee all available examples for each of our Bitmovin API SDK's in our\nGH Example Repository\n.\nUsing the Live Output\nThe bucket will appear in the Outputs list, and in the Wizard under Akamai Netstorage.\nYou can confirm the bucket is created in the API by using\nLive Akamai NetStorage Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/44b1cf6-Screenshot_2024-04-05_at_19.10.21.png",
      "https://files.readme.io/4722207-Screenshot_2024-04-06_at_11.39.20.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/automatic-shutdown-options",
    "title": "Automatic Shutdown Options",
    "text": "Overview\nAutomatic Shutdown offers three shutdown options to suit different scenarios\nPlanned Automatic Shutdown\nAutomatically stops the encoding based on a predefined schedule. Users can set the duration in minutes or hours, after which the encoding will terminate. This is useful for scheduled broadcasts where the end time is known.\nStop Encoding if No Input is Detected After Startup\nIf no input signal is received within a specified time after starting the encoding, it will automatically shut down. This option helps avoid unnecessary encoding if there’s an issue with the input source.\nStop Encoding if Input Signal Disconnects and Doesn’t Reconnect\nThis option stops the encoding if the input signal is lost and fails to reconnect within a set period. It’s beneficial for scenarios where a consistent input stream is required, helping avoid resource usage during extended downtime.\nHow to Use\nNavigate to\nStep 7/7\nin the\nCreate New Live Encoding\nwizard.\nUse the toggles to enable/disable the three shutdown options.\nEnter the desired duration in minutes, hours or days for each option.\nThe encoding will automatically shut down when the specified time is reached.\n📘\nDemo Inputs\nIf the Bitmovin Demo Input is used, the\nPlanned Automatic Shutdown\nfeature is automatically enabled by default and cannot be disabled. This ensures that resources are protected when using demo content for testing.\nUsing the Bitmovin API\nWe provide the following API endpoints to configure the\nautoShutdownConfiguration\nfeature for Live Encoding. See\nautoShutdownConfiguration documentation\nbytesReadTimeoutSeconds\nShuts down the live stream automatically if no input is detected within the specified number of seconds (minimum value: 30 seconds).\nstreamTimeoutMinutes\nEnds the live stream automatically after it has been running for the specified number of minutes (minimum value: 5 minutes).\nwaitingForFirstConnectTimeoutMinutes\nTerminates the live stream if no input connection is established within the specified number of minutes (minimum value: 5 minutes).",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/593cbcb5ac9199c2cdde6baffc8b00066164fee5b08f8e0656d4f9864d471d0f-Automatic_Shutdown_Options.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-an-akamai-netstorage-upload-account",
    "title": "Creating an Akamai NetStorage Upload Account",
    "text": "Overview\nIn this tutorial we will walk through how to create a dedicate Upload Account for an existing Akamai NetStorage Setup. If you don't have one yet, but want to create one, please consult the\nofficial documentation of Akamai\n.\nIf you already know how create an upload account, and where to find the necessary details you can already proceed with the following tutorials:\nHow to create an Akamai NetStorage Input\nHow to create an Akamai NetStorage Output\nCreate/Edit your Akamai Upload Account\nUpload accounts can be created in the Backend portal of your Akamai account and are dedicated \"user\" credentials used to control what and how someone can access your NetStorage and its content. What we are looking into here is how to enable the\nHTTP Access Method\n, respectively where you can find the information you need to create an Akamai NetStorage Input/Output in our service.\nLogin to your Akamai Account at\nhttps://control.akamai.com\nIn the menu click on\nCONFIGURE\n=>\nNetStorage\n=>\nConfiguration\nIn the side bar on the left, click on the\nUpload Accounts\nicon\nCreate a new Upload Account\nIf you have no upload account available yet, you will have to create one first, please follow this guide in the\nAkamai documentation\n.\nEdit an existing Upload Account\nIf you already have an upload account that can access the directories you want to transfer your encodings to, you can simply edit this one and grant it permissions to HTTP Access as well. In your Upload account overview you also see the access methods that are already enabled for your existing upload accounts. If the one you want to use already states\nHTTP\n, this account can be used already to create an Akamai NetStorage Input/Output in your Bitmovin account.\nGet your HTTP API credentials\nYour Akamai NetStorage HTTP API credentials consist of the\nHTTP API Key\nand\nUsername\nof your upload account.\nUsername\n- The username is shown in your Upload account overview in the column\nUpload Account\nHTTP API Key\n- This key is used to authenticate against the\nAkamai NetStorage HTTP API\nand is shown only when you edit an Upload account\nFind your upload account in your overview, click on the three dots on the right side of the row and select\nEdit\n.\nFind the \"Access Method\" area. If no key is listed yet, create a new one with a click on\nAdd or Rotate HTTP API Key\n(1). Then copy the\nKey\n(2).\nFind your NetStorage HTTP API URL\nThis URL is shown in the details view of your Storage Group in the Akamai Backend: Select the \"Storage Group\" section on the left side (blue cloud symbol), then select the storage group you are using in the list by clicking on it. You will be presented with a view like in the image below. The URL stated next NetStorage HTTP API, is the one you would have to use the host URL when creating an Akamai NetStorage Input/Output in our service.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/6f8ea8e-image.png",
      "https://files.readme.io/0ecff0a-image.png",
      "https://files.readme.io/1dc9a86-image.png",
      "https://files.readme.io/d29fd83-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/understanding-mpeg-cenc-clearkey-encryption",
    "title": "Understanding MPEG-CENC ClearKey Encryption",
    "text": "MPEG-CENC ClearKey\nprovides basic encryption of your content and is a viable option if you do not want to pay for the use of a full DRM service. You should keep in mind that this kind of encryption does not provide the same security level as if using a DRM system.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-widevine-drm-protected-content",
    "title": "Creating Widevine DRM Protected Content",
    "text": "Overview\nWidevine is a Hollywood grade DRM technology initially developed by Widevine Technologies and acquired by Google in 2010. For Google it was not only a technology acquisition it was also a strategic play. The acquisition opened new connections into the premium video sector and deepened also the relationship with Hollywood. Beside, that the technology fits perfect into the Google ecosystem and plays well together with Android and YouTube, which will help to expand their overall video business. Widevine is natively\nsupported on broad range\nof devices and browsers such as Google Chrome Browser, Android, Chromecast, etc.\nWidevine DRM Requirements\nIn order to use Widevine DRM a DRM solution provider (e.g. Irdeto, EZDRM, ExpressPlay, Axinom, etc.) is required, or you integrate it a Widevine Proxy Service yourself.\nAbout this example\nThe code snippets shown here are based on the\nfull example\ncalled\nCencDrmContentProtection.java\n, using our\nBitmovin SDK for Java\n.\nHint:\nIf you haven't created any encodings with our Service yet, its recommended to start with our quick start guide called \"\nGet Started with the Bitmovin API\n\" first, before you continue :)\nEncoding with DRM Configuration\nEncryption details\nTo encrypt your content so it can be used with Widevine DRM, an\nencryption key\n(referred to as\nDRM_KEY\nin the example) is required. All other values are optional, however sometimes required by specific DRM vendors, therefore have to be set (\nDRM_WIDEVINE_KID\n,\nDRM_WIDEVINE_PSSH\n, ...).\nThese values of the Widevine DRM configuration are expected in the following format:\nDRM_KEY: (required) You need to provide a key that will be used to encrypt the content (16 byte encryption key, represented as 32 hexadecimal characters)\nDRM_WIDEVINE_KID: also known as Key ID, or ContentID. Its a unique identifer for your content (16 byte initialization vector, represented as 32 hexadecimal characters)\nDRM_WIDEVINE_PSSH: Base64 encoded String, PSSH payload Example:\nQWRvYmVhc2Rmc2FkZmFzZg==\nHINT:\nSome DRM providers provide you with a dedicated service to create and safely store Encryption Keys, so you don't have to create and manage them by yourself. These values are required to generate a proper playback license using DRM solution providers like Irdeto, EZDRM, ExpressPlay, Axinom, etc. to control playback permissions on the client side.\nLearn more\n.\nMuxing DRM Configuration\nWhen you create an encoding, you define\nStreams\nwhich point to the video track of the input file, and couple them with an\nCodec Configuration\nof your choice. These\nStreams\nare then used by\nMuxings\nwhich determine the desired output format of your content.\nWidevine DRM can be used with\nfMP4 Muxings\nor\nWebM Muxings\n, therefore works with H264 or VP9. In order to configure a\nMuxing\nto create DRM encrypted content, you add a\nCencDRM Configuration\n(\nAPI reference\n) to it. Its a general DRM configuration object, where you can provide the required\nkey\nto encrypt the content, along with DRM solution specific details.\nThe example creates two\nStreams\n- one for the video-\nStream\n(H264) and one for the audio-\nStream\n(AAC). For each of the\nStreams\nand\nfMP4 Muxing\nis created, without providing a output destination.\nJava SDK Example - create Encoding\n(\nLine in Example\n)\nJava\nEncoding encoding =\n    createEncoding(\"fMP4 muxing with CENC DRM\", \"Example with CENC DRM content protection\");\n\nHttpInput input = createHttpInput(configProvider.getHttpInputHost());\nOutput output =\n    createS3Output(\n        configProvider.getS3OutputBucketName(),\n        configProvider.getS3OutputAccessKey(),\n        configProvider.getS3OutputSecretKey());\n\nH264VideoConfiguration h264Config = createH264VideoConfig();\nAacAudioConfiguration aacConfig = createAacAudioConfig();\n\nStream videoStream =\n    createStream(encoding, input, configProvider.getHttpInputFilePath(), h264Config);\nStream audioStream =\n    createStream(encoding, input, configProvider.getHttpInputFilePath(), aacConfig);\n    \nFmp4Muxing videoMuxing = createFmp4Muxing(encoding, videoStream);\nFmp4Muxing audioMuxing = createFmp4Muxing(encoding, audioStream);\n...\nIn typical DRM Encoding workflows the\nOutput\nconfiguration is part of The DRM Config\nonly\n. It is added to the muxing by the\ncreateDrmConfig\nmethod. So, it defines which output shall be used to store the encrypted content.\nJava SDK Example - Add DRM Config to Muxing\n(\nLine in Example\n)\nJava\ncreateDrmConfig(encoding, videoMuxing, output, \"video\");\ncreateDrmConfig(encoding, audioMuxing, output, \"audio\");\nIMPORTANT:\nYou can still provide an\nOutput\nconfig to the fMP4 Muxing as well, but it would push the\nunencrypted version of your encoded content\nto this\nOutput\ndestination. So, if you want to create encrypted content only, provide an\nOutput\nconfiguration exclusively as part of your DRM Config.\nJava SDK Example - createDrmConfig() Method\n(\nLine in Example\n)\nJava\nprivate static CencDrm createDrmConfig(\n    Encoding encoding, Muxing muxing, Output output, String outputPath) throws BitmovinException {\n  CencDrm cencDrm = new CencDrm();\n  cencDrm.addOutputsItem(buildEncodingOutput(output, outputPath));\n  cencDrm.setKey(configProvider.getDrmKey());\n  cencDrm.setKid(configProvider.getDrmWidevineKid());\n  \n  CencWidevine widevineDrm = new CencWidevine();\n  widevineDrm.setPssh(configProvider.getDrmWidevinePssh());\n  cencDrm.setWidevine(widevineDrm);\n  \n  ...\n  \n  return bitmovinApi.encoding.encodings.muxings.fmp4.drm.cenc.create(\n      encoding.getId(), muxing.getId(), cencDrm);\n}\nThat's it, so we can start this encoding now with\nexecuteEncoding()\nand create HLS and MPD manifests for it, after the encoding is finished.\nJava SDK Example - Start Encoding and create HLS and MPD Manifests\n(\nLine in Example\n)\nJava\nexecuteEncoding(encoding);\n\ngenerateDashManifest(encoding, output, \"/\");\ngenerateHlsManifest(encoding, output, \"/\");\ngenerateDashManifest\nand\ngenerateHlsManifest\nare using the DefaultManifest API call for MPEG-DASH (\nExample\n) and HLS (\nExample\n). These methods are easy to configure and enable to you create a simple manifest that for each streaming format that reference all the contents that were created and can be used by the respective Streaming Format.\nWhile your MPD, will only show the DRM protected Video- and Audio track protected with Widevine DRM, your HLS manifest holds the details to play this content with Widevine or Fairplay DRM.\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/manifest-generator-v2",
    "title": "Manifest Generator V2",
    "text": "The manifest generator is the component that builds the actual manifest text file(s) that are written to the output storage, based on a manifest API resource which you configured beforehand.\nThere are two major versions of this component:\nLEGACY\n: As the name suggests, this is outdated and should only be used in existing and proven workflows.\nV2\n: A new and completely overhauled implementation of the generator that brings many improvements regarding consistency, compatibility and specification conformity of manifests. It receives constant improvements via minor version upgrades and is the recommended choice.\nThis guide will help you assess if there is a need to take action and explains how to migrate to V2 if desired.\nTo whom this applies\nThe manifest generator that is used by default depends on the initial\nsign-up date\nof your organization:\nafter 2023-05-31\n: The recommended version (\nV2\n) is your default for all scenarios and it's the only version your are allowed to use. There's nothing you need to care about.\n2021-07-27 to 2023-05-31\n: The recommended version (\nV2\n) is your default for just-in-time manifests. For post-encoding manifests, it is\nLEGACY\n. If you are using the latter, you might consider migrating to\nV2\n.\nbefore 2021-07-27\n:\nLEGACY\nis your default for all scenarios and you might consider migrating to\nV2\n.\nExceptions to the above:\nLive-2-VoD\nis not yet supported by V2. A post-encoding manifest that references a LIVE encoding will always automatically fall back to the LEGACY generator.\nOld encoder versions\n: When using an encoder version older than the minimum versions listed further below, the mentioned defaults do not apply, but the LEGACY generator will be used.\nWhy you should migrate\nOne of the main reasons for introducing a new major version was the fact that post-encoding manifests and just-in-time manifests use separate LEGACY implementations with slightly different feature sets. With V2, both ways of generation yield equal results.\nFeatures and improvements introduced after the release of V2 are only available in V2. LEGACY will not receive any further minor version upgrades.\nA central goal for generator V2 was to improve consistency, compatibility and specification conformity of the resulting manifests.\nImprovements of V2 over LEGACY\nThis is a detailed listing of the changes to expect in the generated manifests when migrating from LEGACY to V2. It is up-to-date as of 2023-05-31. All future improvements or fixes can be found in the release notes.\nDASH\nMPD, Period and Representation elements have an\n@id\nattribute with their value corresponding to the IDs of the API resource.\nIf configured, the\n@dependencyId\nattribute on Representation elements is always set for all, not just Dolby audio representations.\nThe MediaPresentationDurations can be slightly different from being more spec conform.\nThe DASH timestamps of Segment entries in segment timeline manifests can be slightly different because V2 uses more precise data.\nElement order of AdaptationSets is more consistent in On-Demand manifests.\nLEGACY always uses the profile\nurn:mpegprofile:isoff-live:2011\nfor default manifests.\nV2’s profile depends on the codec types in the video adaptation sets.\nH264:\nurn:mpeg:dash:profile:isoff-live:2011,urn:com:dashif:dash264\nVP9:\nurn:mpeg:dash:profile:full:2011\nothers:\nurn:mpeg:dash:profile:isoff-live:2011\nPlease note that this only pertains to DASH-live profile manifests. For DASH-on-demand manifests, the profile string is always\nurn:mpeg:dash:profile:isoff-on-demand:2011\n.\nV2 adds the attribute contentType=text to subtitle representations which LEGACY does not add.\nV2 does not truncate the protocol part of the\nvttUrl\nspecified for DashVttRepresentation (which will be the BaseURL text content in the resulting manifest XML).\nSetting\nvttUrl\nto\nhttps://some-domain.com/some-path.vtt\nresults in:\nLEGACY:\n<BaseURL>//some-domain.com/some-path.vtt</BaseURL>\nV2:\n<BaseURL>https://some-domain.com/some-path.vtt</BaseURL>\nFor DASH On Demand (single-file muxing) manifests, V2 correctly inserts multiple “Role” elements on AdaptationSets. LEGACY writes the roles comma-separated, whereas V2 now correctly inserts a Role element for each one configured. An example is shown below:\nLEGACY:\n<Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"alternate,commentary\"/>\nV2:\n<Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"alternate\"/>\n<Role schemeIdUri=\"urn:mpeg:dash:role:2011\" value=\"commentary\"/>\nFor DASH On Demand (single-file muxing) manifests, if the customer sets the duration of a period resource, V2 propagates the set period duration to the duration of the period and the MediaPresentationDuration.\nLegacy does not do that, i.e. the period duration is not set in the DASH manifest nor propagated to the MediaPresentationDuration.\nNote: For other DASH Profiles (i.e. DASH Live), the set period duration is propagated by both Legacy and V2.\nDisclaimer: Explicitly setting period durations is not recommended.\nIf\nstartSegmentNumber\nor\nstartKeyframeId\nare set in combination with representations of type\nTIMELINE\n,\npresentationTimeOffset\nis calculated more precisely, preventing A/V sync issues that can occur with LEGACY.\nAttributes of XML elements can be ordered differently.\nHLS\nThe attributes of elements are ordered and their order is stable, making comparisons significantly easier.\nV2 adds\nassocLanguage\nand\ncharacteristics\nattributes where needed. LEGACY does not support adding them.\nThe codec value of Variant Streams for ProgressiveTsMuxings and TsMuxings, which contain a video and an audio stream, does not contain duplicate values in V2\nLEGACY:\navc1.4D401E,ac-3,avc1.4D401E,ac-3\nV2:\navc1.4D401E,ac-3\nI-FramePlaylists which point to a muxing that does not support I-FramePlaylists (e.g. TsMuxings, or Mp4Muxings with a\nFragmentMp4ManifestType\nother than\nHLS_BYTE_RANGES_AND_IFRAME_PLAYLIST\n) fail instead of writing an invalid file to the storage. LEGACY writes a file that only contains the word “null”.\nThe target duration is sometimes not updated to the highest occurring value in all media playlists by LEGACY. V2  handles this correctly.\nFor MediaInfos/StreamInfos, where the streamId set by the customer is empty/blank, V2 sets the segmentDurations more accurately.\nV2 handles incorrectly set streamIds differently:\nif the streamId is not set on the variantStream/mediaInfo\nif the muxing only has one stream, we use this stream\nif the muxing has more than one stream, the manifest generation fails\nif the streamId is set\nif it points to a stream that is not part of the muxing, the manifest generation fails\nIf multiple ClosedCaptionMediaInfos are configured for a single manifest, LEGACY adds duplicated entries for each; this is fixed in V2.\nSMOOTH\nV2 correctly sets the\nQualityLevels\nattribute in the StreamIndex elements to the count of QualityLevel elements present.\nV2 correctly sets the\nIndex\nattribute in the QualityLevel elements by increasing the number with each element.\nHow to migrate\nBoth generator versions rely on the exact same API resources for configuring your manifest, making it very easy to migrate.\nDepending on your workflow, you are using one of these \"start\" calls to trigger the generation of manifests:\npost-encoding manifests:\nDASH:\nStart DASH Manifest Creation\nHLS:\nStart HLS Manifest Creation\nSmooth Streaming:\nStart Smooth Streaming Manifest Creation\njust-in-time manifests\nVoD:\nStart Encoding\nLIVE:\nStart Live Encoding\nAll of those requests can contain an optional property\nmanifestGenerator\n, which allows to opt for version\nV2\n. (Note that opting for\nLEGACY\nis not possible if\nV2\nis your default)\nMinimum encoder version\nJust-in-time manifests\n: The minimum encoder version that supports V2 is\n2.70.0\n(for all manifest types). Note that the minor version of the generator is bound to the encoder version. The newer the encoder, the newer the manifest generator.\nPost-encoding manifests\nrely on data produced by the encoder and are therefore also dependent on the encoder version. Using an up-to-date encoder ensures best results. The following minimum versions apply:\nDASH: 2.121.0\nHLS: 2.111.0\nSmooth Streaming: 2.108.0",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/bitmovin-cloud-connect",
    "title": "Bitmovin Cloud Connect",
    "text": "Historically, Bitmovin has provided its API through a Managed Cloud installation. Each customer's Bitmovin instance can be hosted on the cloud provider of their choice with backups on the other cloud providers’ infrastructure for redundancy.\nThe major limitation to this setup is that your video data must be passed to Bitmovin's servers. This may be a problem if you have restrictive content agreements or have already negotiated favorable rates with a cloud hosting provider. Fortunately, Bitmovin Cloud Connect encoding now makes it possible to install Bitmovin's API directly on your infrastructure, so you can ensure that your private data never leaves your cloud data center.\nCloud Connect is currently available on Amazon AWS, Microsoft Azure, and Google Cloud Platform. For more information on use cases and benefits, please read our Blog post \"\nBring Your Own Cloud: Introduction to Bitmovin Cloud Connect Encoding\n\".\nUsing Bitmovin Cloud Connect with cloud providers\nUsing Bitmovin Cloud Connect with AWS\nUsing Bitmovin Cloud Connect with Azure\nUsing Bitmovin Cloud Connect with GCP",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-encoding-standby-pools",
    "title": "Live Encoding Standby Pools",
    "text": "Introduction\nThe Bitmovin Live Encoder has been optimised to start up as quickly as possible, and while the software deployment and configuration takes around 10 seconds one element that Bitmovin can not control is the time it takes for a cloud provider to allocate a machine instance for deployment. This time is seen in the dashboard as the queued time, and this can vary from cloud provider, region and current demand.\nQueue time is displayed in the Live Encoding Status page Event Timeline\nFor some use cases, even 10 seconds is too long to wait and there are requirements for instant availability of a Live Encoder, providing ingest and output. These use cases could be a platform offering user generated content (similar to Twitch or YouTube) or breaking news platforms.\n📘\nLive Standby Pools remove queue and configuration time\nThe Live Standby Pool feature essentially allows customers to have a consistent number of Live Encoders, started, configured and ready to receive an input: offering the ability to go live instantly.\nFor these customers, Bitmovin offers Live Standby Pools which removes the queuing, and configuration time.\nLive Standby Pools are not to be confused with the VOD\nPre-Warmed Pools\nwhich are exclusively available for the VOD Encoding product.\n👍\nAvailable for all Enterprise customers\nThis feature can be enabled for any customer with a enterprise encoding plan (not Trial or PAYG).\nIf you want to explore the feature, please contact your account manager.\nLive Standby Pool FAQs\n1 Are there pricing implications for enabling Live Standby Pools?\nYes. Bitmovin will not charge any additional license fee to enable the feature, however any Live Encoder that is running in a Live Standby Pool will be treated as a running Live Encoding and billed at the agreed rate. The total price of a Live Encoder that originated in a Live Standby Pool will therefore include Pool + Acquired and in use time.\n2 Can I estimate any changes in price?\nYes. You will need to consider the following components:\nThe number of Live Standby Pools you want to be configured\nThe size of the Live Standby Pool\nLive Encoder configuration (Live Encoder HD or Custom Live Configuration options).\nExample: A customer wants to have two pools configured, both using Live Encoder HD options at a rate of $3/hr\nLive Pool 1 = 10\nLive Pool 2 = 5\nIn addition, they expect they'll have 20 in use at any time.\nTherefore, they would have 10 + 5 + 20 = 35 Live Encoders running, at a cost of $3 x 35 = $105 per hour.\n3 Can a Live Standby Pool have multiple configurations?\nNo. Only one configuration can be applied per Live Standby Pool. If multiple configurations are required, which would include cloud provider or region, multiple Live Standby Pools should be created for each configuration.\n4 Can I change the Encoding Template for a running Live Standby Pool?\nYes. A running Live Standby Pool can be updated at anytime, without effecting the current live encoders in a running or acquired state.\n5 Is it possible to update all ready live encoders to use a new Encoding Template?\nYes. Customers have full control of how this change is rolled out. When a Live Standby Pool has the Encoding Template updated, any Live Encoders currently in a running state will have been configured with the previous configuration, based on the previous Encoding Template, only new Live Encoders will start with the new Encoding Template; which for a time would result in pool hosting live encoders with a mix of configurations.\nIf this configuration needs to be updated immidiately, then a pool can be manually re-sized, by setting the pool size to 0, waiting for the live encoders in a ready state to stop, then set the pool size back to the correct value. This operation will trigger a batch of new live encoders to start using the correct Encoding Template, this could take some time so it is recommended to do this during a quiet period of activity.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/4fe9c7ed1af2fa4ea8e7cbd53345d059844444fed39dd41a2358294aff4a2c1e-Screenshot_2024-11-22_at_17.38.02.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/best-practice-guide-rest-api-5xx-errors",
    "title": "Best Practice Guide: REST API 5xx Errors",
    "text": "Introduction\nThe objective of this best practice guide is to provide information on what HTTP server 5xx errors mean; why and where they can possibly occur; and what could be done when they actually occur while using Bitmovin Encoding APIs.\nAudience\nAll users of Bitmovin APIs.\nBrief on 5xx Errors\nHTTP response codes starting with \"5\" indicate scenarios where the server is aware that it has encountered an error or is otherwise incapable of performing the request. This document focuses mainly on the HTTP errors 500, 502, 503, and 504.\nDescription of HTTP Errors\n500 Internal Server Error: A generic error message, given when an unexpected condition was encountered and no more specific message is suitable.\n502 Bad Gateway: The server was acting as a gateway or proxy and received an invalid response from the upstream server.\n503 Service Unavailable: The server cannot handle the request (because it is overloaded or down for maintenance). Generally, this is a temporary state.\n504 Gateway Timeout: The server was acting as a gateway or proxy and did not receive a timely response from the upstream server.\nCall Flow\n5xx error can occur due to one of Bitmovin’s services or any of the components on a network (such as DNS servers, switches, load balancers, proxies, and others) generating errors anywhere in the life of a given request. Prior to contacting Bitmovin, the users must check their network connectivity to ensure the error is not due to the local network or the ISP. Moreover, an error occurring outside of Bitmovin’s services is not visible to Bitmovin thereby making it difficult to find the cause.\nRemediation or possible next steps\nA retry from the user's end should result in a proper response from the server after a while (Please read\nRetry Mechanism\n).\nAdditionally, for 504 Gateway Timeout:\nIf this error occurs on the Encoding start call, it might be that the service still managed to get the Encoding job started. Before retrying the encoding start call we therefore advice to check the encoding status first. If the encoding status is not “CREATED” anymore, it means the job was already started. If the encoding start call is retried on an already started job, the service would return 409 CONFLICT.\nImportant information:\nBitmovin status page\nwill be updated in case of elevated 5xx errors. Users can subscribe to the notifications whenever Bitmovin creates, updates or resolves an incident.\nFor automating workflows, users can access the status using the API:\nhttps://status.bitmovin.com/api/v2/status.json\n(documentation:\nAtlassian Statuspage Status - API\n)\nIn most cases, a retry (as suggested under\nRetry Mechanism\n) should result in a proper response. If that does not happen, Bitmovin will be able to provide more details on the error that occurred. The user needs to record the instance and provide information to Bitmovin (RequestID, UTC time when the error occurred, error code and message, API being exercised, Response body, etc.) for further investigation.\nBitmovin Encoding SDKs\nare yet to be updated according to the recommendations above in order to gracefully handle 5xx errors.\nRetry Mechanism\nIt is recommended that\nExponential Back-off\nor a similar algorithm be used. For a more robust implementation,\nExponential Backoff can be combined with Jitter\n.\nFurther Reading\nAdditional error codes Bitmovin APIs can return are explained here:\nhttps://bitmovin.com/docs/encoding/api-reference/help",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/de663e1-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/supported-hdr-formats-and-conversions-in-bitmovin-encoder",
    "title": "Supported HDR Formats and Conversions in Bitmovin Encoder",
    "text": "Overview\nHDR has become one of video consumption's most important innovations in recent years. Our\nPractical Guide to HDR by Bitmovin\ndescribes the different formats and technology behind them. It describes the different transfer functions and how the signals are presented in the colour spaces. This tutorial gives an overview of the HDR formats that are supported with the Bitmovin Encoder and the possible conversions in between.\nWhat HDR Formats are supported with the Bitmovin Encoder?\nHLG\nHybrid Log-Gamma (HLG) was jointly developed by the BBC and NHK as a broadcast television HDR format that has a wider dynamic range while remaining backwards-compatible with existing SDR transmission standards. As such, it can be displayed correctly on both SDR and HDR monitors.\nHDR10\nHDR10 is an open standard and the most widely used and supported HDR format. It has static metadata that is used to map the content to maximise the display’s brightness and contrast capabilities, applied uniformly to the entire file.\nDolby Vision\nDeveloped by Dolby Laboratories, Dolby Vision is a dynamic HDR format with accompanying metadata that is used to optimise contrast and light levels for every scene or even frame of a video in order to take full advantage of the display’s capabilities and most accurately reproduce the content creator’s intent.\nHDR to SDR Conversion\nThe conversion to SDR is supported for any of the supported HDR formats. This is possible across multiple video codecs, such as H.264, H.265 or VP9. The output format can be specified in the video configuration itself by setting the\ndynamicRangeFormat\nparameter to\nSDR\n.\nPHP\nvideo_configuration = bitmovin_api.encoding.configurations.video.h265.create(\n  h265_video_configuration=H265VideoConfiguration(\n    dynamic_range_format=H265DynamicRangeFormat.SDR\n  )\n)\nConversion between different HDR Formats\nWhile any HDR format can be converted to SDR, some limitations must be considered when we want to convert between different HDR formats. It is also possible to convert from SDR to HDR, which does not process a higher dynamic range but adds the required metadata to be considered HDR content by the player.\nThe conversion is done in the same way as HDR to SDR conversion, by setting the dynamicRangeFormat to\nDOLBY_VISION\n,\nHDR10\n,\nHLG\nor\nSDR\n.\nPHP\nh265_video_configuration=H265VideoConfiguration(\n  dynamic_range_format=H265DynamicRangeFormat.DOLBY_VISION\n)\n\nvideo_configuration_hdr10 = bitmovin_api.encoding.configurations.video.h265.create(\n  h265_video_configuration=H265VideoConfiguration(\n    dynamic_range_format=H265DynamicRangeFormat.HDR10\n  )\n)\n\nvideo_configuration_hlg = bitmovin_api.encoding.configurations.video.h265.create(\n  h265_video_configuration=H265VideoConfiguration(\n    dynamic_range_format=H265DynamicRangeFormat.HLG\n  )\n)\n\nvideo_configuration_sdr = bitmovin_api.encoding.configurations.video.h265.create(\n  h265_video_configuration=H265VideoConfiguration(\n    dynamic_range_format=H265DynamicRangeFormat.SDR\n  )\n)\nMore Information\nA full example of how to set up HDR conversion can be found in our\npublic examples.\nBy setting the\ndynamicRangeFormat\n, specific preset parameters are applied to the video configuration. Using the predefined parameters in the\npresetConfiguration\nensures valid configuration and should be considered while setting specific parameters.\nSupported codecs for HDR content:\nH.265\n: HLG, HDR10, Dolby Vision\nVP9\n: HLG\nMatrix for possible conversions",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/68ac954-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/supported-input-and-output-storages",
    "title": "Supported Input and Output Storages",
    "text": "Overview\nBitmovin supports a wide variety of input and output storage types, which are listed on this page. If you don’t see your storage solution here, feel free to\ncontact our support\nfor direct help.\nInputs & Outputs\nName\nInput\nOutput\nVOD\nLive\nAWS S3\n✅\n✅\n✅\n✅\nGoogle Cloud Storage\n✅\n✅\n✅\n✅\nAzure Blob\n✅\n✅\n✅\n✅\nFTP\n✅\n✅\n✅\nSFTP\n✅\n✅\n✅\nHTTP(S)\n✅\n✅\nAkamai NetStorage\n✅\n✅\n✅\n✅\nAkamai Media Services Live 4.x\n✅\n✅\nS3 compatible (see below)\n✅\n✅\n✅\n✅\nAspera\n✅\n✅\nS3 Compatible Storages\nIf your storage solution is reachable from our encoder (on the public internet or within your Cloud Connect account), and provides an S3-like API, you may find that it is also supported with our\nS3 Generic Input\nand\nOutput\nfeatures. For example,\nAkamai Object Storage (Previously Linode)\nand\nOracle Object Storage\nare already supported using Generic S3 APIs.\nTesting is likely to be required to validate each individual storage product.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/integrating-bitmovin-encoder-with-pallycon-multi-drm",
    "title": "Integrating Bitmovin Encoder with PallyCon Multi-DRM",
    "text": "Overview\nThis tutorial walks through integrating Bitmovin CENC Encoding with PallyCon CPIX DRM in TWO simple steps.\nFor a full working sample refer to the\nAppendix\n3 Minute Video Guide of Bitmovin CENC + Pallycon CPIX Integration\nundefined\nPre-requisites\nCONTENT-ID\n; your script can generate this id to uniquely identify each content\nKMS version\n; this tutorial uses PallyCon KMS version 2 as it supports multi-key encryption\nPallyCon KMS URL\n; most of the time it looks like this\nhttps://kms.pallycon.com/v2/cpix/pallycon/getKey/\nKMS token\n; to uniquely identify and protect your request to the PallyCon KMS\nScreenshot from the\nPALLYCON\nportal\nApplicable Use cases\nSingle Key for all renditions\nMulti-key i.e different keys for different renditions\nIn this tutorial we'll run through use case #2 i.e Multi-Key\n1. Get DRM information from PallyCon\nOverview\nWe'll be discussing multi-key integration i.e 1 key for SD and 1 key for AUDIO. Implementation of the below logic can be found in the method\n_get_pallycon_drm_config @ cenc_drm_content_protection.py\nSteps\nGet the values from the pre-requisites section and update them respectively\nCONTENT_ID = \"my-unique_content_id\"\nPALLYCON_KMS_URL = \"https://kms.pallycon.com/v2/cpix/pallycon/getKey/\"\nPALLYCON_ENC_TOKEN = 'PALLYCON_KMS_TOKEN'\n# Request headers\nheaders = {'content-type': 'application/xml'}\nRequest DRM keys from PallyCon. in this case it's been configured for the Fairplay, Widevine and Playready for 1 SD key and 1 AUDIO key.\na. `<cpix:ContentKey>` - 1 for each key. \n    a.1 In our use case there'd be two such elements\n  b. `cpix:DRMSystem` - the different DRM systems i.e Widevine/Playready/Fairplay we are requesting for each key in #1. \n    b.1 In our usecase there'd be 6 elements (3 for the 1st key and 3 for the 2nd.)\n  c. `cpix:ContentKeyUsageRule` - the intended track type (SD/HD/AUDIO) for each key\n    c.1 In our usecase there'd be 2 such elements are we are requesting 1 key for `SD` and another for `AUDIO`\nParse the PallyCon DRM response into the Pallycon_Drm_Config helper class\n2. Apply PallyCon DRM using Bitmovin CENC API\nAfter getting the PallyCon drm configs we first split the DRM config by tracks i.e SD/AUDIO\ndef get_drm_config_by_track(pallycon_drm_configs: typing.Dict, track: str) -> typing.Dict:\n        return dict(filter(lambda elem: (elem[1]).get_track() == track,pallycon_drm_configs.items()))\n    pallycon_drm_configs = _get_pallycon_drm_config()\n    audio_drm_configs = get_drm_config_by_track(pallycon_drm_configs, \"AUDIO\")\n    sd_drm_configs = get_drm_config_by_track(pallycon_drm_configs, \"SD\")\nLastly we apply the\nPallycon_Drm_Config\nand create the DRM Muxing by using the method\n_create_drm_config @ cenc_drm_content_protection.py\nIt's THAT EASY!\nFAQ\nQuestion: Exception in _get_pallycon_drm_config @ cenc_drm_content_protection.py\nAnswer: This could be due to an\nexpired PallyCon DRM token OR\ninvalid request body\nKindly check with your PallyCon contact for more details.\nAppendix\nWORKING Source code\nClone the\nBitmovin GitHub\nrepository\nuse this updated\ncenc_drm_content_protection_v1.2\nfile w/ the PallyCon integation in place of the GitHub sample\nAdditional reads\nthe underpinnings of the SPEKE integration OR\nthe CENC integration option\nplease visit the reference By\nPallyCon\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/bd4e9d4-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-live-standby-pools",
    "title": "Creating Live Standby Pools",
    "text": "Live Standby Pools are created through either the API or Dashboard. For first time configuration we would recommend using the Dashboard, but this guide will provide information on both methods.\nPre-Requisites\nIf the feature has been enabled you'll see a new option in Live Encoding -> Configurations -> Live Pools\nIf the Live Pools option can not be seen then the feature needs to be enabled, just ask your account team for this along with the required adjustments to the Account Limits.\nFor now the page will look like this. Before creating a new pool, you will need to create a test an Encoding Template.\nCreating and Testing a valid Encoding Template\nMake sure you have first followed the guide on\ncreating and testing a valid Encoding Template\n, the last thing you want to happen is to create a Live Standby Pool with multiple Live Encoders all going into error.\nCreate a new Live Standby Pool\nUsing the Dashboard press Create new pool and you'll see the following form.\nGive the pool a name, and set the pool size.\n👍\nTip\nStart with a small pool size, these will be created as soon as the Live Pool is configured. We suggest starting with 1 or even 0. This number can be adjusted at any time.\nThen select an Encoding Template, this should be one that was previously tested and validated to work correctly.\nYou will be encouraged to review the Template and approve before moving to the next step.\nThis operation can be done in the API using this endpoint -\nCreate new standby pool\nIf you are successful then you'll be returned to the Live Pool screen and you're pool will appear in the object list in a healthy state.\nIf configured with a pool size of 1 or more a Live Encoding will immediately start and providing there are no errors enter a running state when the infrastructure is available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/4afc4d063c83b11a9486b34a3aad12ba79d429523cbd8584029d7e49f7851889-Screenshot_2024-11-29_at_15.31.30.png",
      "https://files.readme.io/0d055f904ed5727e8d2380e5a9ec250e6b656142b50fb47f5fb76c602c6a4695-Screenshot_2024-11-29_at_15.35.12.png",
      "https://files.readme.io/7f21f46e2019c3d7c5a7468660e718007c376ed653e7a183c404e0130278bc50-Screenshot_2024-11-29_at_16.48.25.png",
      "https://files.readme.io/d2ff2122752e57cee7838233ea336e2860668dcf382886f7cc701bdc9f0e7f3a-Screenshot_2024-11-29_at_16.53.49.png",
      "https://files.readme.io/6f637af658fa3692497dc7ddca4d9260e9ec3e0e364153d7e5f3900dcdd60f0c-Screenshot_2024-11-29_at_16.55.20.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rtmp-api-configuration",
    "title": "RTMP API Configuration",
    "text": "Introduction\nThe Bitmovin cloud encoding service is a powerful tool for live streaming, and our API makes it easy to implement. This tutorial concentrates on feeds contributed with the RTMP protocol, which are the simplest to setup. There are basically 4 steps involved when it comes to our live streaming service in the cloud.\n1. Ingest RTMP Stream to our Live Encoder\nUsually a mezzanine or \"contribution\" encoder that is processing the live signal will transcode this signal to a high quality mezzanine format and ingest it at the RTMP ingest point in our live encoder. You can now use such an encoder from Elemental, Teradek, Teracue, or any other vendor, or use software like the popular OBS studio or ffmpeg.\n2. Encoding of the Input Stream to MPEG-DASH and HLS\nYou can define multiple output resolutions and bitrates for MPEG-DASH and HLS, define if you want to encode to H.264 (AVC) or H.265 (HEVC). There are literally no limits in defining what output you want from our live encoder, e.g. it can easily handle multiple 4k 60FPS streams encoded to HEVC.\n3. Direct Output to Your Storage\nOur live encoder writes the encoded files directly to your AWS S3 or GCS bucket. You could use this output storage as origin for your CDN or for multiple CDNs. This works well with GCS as\nGoogle offers a CDN Interconnect\nfor several CDNs such as Akamai, Fastly, Level3, HighWinds, CloudFlare, Limelight, and Verizon. AWS S3 could also easily be connected with their\nCloudfront CDN\nand provides you an out-of-the-box solution in one cloud.\n4. Playback on any Device, or Browser\nIf you are using the Bitmovin HTML5 player your content will playback on any device or browser. However, you are not limited to use our player and can also choose among others as described in our recent blog post on\nencoding for multiple HTML5 Players\n. You can also use the other Bitmovin\nPlayer SDKs\nif you want to build native apps on iOS, Android, Roku and other devices.\nPreparation\nTo run the example in this tutorial, you need a\nBitmovin API Key\nand credentials for an\nAmazon AWS S3 bucket\n.\nStarting With The API\nNow, let’s see how we can start a live stream that will generate MPEG-DASH and HLS streams from an RTMP input.\nThe following example shows the setup of an RTMP live stream, and will be utilizing our\nOpen API SDK for Java\n. The\nfull example\ncan be found in our\nOpen API Example Repository\non Github.\nSetting up Your Properties file\nFirst we need to setup the Bitmovin API client with your API key and S3 bucket credentials. For this, create a directory named\n.bitmovin\nin your user’s home directory. Then, create a text file named\nexamples.properties\nin the\n.bitmovin\ndirectory, with the following content:\nBITMOVIN_API_KEY=<YOUR_API_KEY>\nS3_OUTPUT_BUCKET_NAME=<YOUR_S3_BUCKET_NAME>\nS3_OUTPUT_ACCESS_KEY=<YOUR_S3_ACCESS_KEY>\nS3_OUTPUT_SECRET_KEY=<YOUR_S3_SECRET_KEY>\nS3_OUTPUT_BASE_PATH=/output/encoding_test/doc/srt\nUsing ConfigProvider to Initialize the API\nThe OpenAPI client includes a\nconfigProvider\nobject which has access to your\nexamples.properties\nfile and thus, to your Bitmovin and S3 credentials. Before we can do anything, we must set up the\nconfigProvider\n.\nNext, we set up\nbitmovinApi\n, the basic object accessing the Bitmovin API. It gets the API key from\nconfigProvider\n.\nJava\nconfigProvider = new ConfigProvider(args);\nbitmovinApi =\n    BitmovinApi.builder()\n        .withApiKey(configProvider.getBitmovinApiKey())\n        .withLogger(\n             new Slf4jLogger(), Level.BASIC) \n        .build();\nYou will find this code in the\nmain()\nmethod of the\nRtmpLiveEncoding\nclass. This is the method where the main workflow resides. It is also the method you later will run in order to start the encoding.\nCreate an Encoding\nCreate an\nencoding\nwith the simple line\nJava\nEncoding encoding =\n        createEncoding(\"Live Encoding Test\", \"Live encoding with HLS and DASH manifest\");\nThis creates an encoding resource where we will add all other configuration resources to it.\nPlease note:\nFor production, to guarantee a stable connection between the contribution encoder and our live encoder, it is crucial to carefully select the cloud and region in which your live encoder will run to reduce latencies.\nOur encoder uses Western Europe as the default region. You can set cloud regions by adding attributes to the encoding object in the\ncreateEncoding()\nmethod.\nSelect the RTMP Live Input\nBitmovin's live encoding service supports\nRTMP Push\nas ingest stream, it expects the input stream to be pushed to the live stream. Therefore, your Bitmovin Account is already pre-configured with an\nRTMP Input\nwhich you can use to configure your live stream.\nJava\nRtmpInput input = getRtmpInput();\nIf you are interested in the details, please read on. Otherwise you may skip the rest of the section and continue with\nCreate an Output\n.\nDetails\nThe code above calls a method named\ngetRtmpInput()\nwhich looks like this:\nJava\nprivate static RtmpInput getRtmpInput() throws BitmovinException {\n    return bitmovinApi.encoding.inputs.rtmp.list().getItems().get(0);\n}\nThe method returns the pre-configured RTMP Input resource of your account.\nPlease note:\nThe IP address of the Bitmovin live encoder (where it listens for an incoming RTMP ingest) will only be determined after you start the encoding, as you will later see. The Bitmovin live encoder's address is chosen by Bitmovin and will typically be different for every new encoding, even if you stop and start the encoding to change parameters. You cannot configure the Bitmovin encoder to assume a certain IP address.\nCreate an Output\nIn this tutorial we are using an AWS S3 bucket as output location of our live encoder. However, it's a simple change to use an Google Cloud Storage bucket as output instead if you prefer.\nThe\noutput\nobject gets the credentials for the bucket from the\nconfigProvider\n.\nJava\nOutput output = createS3Output(\n                    configProvider.getS3OutputBucketName(),\n                    configProvider.getS3OutputAccessKey(),\n                    configProvider.getS3OutputSecretKey());\nAdd Video and Audio Codec Configurations\nIn this section, you will determine the formats into which your live stream will be encoded.\nThe encoding, or transcoding, will happen in the following steps:\nSelect the video and audio tracks of the input stream\nConfigure the video resp. audio renditions into which the input tracks will be encoded\nSelecting the video and audio tracks from the input stream\nAn input stream can have multiple video and multiple audio tracks, e.g for different angles and languages. For this tutorial, we assume that the input stream (coming from the contribution encoder) only contains one video and one audio track.\nFor this simple but widely used track layout it is sufficient to tell Bitmovin encoder to select the first available track as source for the video renditions, and the second available track as source for the audio renditions. The first available track has the number\n0\nand the second has the number\n1\n. We will meet these numbers later in the process.\nConfigure Video and Audio Renditions\nA rendition is a way to encode content. For adaptive streaming we typically use multiple video renditions - high quality for fast networks and low bandwidth for slow networks. We only use one audio rendition because audio bandwidth is low compared to video and can be used for both fast and slow network conditions.\nA codec configuration contains the configuration for a video rendition or an audio rendition. We will link the video input track to three codec configurations for three different video renditions. We will link the audio input track to one codec configuration.\nThe following code defines a set of three H264 video profiles. The parameters are the\nname\n, the\nbandwidth\n(in bps), the\nheight\n, and the\noutput path\n. The last parameter is always\n0\nto select the first track of the input stream, which is the video track.\nJava\nprivate static List<VideoConfig> videoProfile =\n  Arrays.asList(\n    new VideoConfig(\"480p\", 800_000L, 480, \"/video/480p\", 0),\n    new VideoConfig(\"720p\", 1_200_000L, 720, \"/video/720p\", 0),\n    new VideoConfig(\"1080p\", 3_000_000L, 1080, \"/video/1080p\", 0));\nThe code below transforms the video profiles into codec configurations of type\nH264VideoConfiguration\n. This will encode the input stream's video part using the H.264 codec. Of course it is possible to use other codecs as well, such as VP9 or H.265/HEVC.\nJava\nfor (VideoConfig videoConfig : videoProfile) {\n      H264VideoConfiguration h264Configuration =\n          createH264VideoConfig(videoConfig.height, videoConfig.bitRate);\n      Stream stream =\n          createStream(encoding, input, h264Configuration, videoConfig.inputStreamPosition);\n      createFmp4Muxing(encoding, stream, output, videoConfig.outputPath);\n    }\nHere the lines mean the following:\nJava\nStream stream =\n           createStream(encoding, input, h264Configuration, videoConfig.inputStreamPosition);\nThis line creates a\nh264Configuration\nobject which uses the\nheight\nand the\nbitrate\nof the video profile. Because the aspect ratio of the source is kept, with\nheight\nyou can control the resolution of the rendition and with\nbitRate\nthe bandwith and hence, quality.\nJava\nStream stream =\n           createStream(encoding, input, h264Configuration, videoConfig.inputStreamPosition);\nThis line links the\nh264Configuration\nobject to the\ninput\nstream's track which is depicted by the video profile's\ninputStreamPosition\n(Remember that this is\n0\nfor the video track). The line basically defines a task to \"encode the input stream's video track with the given H.264 configuration\". Also, this task is linked to the\nencoding\nobject we created earlier, making it part of the encoding. The whole encoded video rendition is referenced as a\nstream\n.\nJava\ncreateFmp4Muxing(encoding, stream, output, videoConfig.outputPath);\nThis line is explained in the next section of this tutorial.\nFor audio it works pretty much the same way as for video as you can see in this code sample:\nFirst, the predefined audio profile:\nJava\nprivate static List<AudioConfig> audioProfile =\n     Collections.singletonList(new AudioConfig(\"128kbit\", 128_000L, \"/audio/128kb\", 1));\nThen, the code that creates an AAC audio configuration out of the profile and links it to the audio track of the input stream\nJava\nfor (AudioConfig audioConfig : audioProfile) {\n      AacAudioConfiguration aacConfig = createAacAudioConfig(audioConfig.bitrate);\n      Stream audioStream =\n          createStream(encoding, input, aacConfig, audioConfig.inputStreamPosition);\n      createFmp4Muxing(encoding, audioStream, output, audioConfig.outputPath);\n    }\nAgain, the line\ncreateFmp4Muxing(..)\nis explained in the next section.\nCreate Muxings for MPEG-DASH and HLS\nIn the previous section we have explained how the input stream is encoded, but in order to write down the encoded data it must be packaged into a container format.\nAs the content will be used for MPEG-DASH and HLS streaming, we choose a container format that supports both, named\nFragmented MP4\nor\nfMP4\n. This format cuts the stream in multiple small, numbered files called\nSegments\n.\nThe following line of code, which was already seen in the previous section, defines an\nfMP4\ncontainer format. Because writing an encoded stream into a container is also known as \"multiplexing\", or short\nmuxing\n, container formats are also known as\nmuxings\n.\ncreateFmp4Muxing(encoding, stream, output, videoConfig.outputPath);\nThis line takes an encoded video rendition (\nstream\n) from an\nencoding\n, and packages it into the\nfMP4\nformat. Then it writes the\nfMP4\nsegments to the\noutput\n, which is the S3 bucket we configured earlier. The segments are written into the\noutputPath\ndefined in the video profile.\nCreate Muxings for MPEG-DASH and HLS\nIn order to create MPEG-DASH and HLS content the encoded data needs to be packaged accordingly. In the following lines of code we will define segmented fMP4 for MPEG-DASH and segmented TS for HLS. Here you also define how long the video segments should be. This can be an important parameter when creating video clips out of your live stream. The smaller this value, the more accurate you can “cut” the video clip, and reduce latency. However too short a value and you will reduce the encoding performance and quality of the output for a given bitrate. By default, our encoder will choose 4 sec as segment length, which is what we would recommend you start with for best balance between latency and encoding efficiency. We also define where the segments should be stored in your output bucket. You have full control over the output location of the video and the audio streams.\nNote that it is also possible to create VoD manifests out of the encoded data createdfrom the live stream in this way. This is explained in our blog post:\nImplement Live-to-VoD with the Bitmovin API\n.\nFirst we will create the required fMP4 muxings for MPEG-DASH:\nJava\nar videoFMP4Muxing1080p = bitmovin.Encoding.Encoding.Fmp4.Create(encoding.Id,\n    CreateFMP4Muxing(videoStream1080p, output, OUTPUT_PATH + \"video/1080p_dash\", segmentLength));\nvar audioFMP4Muxing = bitmovin.Encoding.Encoding.Fmp4.Create(encoding.Id,\n    CreateFMP4Muxing(audioStream, output, OUTPUT_PATH + \"audio/128kbps_dash\", segmentLength));\nThe following shows the same for segmented TS muxings:\nJava\nvar videoTsMuxing1080p = bitmovin.Encoding.Encoding.Ts.Create(encoding.Id,\n    CreateTsMuxing(videoStream1080p, output, OUTPUT_PATH + \"video/1080p_hls\", segmentLength));\nvar audioTsMuxing = bitmovin.Encoding.Encoding.Ts.Create(encoding.Id,\n    CreateTsMuxing(audioStream, output, OUTPUT_PATH + \"audio/128kbps_hls\", segmentLength));\nIf you added multiple video renditions in the previous steps, you also need to create fMP4 and TS muxings for each rendition.\nDefine the MPEG-DASH and HLS Live Manifests\nSo far we have defined how to encode the input streams and package the encoded streams into\nfMP4\nsegements which will reside on your output bucket.\nIn order to playback content using MPEG-DASH or HLS, we also need to have\nmanifests\n. A manifest is a text or XML file which describes the adaptive output stream so a player always knows which segments to retrieve and playback next. Manifests tell the player which renditions are available and where to get them. Typically a stream has a set of interlinked manifests. HLS and MPEG-DASH manifests differ in format. But both formats eventually retrieve and playback the\nfMP4\nsegments created by the Bitmovin live encoder.\nWith the Bitmovin API you have full control over creating manifests, e.g. create multiple manifests with different renditions.\nWhen creating the Live MPEG-DASH or Live HLS manifest (or manifests), you also specify where it is output, with path location and filename for the manifest.\nJava\nDashManifest dashManifest = createDashManifest(output, \"/\", encoding);\nHlsManifest hlsManifest = createHlsManifest(output, \"/\", encoding);\n\nLiveDashManifest liveDashManifest = new LiveDashManifest();\nliveDashManifest.setManifestId(dashManifest.getId());\nliveDashManifest.setTimeshift(300D);\nliveDashManifest.setLiveEdgeOffset(90D);\n\nLiveHlsManifest liveHlsManifest = new LiveHlsManifest();\nliveHlsManifest.setManifestId(hlsManifest.getId());\nliveHlsManifest.setTimeshift(300D);\nYou also have the option to set specific live options, such as the timeshift parameter that defines how many seconds a user will be able to seek back in time. In the example below we allow the users to seek back 5 minutes. For MPEG-DASH you can also define how far away from the real live signal the player will start to playback the segments. If you are not aiming for low latency live streams, choose a value between 60 and 120 seconds to give the player enough room for buffering.\nStart the RTMP Live Encoding\nNow that we have defined every aspect of the live encoding, we can finally start the live stream using both created manifests in the start call.\nFor this, we define a new\nStartLiveEncodingRequest\nobject named\nstartRequest\nand add both manifests to it:\nJava\nStartLiveEncodingRequest startRequest = new StartLiveEncodingRequest();\nstartRequest.addDashManifestsItem(liveDashManifest);\nstartRequest.addHlsManifestsItem(liveHlsManifest);\nAutomatic shutdown\nOptionally an automatic shutdown can be defined for the live encoding. The automatic shutdown can be defined via the\nLiveAutoShutdownConfiguration\n. There are two possible settings. The\nbytesReadTimeoutSeconds\ndefines how long the live stream will run after an input loss and the\nstreamTimeoutMinutes\ndefines the time after which the live stream will be automatically shut down.\nJava\nLiveAutoShutdownConfiguration shutdownConfig = new LiveAutoShutdownConfiguration();\nshutdownConfig.setStreamTimeoutMinutes(240L);\nshutdownConfig.setBytesReadTimeoutSeconds(600L);\nstartRequest.setAutoShutdownConfiguration(shutdownConfig);\nStarting the Example\nThe\nstartLiveEncodingAndWaitUntilRunning()\nmethod with both the configured\nencoding\nand the\nstartRequest\nwill start the encoding.\nJava\nstartLiveEncodingAndWaitUntilRunning(encoding, startRequest);\nTo start the example and actually run a live encoding with RTMP input, run the\nmain()\nmethod of the\nRtmpLiveEncoding\nclass in the\nRtmpLiveEncoding.java\nfile.\nRetrieve RTMP Ingest Point Information\nAfter starting the live encoding, you need to wait for it to be ready before you can ingest your RTMP stream. This may take a few minutes.\nOnce it is ready you can easily query the live stream details and - for example - print it to the console:\nJava\nLiveEncoding liveEncoding = waitForLiveEncodingDetails(encoding);\n    logger.info(\n        \"Live encoding is up and ready for ingest. RTMP URL: rtmp://{}/live StreamKey: {}\",\n        liveEncoding.getEncoderIp(),\n        liveEncoding.getStreamKey());\nThe console output will contain the RTMP Push URL and the stream key. You need to pass this information to your contribution encoder to configure the RTMP URL it should publish to.\nAs stated before, you should choose your encoding region wisely to have a stable RTMP ingest from your location. However, should the RTMP signal be interrupted to our ingest point, we will continue to stream the last image we got from your encoder as still image in order to avoid interrupting playback of the stream. As soon as your encoder is able to connect to our ingest point again, we will resume streaming your content.\nShutdown the Live Encoding\nBecause of this mechanism to provide a continuous output, the live encoding won't stop automatically when the input feed stops. You therefore have to shut the encoding down at the end of your event, to avoid generating unnecessary costs in your account. The following code shows how to stop a live encoding:\nJava\nlogger.info(\"Shutting down live encoding!\");\nbitmovinApi.encoding.encodings.live.stop(encoding.getId());\nwaitUntilEncodingIsInState(encoding, Status.FINISHED);\nYou can also use the Bitmovin dashboard to stop the encoding. The controls are shown in the Live Encoding details.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/9ca54db-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-nagra-nexguard-filemarker-a-b-watermarking",
    "title": "Using Nagra NexGuard FileMarker A/B Watermarking",
    "text": "Overview\nNexGuard FileMarker is based on the concept of A/B watermarking. If you enable it for an input stream the Bitmovin encoder will automatically generate two output streams. The A stream and the B stream.\nThe A stream will follow your configured segment naming template (e.g. segment_0.ts) while the B stream is prefixed with b. (e.g.: b.segment_0.ts). When one of your customers starts watching a protected video, your CDN will serve them unique sequence of A and B segments. This unique sequence enables you to trace any leaked video to the time and customer you originally provided the video for.\nRequirements\nEncoder version 2.62 or higher\nA Nagra Account to obtain license and configuration values\nKnown Limitations\nThe NexGuard FileMarker Specification exposes multiple restrictions to the encoding to ensure that a video is compatible with the different CDN providers. A non exhaustive list of restrictions is given here:\nSupported Muxings\nfMP4\n,\nTS\n,\nWebM\nSegment Naming conventinons\nSegment naming must be\n<filename>_<number>.<extension>\nInit naming must be\n<filename>_init.<extension>\nThe Nagra NexGuard FileMarker relies heavily on the naming scheme of your segments and of the init file to be able to do the a/b switching correctly. The filename can be chosen freely. For segments we would recommend the default e.g.\nsegment_<number>.m4s\nand for the init segment we would recommend a unique filename with which you can identify your video e.g.:\nbigBuckBunny_init.mp4\n.\nSupported Framerates\n23.976\n,\n24.0\n,\n25.0\n,\n29.97\n,\n30.0\n,\n48.0\n,\n50.0\n,\n59.94\n,\n60.0\nSupported Resolution\n320\n<=\nwidth\n<=\n5120\n240\n<=\nheight\n<=\n3200\nOthers\nGOP\nsize has to be\n2\nor\n2.002\nseconds\nMinimum length of the video asset - Required to ensure that a unique sequence of A and B segments can be generated.\nCreate an Encoding\nThis tutorial is based on our\nPer-Title Encoding Example on Github\nand will state only the adjustments it would need to create NexGuard Watermarking compatible content.\nAs with every encoding, we will need an\nEncoding\nObject, that holds all related configuration objects, as well as an\nInput\nand\nOutput\nObject.\nEncoding\nJava\nEncoding encoding = createEncoding(\"Per-Title encoding\", \"Per-Title encoding with NexGuard Watermarking\");\nCreate or reuse an Input\nHTTP is just used for simplicity in this example, as there is no limitation in where the input file can be downloaded from for that use-case. Of course, you can also reuse existing\nInput\nobjects available in your Bitmovin Account.\nJava\nHttpInput input = createHttpInput(configProvider.getHttpInputHost());\nCreate or reuse an Output\nAkamai NetStorage is used knowing that the Akamai CDN does support the Nagra FileMarker Specification. Of course, you can also reuse existing Input objects available in your Bitmovin Account.\nIMPORTANT\n: While you can write this content to any support\nOutput\n, make sure that your CDN supports the NexGuard FileMarker specification to make use of the A/B watermarking, otherwise you won't be able to use it.\nJava\nOutput output =\n  createAkamaimNetstorage(\n            configProvider.getAkamaiHost(),\n            configProvider.getAkamaiUsername(),\n            configProvider.getAkamaiPassowrd());\nCreate an H264 Video Configuration\nMake sure that you to configure a supported frame rate and gop size. As stated in \"Known Limitations\", NexGuard Watermarking requires a GOP size of 2 seconds. The Bitmovin API requires the GOP size in frames. To convert between the two calculate frame rate _required gop size in seconds. E.g. in this case 24x2 equals a max gop size of 48.\nJava\nprivate static H264VideoConfiguration createBaseH264VideoConfig() throws BitmovinException {\n    H264VideoConfiguration config = new H264VideoConfiguration();\n    config.setName(\"Base H.264 video config\");\n    config.setPresetConfiguration(PresetConfiguration.VOD_STANDARD);\n    config.setRate(24D);\n    config.setMinGop(48);\n    config.setMaxGop(48)\n    return bitmovinApi.encoding.configurations.video.h264.create(config);\n  }\nCreate Video Stream\nA\nStream\nObject effectively maps a stream of the input file to a given\nCodecConfiguration\n, in this case the one created in the step before.\nJava\nStream videoStream =\n        createStream(\n            encoding,\n            input,\n            configProvider.getHttpInputFilePath(),\n            createBaseH264VideoConfig(),\n            StreamMode.PER_TITLE_TEMPLATE);\nAdd NexGuard FileMarker to Video Stream\nTo enable the NexGuard Watermarking on the video streams, we will configure the\nNexGuardFileMarker\nobject and create it with a reference to the\nvideoStream\nfrom before.\nHINT:\nThe values for the fields\nlicense\nand\npreset\nare provided to you by your Nagra contact. Please see the\nAPI Reference\nfor all available properties.\nJava\nNexGuardFileMarker nexguardFileMarker = new NexGuardFileMarker();\nnexguardFileMarker.setWatermarkType(NexGuardWatermarkingType.OTT);\nnexguardFileMarker.setLicense(license);\nnexguardFileMarker.setPreset(preset);\n//nexguardFileMarker.setStrength(<string>); //use it only when instructed to by Nagra\n\nbitmovinApi\n    .encodings\n    .streams\n    .watermarking\n    .nexguardFileMarker\n    .create(encoding.getId(), videoStream.getId(), nexguardFileMarker);\nCreate a Video Muxing\nNext we configure a muxing for the stream. Note that the init segment needs to be follow the\n<filename>_init.mp4\nnaming template stated in\nKnown Limitations\n.\nJava\ncreateFmp4Muxing(encoding, output, \"video/{height}/{bitrate}_{uuid}\", videoStream);\n private static Fmp4Muxing createFmp4Muxing(\n      Encoding encoding, Output output, String outputPath, Stream stream) throws BitmovinException {\n    MuxingStream muxingStream = new MuxingStream();\n    muxingStream.setStreamId(stream.getId());\n    Fmp4Muxing muxing = new Fmp4Muxing();\n    muxing.addOutputsItem(buildEncodingOutput(output, outputPath));\n    muxing.addStreamsItem(muxingStream);\n    muxing.setSegmentLength(2.0);\n    muxing.setInitSegmentName(\"pertitleNexguard_init.mp4\")\n    return bitmovinApi.encoding.encodings.muxings.fmp4.create(encoding.getId(), muxing);\n  }\nCreate a Audio Codec Configuration\nJava\nAacAudioConfiguration aacConfig = createAacAudioConfig();\nCreate a Audio Stream\nJava\nStream audioStream =\n        createStream(\n            encoding, input, configProvider.getHttpInputFilePath(), aacConfig, StreamMode.STANDARD);\nAdd Nexguard Filemarker to Audio Stream\nTheoretically it is possible to mix non watermarked and watermarked streams in a single encoding, therefore we will need to tell the Encoder that this audioStream is part of the watermarking process and needs to be\nduplicated\n.\nThis duplication is necessary because NexGuard enabled CDNs do the A/B switching only based on the file extension and both audio and video segments usually share the same extension.\nJava\nNexGuardFileMarker nexguardFileMarker = new NexGuardFileMarker();\nnexguardFileMarker.setWatermarkType(NexGuardWatermarkingType.DUPLICATED);\nnexguardFileMarker.setLicense(license);\nnexguardFileMarker.setPreset(preset);\n\nbitmovinApi\n    .encodings\n    .streams\n    .watermarking\n    .nexguardFileMarker\n    .create(encoding.getId(), audioStream.getId(), nexguardFileMarker);\nCreate Audio Muxing\nJava\ncreateFmp4Muxing(encoding, output, \"audio\", audioStream);\nConfigure and Start Encoding\nJava\nStartEncodingRequest startEncodingRequest = new StartEncodingRequest();\nstartEncodingRequest.setPerTitle(buildPerTitleStartRequest());\nJava\nexecuteEncoding(encoding, startEncodingRequest);",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/dynamic-range-format-presets",
    "title": "Dynamic Range Format Presets",
    "text": "Introduction\nWhen transcoding your HDR content, or converting content between different HDR formats or to SDR, you need to get the codec configuration settings that conrol the output color settings just right to ensure that the operation is performed correctly and that you have a valid output file.\nTo simplify this process, with encoder v2.98.0 and above, we expose a\ndynamicRangeFormat\nenum property with a range of possible values for common dynamic range formats.\nJava\nH265VideoConfiguration h265VideoConfiguration = new H265VideoConfiguration();\nh265VideoConfiguration.setHeight(OUTPUT_HEIGHT);\nh265VideoConfiguration.setBitrate(OUTPUT_BITRATE);\nh265VideoConfiguration.setDynamicRangeFormat(H265DynamicRangeFormat.HDR10);\nThe value selected sets a number of the relevant individual properties on the codec configuration. The tables below show those properties and assigned values, for each supported codec configuration.\nYou may still overwrite any of the individual settings if necessary.\nH265 Video Configuration\nPreset\nSDR\nHLG\nHDR10\nDOLBY_VISION\nprofile^\nmain\nmain10\nmain10\nmain10\npixelFormat\nYUV420P\nYUV420P10LE\nYUV420P10LE\nYUV420P10LE\nhdr\n-\ntrue\ntrue\n-\nenableHrdSignaling\n-\n-\n-\ntrue\ncolorConfig.colorPrimaries\nBT709\nBT2020\nBT2020\nUNSPECIFIED\ncolorConfig.colorTransfer\nBT709\nARIB_STD_B67\nSMPTE2084\nUNSPECIFIED\ncolorConfig.colorSpace\nBT709\nBT2020_NCL\nBT2020_NCL\nUNSPECIFIED\ncolorConfig.colorRange\n-\n-\n-\nJPEG\ncolorConfig.copyColorTransferFlag\nfalse\nfalse\nfalse\nfalse\ncolorConfig.copyColorPrimariesFlag\nfalse\nfalse\nfalse\nfalse\ncolorConfig.copyChromaLocationFlag\nfalse\nfalse\nfalse\nfalse\ncolorConfig.copyColorSpaceFlag\nfalse\nfalse\nfalse\nfalse\ncolorConfig.copyColorRangeFlag\nfalse\nfalse\nfalse\nfalse\nmasterDisplay*\n-\n-\nG(8500,39850) B(6550,2300) R(35400,14600) WP(15635,16450) L(10000000,1)\nG(13250,34500) B(7500,3000) R(34000,16000) WP(15635,16450) L(10000000,1)\n*\nwhitespaces and carriage return in the table are for readibility. The actual string used in the APIs does not contain any whitespace\n^ the profile set by the\nPresetConfiguration\nwill be overwritten as needed\nH264 Video Configuration\nPreset\nSDR\nprofile\nmain\ncolorConfig.colorPrimaries\nBT709\ncolorConfig.colorTransfer\nBT709\ncolorConfig.colorSpace\nBT709\ncolorConfig.copyColorTransferFlag\nfalse\ncolorConfig.copyColorPrimariesFlag\nfalse\ncolorConfig.copyChromaLocationFlag\nfalse\ncolorConfig.copyColorSpaceFlag\nfalse\ncolorConfig.copyColorRangeFlag\nfalse\nVP9 Video Configuration\nPreset\nSDR\nHLG\ncolorConfig.colorPrimaries\nBT709\nBT2020\ncolorConfig.colorTransfer\nBT709\nARIB_STD_B67\ncolorConfig.colorSpace\nBT709\nBT2020_NCL\ncolorConfig.copyColorTransferFlag\nfalse\nfalse\ncolorConfig.copyColorPrimariesFlag\nfalse\nfalse\ncolorConfig.copyChromaLocationFlag\nfalse\nfalse\ncolorConfig.copyColorSpaceFlag\nfalse\nfalse\ncolorConfig.copyColorRangeFlag\nfalse\nfalse\nInput color\nIn the background, the encoder decides which conversion to apply based only on the input type and the output configuration. If there is no\ndynamicRangeFormat\nspecifically set in the video configuration, the encoder will keep the output color space the same as found in the input.\nThe input color configuration can be specified in the API, and if not specified will be deduced during the internal decoding process. If the input color is not specified and the encoder fails to properly deduce the input color configuration, it will be assumed that the colorspace is:\nBT470BG if the input resolution is SD\nBT709 for input resolution of HD (720p) or above\nBT2020 for input resolutions of 4K or above",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-multi-drm-protected-content-with-intertrust-expressplay",
    "title": "Creating Multi-DRM Protected Content with Intertrust / ExpressPlay",
    "text": "Overview\nThe following tutorial will show you how to create a video distribution platform, ready to target multiple devices and browsers with a DRM system compatible with Apple’s FairPlay, Google’s Widevine, Microsoft’s PlayReady and the open-standards-based Marlin DRM.\nThis solution is an end-to-end solution that integrates the ExpressPlay Multi-DRM system with the Bitmovin Cloud Encoding system. By following this tutorial and using the supporting documents that you will find linked below, you can create a video on demand service with the same speed and quality as Netflix.\nRequirements\nAn account at\nhttps://admin.expressplay.com\nIf you want to use FairPlay DRM, a valid Apple certificate is required and has to be requested at Apple (\nhttps://developer.apple.com/streaming/fps/\n, please \"Request a Deployment Package\")\nMulti-DRM System – High Level Architecture\nUse the Bitmovin Cloud Encoding Service to encode the content\nThe client (HTML5 app or native app) requests the content from the service\nThe service uses the ExpressPlay REST API to request a token for the DRM support by the client\nThe token is passed back to the device\nPlayback and / or download starts\nSetup Bitmovin Cloud Encoding with DRM\nThe following code example shows the configuration required to encrypt your adaptive stream to use Playready, Widevine and Fairplay. You are not required to include all the DRM schemas. For example, you can leave out Fairplay if it is not required.\nImportant\n: This example is using the LaUrl of the test environment offered by Intertrust/ExpressPlay. Please replace it with the production-use LaUrl, which is available in your Intertrust/ExpressPlay backend at\nhttps://admin.expressplay.com\nWidevine and PlayReady DRM\nPlease see\nthis tutorial\n, which explains how to create a combined Widevine and PlayReady DRM protected encoding in more detail. The following parameters are required in order to create a successful DRM protected encoding using the ExpressPlay Multi DRM service.\nJava\nString key = \"0123456789abcdef0123456789abcdef\";\nString kid = \"0123456789abcdef0123456789abcdef\";\nString widevinePssh = \"CAESEAABAgMEBQYHCAkKCwwNDg8aCmludGVydHJ1c3QiASo=\";\nString playreadyLaUrl = \"https://pr.test.expressplay.com/playready/RightsManager.asmx\";\nThe parameters of the configuration have the following meaning:\nkey\n: This is the common content encryption key in hex format of your choice, used for every DRM solution provided in the\nCencDRM\nconfiguration.\nkid\n: This is the common unique identifier for your content key in hex format of your choice, used for every DRM solution provided in the\nCencDRM\nconfiguration.\nWidevine\n-\npssh\n: This is the value for the Widevine pssh box, which is stays the same for the test and production environment offered by ExpressPlay\nPlayReady\n-\nlaUrl\n: This is the URL to the PlayReady license server, that will be added in the metadata of the manifest and could be used by player to request a license.\nEach tutorial provides a full example, which you can use in order to create your own DRM protected content using ExpressPlay MultiDRM Services.\nFairPlay DRM\nPlease see\nthis tutorial\n, which explains how to create a Fairplay DRM protected encoding. The following parameters are required in order to create a successful DRM protected encoding using the ExpressPlay Multi DRM service.\nJava\nString key = \"0123456789abcdef0123456789abcdef\";\nString iv = \"0123456789abcdef0123456789abcdef\";\nString uri = \"skd://expressplay_token\";\nThe parameters of the Fairplay DRM configuration have the following meaning:\nkey\n: You need to provide a key that will be used to encrypt the content (16 byte; 32 hexadecimal characters)\niv\n: The initialization vector is optional. If it is not provided we will generate one for you. (16 byte; 32 hexadecimal characters)\nuri\n: If provided, this URI will be used for license acquisition. Using Expressplay, you have to provide \"skd://expressplay_token\" as\nuri\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/a264d62-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/separating-and-combining-audio-streams",
    "title": "Separating and Combining Audio Streams",
    "text": "Introduction\nThis tutorial illustrates the different ways to handle multi-channel audio, specifically for when you have multiple source files, different audio layouts, and a need to transform your inputs into streamable audio streams.\nFirst, we go through terminology, and some simple use cases, to introduce the main concepts and API resources that are involved in the encoding configuration when it comes to audio.\nThen, we look at use cases that involve channel (re-)mixing from an input stream to an output stream.\nFollowing that, we look at how to (re-)map multiple streams between inputs and outputs.\nFinally, in the last section, we put it all together and look at how to merge or blend multiple multi-channel streams into a single output stream.\nTerminology\nBut first, it helps to understand what we are talking about and agree on the vocabulary.\nA\nchannel\nis the real audio signal, which is usually associated with a speaker in a multi-speaker setup.\nMultiple channels are often grouped together into\nstreams\n. For example a 5.1 surround sound audio stream contains 6 channels. We will often talk about channel layout to refer to the number and order of channels in the stream. Sometimes the term\ntrack\nis used as well: a track refers to the logical entity, the stream refers to the track encoded with a specific codec.\nA Stream is contained in a\nfile\n, and a file can contain multiple streams. A file can also contain video streams alongside audio streams.\nFor this tutorial, we will use illustrations such as the following to help depict the concepts:\nThis represents a single file, with 3 streams.\nThe first stream (stream 0) is a video stream.\nThe next stream (stream 1) is a stereo audio stream with 2 channels: left and right\nThe last stream (stream 2) is a surround audio stream with 6 channels: front left, front right, center, low frequency effects (for a subwoofer), back left and back right\nSimple Stream Handling\nUse Case 1 - Implicit Handling\nIn the simplest case, there is no real audio manipulation taking place. You have an input with an audio stream with a particular channel layout, and you want basically the same to be in your output, with the appropriate codec and bitrate. This use case is our baseline, which we’ll need in order to introduce additional concepts for the following, more complex use cases. Note that we do not currently support a passthrough options in most cases; an input audio stream still needs to be encoded in order to be properly aligned to the encoded video content, especially when creating segmented content.\nLet’s take the following example, with a single stereo stream:\nWe will not go into all aspects of the encoding configuration (there are other tutorials on our website that cover that), but will look specifically at the audio aspect of the configuration. For that, you will need a similar set of resources as for other streams:\nAn\nIngestInputStream\n, which defines where your source file is located on the\nInput\nstorage\nAn audio\nCodecConfiguration\nfor the codec of your choice, configured appropriately\nAn (output)\nStream\nthat defines how the configuration is applied to the input stream\nOne or more\nMuxings\nto define how the stream is containerised in a file and transferred to the\nOutput\nℹ️\nNote:\nFor the purpose of this tutorial and in the example code files associated with it, we will always generate a single MP4 file as output, with all audio streams multiplexed with the video stream. In most situations that involve adaptive bitrate streaming with manifests such as HLS and DASH, each stream will need to be output in its own separate muxing.\nJava\nIngestInputStream ingestInputStream = new IngestInputStream();\ningestInputStream.setInputId(input.getId());\ningestInputStream.setInputPath(inputPath);\ningestInputStream.setSelectionMode(StreamSelectionMode.AUTO);\ningestInputStream = bitmovinApi.encoding.encodings.inputStreams.ingest.create(encoding.getId(), ingestInputStream);\n\nAacAudioConfiguration config = new AacAudioConfiguration();\nconfig.setName(\"AAC 128 kbit/s\");\nconfig.setBitrate(128_000L);\nconfig = bitmovinApi.encoding.configurations.audio.aac.create(config);\n\nStreamInput streamInput = new StreamInput();\nstreamInput.setInputStreamId(ingestInputStream.getId());\n\nStream stream = new Stream();\nstream.addInputStreamsItem(streamInput);\nstream.setCodecConfigId(codecConfiguration.getId());\nstream = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream);\n\nMp4Muxing muxing = new Mp4Muxing();\nMuxingStream muxingStream = new MuxingStream();\nmuxingStream.setStreamId(stream.getId());\nmuxing.addStreamsItem(muxingStream);\n\n// ... then add video stream, define the encoding output, define a filename, etc.\n\nmuxing = bitmovinApi.encoding.encodings.muxings.mp4.create(encoding.getId(), muxing);\nAs you can see from this snippet of code, there was no need to handle any aspect of the channel mapping between the input and output for this simple use case.\nStreamSelectionMode.AUTO\n(which is the default mode so does not even need to be specified) tells the encoder to do its best at finding an audio stream in the source that makes sense to use as input stream.\nℹ️\nNote:\nif you have previously configured encodings without an\nIngestInputStream\nand instead referred to the input file directly in the Stream's creation payload, read\nthis FAQ\nto understand why we recommend that you switch to the first method.\nA full code sample with our various SDKs is provided\nin our example repository.\nUse Case 2 - Distinct Input Files\nA situation that occurs regularly is one in which each input stream comes in a distinct file. In particular, this is what you will have if you are receiving IMF packages from your content provider, since they store each track in a separate “essence” (hear: file).\nBitmovin also allows you to work with streams that are in separate files, whether or not the files also contain a video stream.\nThis is handled in much the same way as in the previous example, but now you need to have multiple\nIngestInputStreams\n,\nCodecConfigurations\nand\nStreams\n. If we refactor the code to have functions for creation of each of the resources above, the code might therefore look like the following:\nJava\nAacAudioConfiguration aacConfig = createAacStereoAudioConfig();\nAc3AudioConfiguration ac3Config = createAc3SurroundAudioConfig();\n\nIngestInputStream stereoIngestInputStream = createIngestInputStream(encoding, input, \"source_audio_stereo.xmf\");\nIngestInputStream surroundIngestInputStream = createIngestInputStream(encoding, input, \"source_audio_surround.xmf\");\n\nStream audioStream1 = createStream(encoding, stereoIngestInputStream, aacConfig);\nStream audioStream2 = createStream(encoding, surroundIngestInputStream, ac3Config);\n\n// ... do the same for the video stream ...\ncreateMp4Muxing(encoding, output, Arrays.asList(videoStream, audioStream1, audioStream2));\nA full code sample can be found\nin our example repository.\nChannel Mixing\nIn this section, we look at use cases that require manipulation of the audio channels present in the input audio stream.\nUse Case 3 - Swapping Channels\nAs before, let’s imagine that your input file has a stereo audio stream, but that somehow, the left and right channels have been reversed. It’s more of a theoretical use case (or you should have a serious word with your content provider), but it helps us illustrate in a simple way the next concept: Audio Mixing\nWith Audio Mixing, you can go down to the level of the channel (instead of the stream itself), and manipulate the channel layout.\nJust as before, you will need an\nIngestInputStream\nto point to the source file. But in addition, you now also need to involve an\nAudioMixInputStream\nto apply a transformation to that input stream, before generating your output\nStream\n.\nThe transformation here is simple: take each channel (by position in the input stream) and re-map it to the opposite output channel.\nJava\n// create the IngestInputStream\nIngestInputStream audioIngestInputStream = createIngestInputStream(encoding, input, \"source.mp4\");\n\n// define the source channels\nAudioMixInputStreamSourceChannel sourceChannel0 = new AudioMixInputStreamSourceChannel();\nsourceChannel0.setType(AudioMixSourceChannelType.CHANNEL_NUMBER);\nsourceChannel0.setChannelNumber(0);\n\nAudioMixInputStreamSourceChannel sourceChannel1 = new AudioMixInputStreamSourceChannel();\nsourceChannel1.setType(AudioMixSourceChannelType.CHANNEL_NUMBER);\nsourceChannel1.setChannelNumber(1);\n\n// define the mapping to output channels\nAudioMixInputStreamChannel outputChannel0 = new AudioMixInputStreamChannel();\noutputChannel0.setOutputChannelType(AudioMixChannelType.CHANNEL_NUMBER);\noutputChannel0.setOutputChannelNumber(0);\noutputChannel0.setInputStreamId(audioIngestInputStream.getId());\noutputChannel0.addSourceChannelsItem(sourceChannel1);\n\nAudioMixInputStreamChannel outputChannel1 = new AudioMixInputStreamChannel();\noutputChannel1.setOutputChannelType(AudioMixChannelType.CHANNEL_NUMBER);\noutputChannel1.setOutputChannelNumber(1);\noutputChannel1.setInputStreamId(audioIngestInputStream.getId());\noutputChannel1.addSourceChannelsItem(sourceChannel0);\n\n// add it all to an AudioMixInputStream and define the output channel layout\nAudioMixInputStream audioMixInputStream = new AudioMixInputStream();\naudioMixInputStream.setName(\"Swapping channels 0 and 1\");\naudioMixInputStream.setChannelLayout(AudioMixInputChannelLayout.CL_STEREO);\naudioMixInputStream.setAudioMixChannels(Arrays.asList(outputChannel0, outputChannel1));\naudioMixInputStream = bitmovinApi.encoding.encodings.inputStreams.audioMix.create(\n  encoding.getId(), audioMixInputStream);\n\n// Configure an audio codec\nAacAudioConfiguration aacConfig = createAacStereoAudioConfig();\n\n// And finally, create an output Stream with the AudioMaxInputStream and the codec\nStream stream = new Stream();\nStreamInput streamInput = new StreamInput();\nstreamInput.setInputStreamId(audioMixInputStream.getId());\nstream.addInputStreamsItem(streamInput);\nstream.setCodecConfigId(codecConfiguration.getId());\nstream = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream);\nLet’s highlight a couple of important points from that snippet of code:\nwhere before we created the\nStream\nfrom an\nIngestInputStream\n, it is now from the\nAudioMixInputStream\nthat we do so, since it represents the result of the transformation of the input stream (line 40).\nthe same audio\nIngestInputStream\nis used in the definition of both output channels in the configuration above (lines 17 and 23).\nYou can find a full code sample\nin our example repository\nAnother similar use case is if one of your logical source channels is hard “panned” to either the left or right channel of an input stereo pair, leaving the other channel empty. You can then simply select it and copy it to both output channels to center it.\nUse Case 4 - Downmixing 5.1 to 2.0\nLet’s look now at a more complex use case, which involves multiple source channels being combined into the same output channels.\nIn this example, we have a source file with a 5.1 surround audio stream, which we want to convert into a stereo-only stream in the output. We have decided that the output channels should be mixing the corresponding front channel, the center channel at lower volume, and the corresponding back channel at even lower volume.\n⚠️\nWarning:\nNote that we are not claiming that this is the right or only way of downmixing 5.1 to 2.0. You will need to discuss the correct mechanism with your content provider. The example above is only for the purpose of illustrating the advanced concepts of audio mixing.\nEnter the concept of\ngain\n. This allows us to define at what level (volume) one of the source channels should be combined with others.\nCode-wise, this looks fairly similar to the previous use case, but (naturally) with more configuration. Let’s look at just the left channel’s mixing definition:\nJava\n// define the source channels\nAudioMixInputStreamSourceChannel sourceChannelL = new AudioMixInputStreamSourceChannel();\nsourceChannelL.setType(AudioMixSourceChannelType.FRONT_LEFT);\nsourceChannelL.setGain(1.0);\n\nAudioMixInputStreamSourceChannel sourceChannelC = new AudioMixInputStreamSourceChannel();\nsourceChannelC.setType(AudioMixSourceChannelType.CENTER);\nsourceChannelC.setGain(0.8);\n\nAudioMixInputStreamSourceChannel sourceChannelLs = new AudioMixInputStreamSourceChannel();\nsourceChannelC.setType(AudioMixSourceChannelType.BACK_LEFT);\nsourceChannelC.setGain(0.5);\n\n// define the mapping to output channels\nAudioMixInputStreamChannel outputChannelL = new AudioMixInputStreamChannel();\noutputChannel0.setOutputChannelType(AudioMixChannelType.FRONT_LEFT);\noutputChannel0.setInputStreamId(audioIngestInputStream.getId());\noutputChannel0.setSourceChannels(\n  Arrays.asList(sourceChannelL, sourceChannelC, sourceChannelLs)\n);\nNotice also another difference with the previous use case. Instead of defining source channels by their position in the stream, we choose them by\nchannel type\n. If your input file has been correctly tagged, this simplifies the code and also ensures that you can handle input files that may have slightly different channel layouts (since there are multiple ways of laying out 5.1 channels in a stream in the industry)\nYou can find a full code sample\nin our example repository\n, which streamlines the example above by using functions and helper classes to improve readability.\nWhat about the AudioMix Filter?\nYou may have seen in our API documentation that we also support a\nfilter for audio mixing\n. It is functionally equivalent to the\nAudioMixInputStream\nwhen it comes to audio mixing configurations and use cases and was our initial approach to provide this feature.\nAudioMixInputStreams\nare the successor of it, so it is highly recommended to use these instead of the\nAudiomixFilter\ngoing forward as the latter will be deprecated. In particular, if you are also intending to use other functionality enabled through\nInputStreams\nresources, such as\ntrimming and concatenation\n, you will only be able to define the audio manipulation through an\nAudioMixInputStream\nin the chain of\nInputStreams\n.\nStream Mapping\nIn the previous section, we looked at how to (re-)mix audio channels in the audio input stream. In this third part, we now look at use cases that have a different number of audio streams between the input and output, and where multiple audio streams need to be combined with each other to generate an output stream.\nUse Case 5 - Mono Input Tracks\nIt is a very frequent use case, particular in broadcast workflows. The source file has multiple (often PCM) streams/tracks, each with a single mono channel that represents one of the output channels. Let’s take a middle-of-the-road example here, with 8 mono streams that contain the channels for a stereo pair and a surround audio layout.\nWe have so far met all the resources and concepts necessary to configure an encoding for this use case, but this example allows us to revisit a couple of points highlighted earlier in the tutorial.\nIn use cases 3 and 4, we had a single\nIngestInputStream\nto select the single audio stream from the stream, which could be used in all aspects of the configuration thereafter. As shown in use case 1, we could also use the automatic\nStreamSelectionMode\nto let the encoder implicitly select that audio input stream in the source file.\nWe now will have to be more explicit, and select exactly the right audio stream as input stream for our configuration, and map it appropriately in the\nAudioMixInputStream\nconfiguration to the relevant output channel.\nLet’s look at the code for the mapping of the first two channels into the stereo pair\nJava\n// create distinct IngestInputStream for each input stream\nIngestInputStream track0IngestInputStream = new IngestInputStream();\ntrack0IngestInputStream.setInputId(input.getId());\ntrack0IngestInputStream.setInputPath(\"source_8tracks.mp4\");\ntrack0IngestInputStream.setSelectionMode(StreamSelectionMode.AUDIO_RELATIVE);\ntrack0IngestInputStream.setPosition(0);\ntrack0IngestInputStream = bitmovinApi.encoding.encodings.inputStreams.ingest.create(encoding.getId(), track0IngestInputStream);\n\nIngestInputStream track1IngestInputStream = new IngestInputStream();\ntrack1IngestInputStream.setInputId(input.getId());\ntrack1IngestInputStream.setInputPath(\"source_8tracks.mp4\");\ntrack1IngestInputStream.setSelectionMode(StreamSelectionMode.AUDIO_RELATIVE);\ntrack1IngestInputStream.setPosition(1);\ntrack1IngestInputStream = bitmovinApi.encoding.encodings.inputStreams.ingest.create(encoding.getId(), track1IngestInputStream);\n\n// define how to select the single channel from a mono track\nAudioMixInputStreamSourceChannel sourceChannelMono = new AudioMixInputStreamSourceChannel();\nsourceChannelMono.setType(AudioMixSourceChannelType.CHANNEL_NUMBER);\nsourceChannelMono.setChannelNumber(0);\n\n// define output channels and mapping from source channels\nAudioMixInputStreamChannel outputChannelL = new AudioMixInputStreamChannel();\noutputChannelL.setOutputChannelType(AudioMixChannelType.FRONT_LEFT);\noutputChannelL.setInputStreamId(track0IngestInputStream.getId());\noutputChannelL.setSourceChannels(Collections.singletonList(sourceChannelMono));\n\nAudioMixInputStreamChannel outputChannelR = new AudioMixInputStreamChannel();\noutputChannelR.setOutputChannelType(AudioMixChannelType.FRONT_LEFT);\noutputChannelR.setInputStreamId(track1IngestInputStream.getId());\noutputChannelR.setSourceChannels(Collections.singletonList(sourceChannelMono));\n\n// define and create the audio mix\nAudioMixInputStream stereoMixInputStream = new AudioMixInputStream();\nstereoMixInputStream.setChannelLayout(AudioMixInputChannelLayout.CL_STEREO);\nstereoMixInputStream.setAudioMixChannels(Arrays.asList(outputChannelL, outputChannelR))\n\nstereoMixInputStream = bitmovinApi.encoding.encodings.inputStreams.audioMix.create(\n  encoding.getId(), stereoMixInputStream);\nIn this code sample, notice how each\nAudioMixInputStreamChannel\nuses a different\nIngestInputStream\nto grab a specific audio stream in the input file. As for each\nIngestInputStream\n, it selects the exact source stream by its position in the input file, in relative order between all audio streams. You could also use\nStreamSelectionMode.POSITION_ABSOLUTE\nif you prefer.\nTip:\nto determine how many streams your input file has, in what order, and with what audio layout, use\nmediainfo\nor\nffprobe\n*\nYou can find a full code sample\nin our example repository\n, which streamlines the example above by using functions and helper classes to improve code readability.\nUse Case 6 - Separate Input Files for different Channels\nAlthough this use case is not frequent, it is just a special case of the previous example, and can be handled in the exact same way. The only difference is that the input file path may now be different for different\nIngestInputStreams\n, instead of (or in addition to) the source channel number.\nWith the same mechanism, you can also now hopefully see how you could replace a single channel in a multi-channel stream with one from a different file. We will not even ask why you would want to do such a thing…\nStream Merging (or Blending)\nIn the previous section, we saw how we can map channels from distinct input streams onto output channels. In this fourth and final part, we will see how we can go one step further and use all the concepts seen so far together, and merge multiple input streams into output streams.\nUse Case 7 - Voice-Over or Background Music\nIn this use case, let’s imagine that we have a source file containing a stereo audio track. In a separate file, we have another stereo track such as a voice-over commentary or background music. To combine those, we need one final tool at our disposal: the ability to blend streams together:\nThis can be achieved very simply by providing a list of\nInputStreams\nwhen creating a\nStream\n. Blending is done channel by channel, so all input streams need to have identical channel layouts.\nJava\nIngestInputStream mainAudioIngestInputStream = createIngestInputStreamWithPosition(encoding, input, \"original.mp4\", 1);\nIngestInputStream voiceOverIngestInputStream = createIngestInputStreamWithPosition(encoding, input, \"voiceover.mp4\", 0);\n\nStream audioStream = createStream(encoding, Arrays.asList(mainAudioIngestInputStream, voiceOverIngestInputStream), aacConfig);\nℹ️\nNote:\nThe behavior is different if all input streams are mono: In this case, they will not be blended but combined into a multi-channel stream (e.g. stereo for 2 input streams, 5.1 for 6 input streams)\nWith volume adjustment\nTo make this example a little more advanced, and to show how all features can be combined, let’s also reduce the volume on the original track, so that the voice-over can be heard more clearly. To apply a volume adjustment, we need to pass the original track trough an\nAudioMixInputStream\nand define the\ngain\nvalue for each channel, as demonstrated earlier.\nJava\nIngestInputStream videoIngestInputStream = createIngestInputStreamWithPosition(encoding, input, \"original.mp4\", 0);\nIngestInputStream mainAudioIngestInputStream = createIngestInputStreamWithPosition(encoding, input, \"original.mp4\", 1);\nIngestInputStream voiceOverIngestInputStream = createIngestInputStreamWithPosition(encoding, input, \"voiceover.mp4\", 0);\n\nAudioMixInputStream gainAdjustedAudioMixInputStream = new AudioMixInputStream();\ngainAdjustedAudioMixInputStream.setChannelLayout(AudioMixInputChannelLayout.CL_STEREO);\n\nfor (int i=0; i<=1; i++) \n{\n  AudioMixInputStreamSourceChannel sourceChannel = new AudioMixInputStreamSourceChannel();\n  sourceChannel.setType(AudioMixSourceChannelType.CHANNEL_NUMBER);\n  sourceChannel.setChannelNumber(i);\n  sourceChannel.setGain(0.5);   // reduce volume to 50%\n  \n  AudioMixInputStreamChannel inputStreamChannel = new AudioMixInputStreamChannel();\n  inputStreamChannel.setOutputChannelType(AudioMixChannelType.CHANNEL_NUMBER);\n  inputStreamChannel.setOutputChannelNumber(i);\n  inputStreamChannel.setInputStreamId(mainAudioIngestInputStream.getId());\n  inputStreamChannel.addSourceChannelsItem(sourceChannel);\n  \n  gainAdjustedAudioMixInputStream.addAudioMixChannelsItem(inputStreamChannel);\n}\n\ngainAdjustedAudioMixInputStream = bitmovinApi.encoding.encodings.inputStreams.audioMix.create(\n  encoding.getId(), gainAdjustedAudioMixInputStream);\n\nStream videoStream = createStream(encoding, Collections.singletonList(videoIngestInputStream), h264Config);\nStream audioStream = createStream(encoding, Arrays.asList(voiceOverIngestInputStream, gainAdjustedAudioMixInputStream), aacConfig);\nYou can find a full code sample\nin our example repository\n, which shows a slightly different use case, in which the same source file contains 2 stereo streams, instead of them coming from different files.\nSummary\nTo conclude, let’s summarise the concepts involved and how they translate into the Bitmovin API:\nUse an\nIngestInputStream\n(\nendpoint\n) to define where your input file is\nand set an explicit\nStreamSelectionMode\nif you need to select a specific stream from that file\nuse multiple\nIngestInputStreams\nif you have multiple streams that contain your source channels\nAdd an\nAudioMixInputStream\n(\nendpoint\n) if you need to mix and map different input channels into output channels\nand set a\ngain\non individual source channels if you want to alter their volume\nMerge multiple input streams with the same audio layout by adding them to the output\nStream\n(\nendpoint\n)\nTip:\nRemember also that if you want your encoding workflow to be able to cater for inputs with different audio layouts (for example if you receive some files with 1 stream with 6 channels, and some with 6 mono streams), you could use\nStream Conditions\nto apply the correct logic.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/4f86609-image.png",
      "https://files.readme.io/8863083-image.png",
      "https://files.readme.io/f5018e9-image.png",
      "https://files.readme.io/c7560dc-image.png",
      "https://files.readme.io/bd42070-image.png",
      "https://files.readme.io/8eba639-image.png",
      "https://files.readme.io/eb2bfa5-image.png",
      "https://files.readme.io/6f8770c-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-an-encoding-with-hardware-acceleration",
    "title": "Creating an Encoding with Hardware Acceleration",
    "text": "Overview\nHardware-acceleration is a mechanism to leverage hardware capabilities for specific use cases. This usually results in increased efficiency or faster processing times in comparison to software processing on general-purpose hardware.\nSpecifically, we integrated hardware-acceleration through GPU-acceleration with\nNVIDIA T4\nGPUs on\nAmazon EC2 G4dn\ninstances (see Blog post about\nGPU Acceleration for Cloud Video Encoding\n).\n📘\nRecommendation: short form video content\nHardware-acceleration is recommended for\nshort form\nvideo content of\nup to 5 minutes\nin length. Please also refer to\nHow to reduce short form video content turnaround times\n.\nRequirements\nOpen API SDK v1.179.0+\nEncoder version v2.180.0+\nVoD encoding only, not live\noutput video codecs must be H264/AVC or H265/HEVC\nSupports only pixel formats YUV420 (input) to YUV420 (output)\nWorks only on AWS regions (Managed cloud and Cloud Connect)\nNote: audio is not affected by hardware-acceleration\n🚧\nFail-fast for incompatible encoding configurations\nAn encoding will fail with a \"Hardware encoding unsupported configuration\" message when the configuration is incompatible with the requirements and known limitations.\nKnown Limitations\nThe following features are not compatible with hardware/GPU-acceleration:\nno video filters\nno PerTitle\nno Dolby Vision\nno Forensic Watermark\nno BurnIn Subtitle\nno Thumbnails\nno PSNR\nno Multi Pass\nStart an Encoding using hardware-acceleration\nTo make use of hardware-acceleration, the preset\nVOD_HARDWARE_SHORTFORM\nfor H264/AVC or H265/HEVC must be selected.\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nEncoding encoding = new Encoding();\nencoding.setEncoderVersion(\"STABLE\");\n\n//Make sure to use AWS\nencoding.setCloudRegion(CloudRegion.AWS);\n\n...\n//your encoding configurations...\n...\n\n//Create codec configuration with a hardware-acceleration preset\nH264VideoConfiguration h264VideoConfiguration = new H264VideoConfiguration();\nh264VideoConfiguration.setName(\"Your first CodecConfig with hardware-acceleration\");\nh264VideoConfiguration.setPresetConfiguration(PresetConfiguration.VOD_HARDWARE_SHORTFORM);\nh264VideoConfiguration.setProfile(ProfileH264.HIGH);\nh264VideoConfiguration = bitmovinApi.encoding.configurations.video.h264.create(h264VideoConfiguration);\n\n...\n//your encoding configurations...\n...\n\n//Start an Encoding with StartEncodingRequest Configuration\nStartEncodingRequest startEncodingRequest = new StartEncodingRequest();\nbitmovinApi.encodings.start(encoding.getId(), startEncodingRequest);",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/required-permissions-for-gcs-buckets-for-encoding-input-and-output",
    "title": "Required Permissions for GCS Buckets for Encoding Input and Output",
    "text": "When using GCS buckets as Bitmovin inputs and outputs you will need to configure them to grant the appropriate roles and access control policies to the Bitmovin encoder.\nInput Buckets\nGCS role\n- The\nStorage Object Viewer\nrole must be added to the relevant IAM user (for which credentials or a service account is used in the Bitmovin configuration) in order to grant read permissions.\nGCS Access\n- As long as the proper GCS role was set, Input Buckets can be configured either as\npublicly accessible\nor private.\nGCS Access Control Policy\n- As long as the proper GCS role was set, the bucket access control policy can be set either as\nUniform\nor\nFine-Grained\nGCS role (for IAM user)\nGCS Access\nGCS Access Control Policy\nStorage Object Viewer\nPublic or Private\nUniform or Fine-Grained\nOutput Buckets\nGCS role\n- The Storage Object Admin role must be added to the relevant IAM user.\nGCS Access\n- As long as the proper GCS role was added, Output Buckets can be configured either as publicly accessible or private.\nGCS Access Control Policy and Bitmovin ACL\n- The bucket access control policy and the Bitmovin ACL settings must be set according to the table below.\nGCS role (for IAM user)\nGCS Access\nGCS Access Control Policy\nBitmovin ACL\nStorage Object Admin\nPublic / Private\nUniform\n1\nprivate\nStorage Object Admin\nPublic / Private\nFine-Grained\n2\nprivate/public_read\n1\nWhen using Uniform access control policy type, GCS applies access control policies to all objects in the Bucket based on the IAM permissions - and all access granted by object ACLs are revoked. To get Bitmovin outputs working properly with this approach, all\nACLs\nin\nEncodingOutput.acl[*].permission\nmust be set to private\n2\nWhen using Fine-Grained access control policy type, GCS applies access control policies based on\nAccess Control Lists (ACLs)\n. In this way, the ACLs set in\nEncodingOutput.acl[*].permission\nare applied to each object to be written in the bucket.\nTake into account that Bitmovin ACLs in\nEncodingOutput.acl[*].permission\nare set to\npublic_read\nby default - for example when creating encoding jobs from the Bitmovin Dashboard, so if you do not set an ACL explicitly, you should set a\nFine-Grained\ncontrol policy type on the bucket in order to get it working",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/stream-conditions",
    "title": "Stream Conditions",
    "text": "Introduction\nStream conditions allow you to add conditional logic in your encoding configuration. They ensure that your workflow can automatically adapt to different types of input files and streams.\nThis page lists the attributes and values available when attaching stream conditions to\nStream\nresources\nthrough its\nconditions\nparameter.\nUse the\ntutorial\nfor more information on how to use stream conditions.\nCondition object\nA\nCondition\nhas four parameters:\ntype\n\"CONDITION\"\n\"AND\"\n\"OR\"\nSet it to \"CONDITION\" to describe a single condition you want to provide. If you need more than one condition, please see the \"Conjuction Object\" section.\nattribute\nIndicates what aspect of the input is being checked\noperator\nIndicates the logical operation to perform between source value and expected value. Possible operators are\n- \"==\"\n- \"!=\"\n- \"<=\"\n- \"<\"\n- \">\"\n- \">=\"\nNote that if you use our SDKs, the operators are exposed through a ConditionOperator enumeration:\n- \"EQUAL\" for \"==\"\n- \"NOT_EQUAL\" for \"!=\"\n- \"LESS_THAN_OR_EQUAL\" for \"<=\"\n- \"LESS_THAN\" for \"<\"\n- \"GREATER_THAN\" for \">\"\n- \"GREATER_THAN_OR_EQUAL\" for \">=\"\nvalue\nAllows you to state what value to compare against the value extracted from the input file or stream.\nAll values (including numbers and booleans) should be provided as strings and will be cast automatically into the appropriate type when the encoder performs the condition check.\nExample\nThis condition checks if the total count of audio streams within an input file is greater than or equal to 2.\nJSON\nJava\n{\n  type: \"CONDITION\",\n  attribute: \"AUDIOSTREAMCOUNT\",\n  operator: \">=\",\n  value: \"2\"\n}\nprivate static Stream createStream(\n      Encoding encoding, Input input, String inputPath, CodecConfiguration codecConfiguration)\n      throws BitmovinException {\n    /*\n\t\t* See the full Example at, where you can add this Stream Condition at: \n    * https://github.com/bitmovin/bitmovin-api-sdk-examples/blob/main/java/src/main/java/DefaultManifests.java#L231\n\t\t*/\n\t\t\n    Stream stream = new Stream();\n    //...\n  \n    //Stream Condition - Only process this Stream if the input file has >= 2 audio tracks\n    Condition audioStreamCountCondition = new Condition();\n\t  audioStreamCountCondition.setAttribute(\"AUDIOSTREAMCOUNT\");\n\t  audioStreamCountCondition.setOperator(ConditionOperator.GREATER_THAN_OR_EQUAL);\n    audioStreamCountCondition.setValue(\"2\");\n  \n    stream.setConditions(audioStreamCountCondition);\n  \t\n    //...\n  \t\n    return bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream);\n}\nConjunction Object\nA\nConjunction\nconsists of two parameters:\ntype\n\"AND\"\n\"OR\"\nconditions\nExpect an array of condition objects that will be logically chained together based on conjunction object type.\nExample\nJSON\nJava\n{\n  type: \"AND\",\n  conditions: [\n  \t{\n      type: \"CONDITION\",\n      attribute: \"AUDIOSTREAMCOUNT\",\n      operator: \">=\",\n      value: \"2\"\n    },\n    {\n      type: \"CONDITION\",\n      attribute: \"DURATION\",\n      operator: \">=\",\n      value: \"30.56\"\n    }\n  ]\n}\nprivate static Stream createStream(\n      Encoding encoding, Input input, String inputPath, CodecConfiguration codecConfiguration)\n      throws BitmovinException {\n    /*\n\t\t* See the full Example at, where you can add this Stream Condition at: \n    * https://github.com/bitmovin/bitmovin-api-sdk-examples/blob/main/java/src/main/java/DefaultManifests.java#L231\n\t\t*/\n\t\t\n    Stream stream = new Stream();\n    //...\n  \n    //Stream Condition - Only process this Stream if the input file has >= 2 audio tracks\n    Condition audioStreamCountCondition = new Condition();\n\t  audioStreamCountCondition.setAttribute(\"AUDIOSTREAMCOUNT\");\n\t  audioStreamCountCondition.setOperator(ConditionOperator.GREATER_THAN_OR_EQUAL);\n    audioStreamCountCondition.setValue(\"2\");\n\n    //Stream Condition - Only process this Stream if the duration of the input track is >= 30.56s\n    Condition durationCondition = new Condition();\n\t  durationCondition.setAttribute(\"DURATION\");\n\t  durationCondition.setOperator(ConditionOperator.GREATER_THAN_OR_EQUAL);\n    durationCondition.setValue(\"30.56\");\n\n  \t//Logically AND those two conditions and provide that to the Stream\n    AndConjunction andConjunction = new AndConjunction();\n    andConjunction.setConditions(\n      Collections.singletonList(audioStreamCountCondition,durationCondition)\n    );\n  \n  \t//Add conditions to the Stream\n    stream.setConditions(andConjunction);\n  \t\n    //...\n  \t\n    return bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream);\n}\nExtracted values\nAt times, you may find that the condition you configure does not seem to work. It may be that the file you are using does not conform to your expectations.\nTo troubleshoot this, you can verify the result of the analysis stage, by looking at the\nanalysisDetails\nproperty of the\nStream\n(or\ninputStreams.analysisDetails\ndepending on your input configuration), which will be populated during the encoding.\nWe state below, for each attribute, which property in the\nanalysisDetails\nobject will reflect the value extracted from the source during analysis.\nNote that the\nanalysisDetails\nproperty is only available when you call the REST API directly, and is not currently exposed through the SDKs. You can for example use the REST client embedded in our API reference.\nInput file\nThe following attributes are used to check the input file:\nAttribute\nDescription\nExtracted Value\nWorks for VoD\nWorks for LIVE\nINPUTSTREAM\nchecks whether there is a stream to be found in the input file, at the position defined by the\nselectionMode\nparameter of the\nStreamInput\nresource attached to the\nStream\n. You must use the\nEQUAL\ncondition operator to test this condition\ntrue\nor\nfalse\n✅\n✅\nSTREAMCOUNT\nchecks the number of streams (or tracks) present in the input file\nanalysisDetails.numberOfStreams\n✅\n✅\nVIDEOSTREAMCOUNT\nchecks specifically the number of video streams available in the input file\nsize of\nanalysisDetails.videoStreams\narray\n✅\n✅\nAUDIOSTREAMCOUNT\nchecks specifically the number of audio streams available in the input file\nsize of\nanalysisDetails.audioStreams\narray\n✅\n✅\nDURATION\nchecks the duration of the input stream, and is expressed as a decimal representation in seconds\nanalysisDetails.videoStreams[0].duration\nor\nanalysisDetails.duration\n✅\n✖️\nVideo Stream\nIn addition to the above, the following attributes are only available when evaluating conditions against video streams. Extracted values are compared against the property of the appropriate item in the\nanalysisDetails.videoStreams\narray\nAttribute\nDescription\nExtracted Value\nWorks for VoD\nWorks for LIVE\nHEIGHT\nin pixels\nheight\n✅\n✅\nWIDTH\nin pixels\nwidth\n✅\n✅\nBITRATE\nexpressed as an integer in bits per second (bps)\nbitrate\n✅\n✖️\nFPS\nframe rate expressed as a number of frames per second and parsed to a decimal representation (ie. you can also express it as fraction such as \"3000/1001\"\nfps\n✅\n✅\nASPECTRATIO\nas a decimal value. Therefore a value larger than 1 is therefore landscape, and lower than 1 portrait\ncalculated as\nwidth\n/\nheight\n*\npar\n✅\n✅\nROTATION\nin degrees\nrotation\n✅\n✅\nAudio Stream\nThe following attributes apply to audio input streams only. Extracted values are compared against the property of the appropriate item in the\nanalysisDetails.audioStreams\narray\nAttribute\nDescription\nExtracted Value\nWorks for VoD\nWorks for LIVE\nBITRATE\nexpressed as an integer in bits per second (bps)\nbitrate\n✅\n✖️\nLANGUAGE\naudio language. If none is set in the input stream, the value will be \"und\"\nlanguage\n✅\n✅\nCHANNELFORMAT\nused to check on the number of channels in the audio stream. For example, a stereo input track would have a value of 2, a input track with dolby digital would have 6\nchannelFormat\n✅\n✅\nCHANNELLAYOUT\nA tag describing the channel layout. See\nhttps://ffmpeg.org/ffprobe-all.html#Channel-Layout\nfor a list of possible values\nchannelLayout\n✅\n✅",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-heartbeat",
    "title": "Live Heartbeat",
    "text": "The Live Heartbeat is a Global Notification that can be configured to be deployed to all live encoders in an organization. Once enabled as a global variable, the live heartbeat service will be deployed on all instances used for live encodings from that point forward.\nThe service provides a push notification as a webhook to a defined endpoint, each Live Encoder is therefore responsible for providing the notification to that endpoint and in this way it can be considered the single source of truth about a live encoders health and reported status. Deploying the heartbeat directly to the live encoder infrastructure also allows this service to scale, without the risk of their being a bottleneck in the Bitmovin multitenant platform.\nEnabling the Live Heartbeat\nThe live heartbeat is a global setting - therefore once configured all live encodings will start posting updates.\n📘\nIt is possible to configure multiple Heartbeats, to multiple endpoints.\nHowever the payload of will always been the same and can not be modified.\nVia the Dashboard\nTo enable the notification navigate to Notifications in the Configurations settings under Live Encoding\nForm here it is possible to configure triggered event notifications via Webhook & Email, and Heartbeat.\nPressing\nCreate+\nwill open a pop-up window where the Heartbeat can be configured and saved.\nInterval\ncan be set in seconds, by default it will be 20 seconds, at minimum it can be 1 second.\nOnce created the Live Heartbeat can be reviewed and deleted.\nVia the API\nThe new “Live Encoding Heartbeat” webhook can be configured via the\n/notifications/webhooks/encoding/encodings API.\nTo list current heartbeats, use\nList 'Live Encoding Heartbeat' Webhooks\nTo create, use\nAdd 'Live Encoding Heartbeat' Webhook\nTo get details, use\n'Live Encoding Heartbeat' Webhook Details\nTo delete, use\nDelete 'Live Encoding Heartbeat' Webhook\nOnce configured all Live Encodings started afterward will send the webhook in a fixed interval.\nPayload\nBelow is an example payload.\nJSON\n{\n  \"encoding\": {\n    \"cloudRegion\": \"AWS_EU_WEST_1\",\n    \"id\": \"232610f5-ff80-4146-a21b-54dd123fb5fd\",\n    \"name\": \"Test Live Heartbeat\",\n    \"encoderVersion\": \"BETA\",\n    \"type\": \"LIVE\"\n  },\n  \"webhookId\": \"1c03498b-0454-42d4-8f05-48ecb45fe7ee\",\n  \"orgId\": \"1f4cf17c-c6a6-4e9a-8f6f-6776ebe571da\",\n  \"resourceId\": \"cf9e71b5-e136-40ad-a47b-788568a4edc5\",\n  \"resourceType\": \"ENCODING\",\n  \"triggeredAt\": \"2024-06-14T12:15:41.489249069Z\",\n  \"value\": {\n    \"ingest\": {\n      \"healthy\": true,\n      \"status\": \"CONNECTED\",\n      \"streamInfos\": [\n        {\n          \"bitrate\": 1303552,\n          \"codec\": \"h264\",\n          \"healthy\": true,\n          \"incomingBitrate\": 211394.43333333332,\n          \"lastArrivalTime\": \"2024-06-14T12:15:41:455Z\",\n          \"lastTimestamp\": 107858,\n          \"mediaType\": \"video\",\n          \"rate\": 23,\n          \"samplesReadPerSecondAvg\": 23.96666666666667,\n          \"streamId\": \"0\",\n          \"lastTimestampTimescale\": 1000,\n          \"height\": 1080,\n          \"width\": 1920\n        },\n        {\n          \"bitrate\": 128000,\n          \"codec\": \"aac\",\n          \"healthy\": true,\n          \"incomingBitrate\": 16609.149999999998,\n          \"lastArrivalTime\": \"2024-0-14T12:15:41:476Z\",\n          \"lastTimestamp\": 107899,\n          \"mediaType\": \"audio\",\n          \"rate\": 44100,\n          \"samplesReadPerSecondAvg\": 44100.26666666666,\n          \"streamId\": \"1\",\n          \"lastTimestampTimescale\": 1000,\n          \"height\": null,\n          \"width\": null\n        }\n      ]\n    }\n  },\n  \"id\": \"85f0fdde-4f26-42df-bc1a-4807cc586906\",\n  \"eventType\": \"LIVE_ENCODING_HEARTBEAT\",\n  \"customData\": {}\n}\nPayload Explanation\nencoding\n: Contains information about the live encoding configuration\ncloudRegion\n: Cloud region where the encoding is running\nid\n: Unique identifier for the encoding\nname\n: Name of the encoding\nencoderVersion\n: Version of the encoder being used\ntype\n: Indicates the encoding type (that it is a live encoding)\nwebhookId\n: Unique identifier for the webhook\norgId\n: Organization ID that owns this encoding\nresourceId\n:  ID of the resource (if the resourceType is ENCODING it would refer to the encoding ID)\nresourceType\n: Type of the resource for which the webhook was fired. (e.g.: ENCODING)\ntriggeredAt\n: Timestamp (ISO 8601-Format) when the heartbeat was triggered (independent from any ingest data)\nvalue\n: Contains the actual heartbeat data\ningest\n: Information about the input stream\nhealthy\n: Current health status of the ingest\nstatus\n: Current status of the input stream (CONNECTED, WAITING_FOR_FIRST_CONNECT, DISCONNECTED, ERROR, UNKNOWN)\nstreamInfos\n: Array of information about each stream (video/audio)\nEach stream in\nstreamInfos\ncontains:\nbitrate\n:  The bitrate of the input stream as it is signalled by the container format, in bits per second\ncodec\n: Codec being used (eg. h264 for video, aac for audio)\nhealthy\n: Health status of this specific stream\nincomingBitrate\n: Average actual measured incoming bitrate (not to confuse with\nbitrate\nfield) in bytes per second during the last minute\nlastArrivalTime\n: Timestamp (ISO 8601-Format) when the Encoder received a media packet for a specific stream. (e.g. the last arrival time of a video packet).\nlastTimestamp\n:  Presentation timestamp (PTS) (the time at which the decompressed packet will be presented to the user) of the last received input stream packet\nmediaType\n: Type of media (video/audio/subtitles)\nrate\n:  Input stream frame rate for video or input sample rate for audio\nsamplesReadPerSecondAvg\n: Average frames/samples processed per second during the last minute\nstreamId\n: Unique identifier for this stream (0, 1, ..)\nlastTimestampTimescale\n: Timescale for\nlastTimestamp\nheight\n: Video height in pixels (null for audio)\nwidth\n: Video width in pixels (null for audio)\nid\n: Unique identifier for this heartbeat event\neventType\n: Type of event being reported (eg. LIVE_ENCODING_HEARTBEAT)\ncustomData\n: Additional custom data",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/c852c34-Screenshot_2024-08-02_at_16.00.49.png",
      "https://files.readme.io/2329e1d-Screenshot_2024-08-02_at_16.06.23.png",
      "https://files.readme.io/6fc366d-Screenshot_2024-08-02_at_16.02.40.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rtmp-live-stream-input-requirements",
    "title": "RTMP Live Stream Input Requirements",
    "text": "Why Input Compliance Matters\nBitmovin’s encoder delivers high-quality outputs by relying on correctly formatted and consistent input streams. RTMP streams that deviate from these standards may result in issues such as audio-video synchronization problems, dropped frames, or processing errors. Understanding and meeting the following requirements will help avoid common pitfalls.\nRTMP Input Requirements\nStream Format\nTo ensure smooth processing, RTMP streams must:\nFollow RTMP Protocol:\nThe stream must adhere to the Real-Time Messaging Protocol (RTMP) specification, including:\nProper handshakes (C0, C1, C2 sequence).\nStandard chunking and message structures.\nUse Supported Codecs:\nThe stream must use codecs supported by Bitmovin. Refer to the\nSupported Input and Output Formats\npage for details.\nMaintain Format Consistency:\nCodec and format parameters, such as resolution and frame rate, must not change during the stream. A constant frame rate (CFR) is expected and mandatory.\nPacketization and Timing\nCorrect Timestamps:\nPackets must have monotonically increasing timestamps without any backward jumps.\nKeyframe Interval:\nStreams should include periodic keyframes (I-frames) to allow for proper seeking and stream recovery. A keyframe interval of 2–5 seconds is typical.\nAudio-Video Sync:\nThe input stream must ensure that audio and video remain synchronized at the source. Significant drift may lead to playback or transcoding issues.\nConnection and Transport\nHandshake and Connection:\nThe server must correctly implement the RTMP handshake sequence (C0, C1, C2) as per the RTMP specification. This includes negotiating the protocol version.\nChunk Sizes:\nDefault RTMP chunk size is 128 bytes, but this can be negotiated during the handshake. We expect consistent chunk sizes throughout the session.\nStream Metadata\nStreams must provide accurate metadata in AMF (Action Message Format), including:\nVideo Resolution:\nWidth and height of the video.\nFramerate:\nFrames per second.\nAudio Details:\nCodec, sample rate, and channels.\nFramerate Handling:\nWe use the framerate for various fallback handlings (such as - but not limited to - gap filling). The framerate is read from the AMF metadata.\nThe Bitmovin encoder uses a heuristic approach to handle discrepancies:\nIf the metadata\nor\ncalculated\nr_base_frame_rate\nmatches a commonly used value, it is prioritized.\nIf neither is plausible, the system uses the first valid value that is not zero.\nError Handling\nPacket Loss Tolerance:\nMinor packet loss is handled gracefully. However, severe loss or malformed packets may cause dropped frames or processing errors.\nReconnection:\nThe encoder attempts to reconnect automatically if the RTMP connection drops.\nBuffering:\nSome buffering is employed to mitigate jitter and bitrate variations. Excessive jitter or highly variable bitrates may still impact processing.\nWhat Happens With Non-Compliant Streams?\nWhen input streams do not meet the outlined requirements, issues may arise, such as:\nA/V Sync Errors:\nCaused by incorrect timestamps or drift.\nFrame Drops:\nDue to missing keyframes or malformed packets.\nMetadata Conflicts:\nLeading to fallback mechanisms that may not produce the expected results.\nBitmovin’s encoder includes several corrective measures designed to address common issues found in user-generated content (UGC). These measures help mitigate problems such as inconsistent metadata or minor synchronization issues.\nHowever, streams that deviate significantly from the outlined requirements may still result in issues that cannot be fully corrected by the encoder. Ensuring compliance at the source remains essential to achieve optimal results.\nBest Practices for Streamers\nTo ensure the best results with Bitmovin’s encoding service:\nValidate Your Stream:\nUse tools to verify that your RTMP stream complies with the specifications outlined here.\nInclude Keyframes:\nConfigure your encoder to insert periodic I-frames (typically 2–5 seconds).\nMonitor Sync:\nTest audio-video synchronization at the source.\nIf issues persist, provide detailed information to the Bitmovin support team, including logs, metadata, and a sample stream for analysis.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/per-title-configuration-options",
    "title": "Per-Title Configuration Options",
    "text": "Overview\nWhile the Per Title Algorithm can determine all it needs on its own, you can also customize and adjust the way it works in order to be able to make consider your specific encoding requirements as well. Therefore you have several options available for each codec that is supported by our Per-Title encoding algorithm to tune its results to your requirements.\nStream Modes\nStream modes are defining the nature of the encoding: Fixed bitrate ladder, also referred as STANDARD vs. Per-Title.\nSTANDARD\n- Standard encoding. This is used to configure renditions that must show up in the output, i.e. will get encoded exactly as configured (no Per-Title encoding).\nPER_TITLE_TEMPLATE\n- This stream will be used as a template for the Per-Title encoding. The Codec Configuration, Muxings, DRMs and Filters applied to the generated Per-Title profile will be based on the same, or closest matching resolutions defined in the template. Please note, that template streams are not necessarily used for the encoding - they are just used as template. Every output (e.g. Muxing or DRM) must define either {uuid} or {bitrate} in the output path. These placeholders will be replaced during the generation of the Per-Title profile.\nPER_TITLE_TEMPLATE_FIXED_RESOLUTION\nIt works the same way as the\nPER_TITLE_TEMPLATE\nmode, but it will use the resolution defined in your codec configuration as stated. By contrast,\nPER_TITLE_TEMPLATE\nwould consider the defined resolution only as a recommendation, which it would abandon if it was able to obtain a better quality at a different resolution.\nPER_TITLE_RESULT\n- (read only) This mode is available to the Per-Title algorithm only, and is used for streams that got generated and added to the encoding automatically. Later on, they can be used to e.g., generate the appropriate manifests. Codec configurations, muxings, DRM configurations are based on the corresponding PER_TITLE_TEMPLATE stream.\nGeneral Codec Configuration Options\nThese options are available for H264, H265, VP9 and AV1 Per-Title configurations. Where applicable, we provide the\ndefault value\nfor each setting.\nminBitrate:\n240000\n(number, optional) - The minimum bitrate that will be used when generating the per-title profile.\nmaxBitrate:\n1500000\n(number, optional) - The maximum bitrate that will be used when generating the per-title profile. It will not generate any rendition with a higher target bitrate.\nminBitrateStepSize:\n1.5\n(number, optional) - This defines the minimum jump from one bitrate to the next higher bitrate, e.g., if the first bitrate is 240,000 and the min step size is 1.5 the next bitrate will be at least 360,000.\nmaxBitrateStepSize:\n1.9\n(number, optional) - This defines the maximum jump from one bitrate to the next higher bitrate, e.g., if the first bitrate is 240,000 and the max step size is 1.9 the next bitrate will be at most 456,000.\ntargetQualityCrf:\n(number, optional) - This defines the desired target quality of the highest rendition generated by the Per-Title algorithm expressed as CRF value. This allows you to define a trade-off between bitrate and quality of the highest rendition.\nIf choosing a lower value the highest rendition will have higher quality but also higher bitrate. The Per-Title algorithm might generate more renditions to hit the target quality.\nIf choosing a higher value the highest rendition will have lower quality but also lower bitrate and the Per-Title algorithm might generate fewer renditions.\nThe default value is codec-dependent:\n22 for H264/AVC\n26 for H265/HEVC and VP9\n34 for AV1\nautoRepresentations:\n(PerTitleAutoRepresentation, optional) - If set, stream configurations will be automatically added to the per-title profile. In that case make sure that you set at least one\nPER_TITLE_TEMPLATE\nstream configuration. All other configurations will be automatically chosen by the per-title algorithm. All relevant settings for streams and muxings will be taken from the closest\nPER_TITLE_TEMPLATE\nstream defined. The closest stream will be chosen based on the resolution specified in the codec configuration.\nThe parameter\nadoptConfigurationThreshold\nallows you to define if the algorithm should prefer the settings from the higher or the lower resolution. E.g., given you have two Per-Title template streams and an\nadoptConfigurationThreshold\nof 0.5, one for a resolution where only the width is set to 382x and one for 1920x. A generated Per-Title result stream with a resolution of 640x would take settings from the Per-Title template stream with the resolution 382x as it is closer to the resolution of 1920x. Conversely a generated Per-Title result stream with a resolution of 1280x would take the settings from the Per-Title template stream with the resolution of 1920x. Note that\nadoptConfigurationThreshold\ncan take a value between 0 and 1, where values closer to 0 will favour the higher resolution, and the lower one for values closer to 1\n(*1)\n.\nIf\nautoRepresentations\nis not set, define one\nPER_TITLE_TEMPLATE\nstream for each resolution that you would like the per-title algorithm to consider when generating the per-title profile.\ncomplexityFactor:\n1.0\n(number, optional) - Will modify the assumed complexity for the Per-Title algorithm (> 0.0). Values higher than 1, will increase complexity and thus select smaller resolutions for given bitrates. This will also result in a higher bitrate for the top rendition. Values lower than 1, will decrease assumed complexity and thus select higher resolutions for given bitrates and also decrease the bitrate of the top rendition.\nAll available options can be found in our\nAPI reference\nas well.\nConfiguration Options for H264, H265\nWhile all of the options described above are available to every codec, the following are specific to H264, and H265 only.\ncodecMinBitrateFactor:\n(optional) This factor is used to calculate the minBitrate of the codec configuration for the generated representations as a multiple of the targetBitrate.\ncodecMaxBitrateFactor:\n(optional) This factor is used to calculate the maxBitrate of the codec configuration for the generated representations as a multiple of the targetBitrate\ncodecBufsizeFactor:\n(optional) This factor is used to calculate the bufsize of the codec configuration for the generated representations as a multiple of the targetBitrate\nAll available options can be found in our\nAPI reference\n.\nFootnotes\n(1) - The\nadoptConfigurationThreshold\nsetting is used when generating per-title result streams, to decide from which of the 2 nearest per-title templates codec settings should be copied. Using the resolution from 2 templates, an actual resolution threshold is calculated in the following way:\nthresholdRes = ((higherRes - lowerRes) * adoptConfigurationThreshold) + lowerRes\nFor any generated resolution\nlower than or equal to\nthis threshold resolution, the lower template is used.\nSo, if the two templates have widths of 1920 and 640, and the\nadoptConfigurationThreshold\nis 0.7, any result stream with a width below or equal to 1536 will use the 640x template.\nIt follows that if you want to make sure that the nearest higher template is always selected for any given result stream, set\nadoptConfigurationThreshold\nto 0\nFinally, note that this calculation works whether you set the height or the width on the per-title template streams, since the aspect ratio of the source is retained when only setting either of them on the codec configuration",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-mpeg-cenc-clearkey-content",
    "title": "Creating MPEG-CENC ClearKey Content",
    "text": "Overview\nClear Key encryption is an interface supported by EME. This interface can be used to deliver MPEG-DASH content with Clear Key. The interface provides the basic functionality that the user could provide a key that will be used for the decryption of the segments. MPEG-DASH signals the key in the Media Presentation Duration (MPD), which is the manifest of MPEG-DASH. All the relevant information that is needed for decryption is included in the MPD.\nAbout this example\nThe code snippets shown here are based on the\nfull example\ncalled\nCencDrmContentProtection.java\n, using our\nBitmovin SDK for Java\n.\nHint:\nIf you haven't created any encodings with our Service yet, its recommended to start with our quick start guide called \"\nGet Started with the Bitmovin API\n\" first, before you continue :)\nEncoding configuration\nThe key part to create an encoding that encodes and encrypt adhering MPEG-CENC standards, is to add a\nCencDRM\nConfiguration to an\nfMP4 Muxing\n.\nSo to encrypt your content an\nencryption key\n(referred to as\nkey\nlater on) is required. A\nContent ID\nor\nkid\nis optional in this case, but can be provided if you want to.\nImportant:\nMPEG CENC ClearKey Encryption provides an additional layer of security for the stored content, the decryption on the client side doesn't happen in an isolated secure environment as it is the case with an actual DRM solution. The Decryption is handled by a HTML5 player directly, therefore the key (and kid if needed) is part of its configuration and therefore accessible on the client side.\nLearn more about DRM\n.\nGeneral configuration values:\nkey\n: (required) You need to provide a key that will be used to encrypt the content (16 byte encryption key, represented as 32 hexadecimal characters)\nkid\n: (optional) also known as Key ID, or ContentID. Its a unique identifer for your content (16 byte initialization vector, represented as 32 hexadecimal characters)\nJava SDK Example - createDrmConfig() Method\n(\nLine in Example\n)\nJava\nprivate static CencDrm createDrmConfig(\n    Encoding encoding, Muxing muxing, Output output, String outputPath) throws BitmovinException {\n  CencDrm cencDrm = new CencDrm();\n  cencDrm.addOutputsItem(buildEncodingOutput(output, outputPath));\n  cencDrm.setKey(\"cab5b529ae28d5cc5e3e7bc3fd4a544d\");\n  cencDrm.setKid(\"08eecef4b026deec395234d94218273d\");\n  \n  return bitmovinApi.encoding.encodings.muxings.fmp4.drm.cenc.create(\n      encoding.getId(), muxing.getId(), cencDrm);\n}\nHow to play MPEG-CENC ClearKey content?\nNow, that you are able to create this content, you most likely want to play it as well :) Therefore, please see our tutorial,\nhow you can play MPEG-CENC ClearKey encrypted content\nwith our Bitmovin Player\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/vp9-presets",
    "title": "VP9 Presets",
    "text": "VP9 Presets\nVoD Quality Preset Configurations\nVoD Quality Presets\nVOD_STANDARD\nVOD_HIGH_QUALITY\naqMode\nVARIANCE\nVARIANCE\narnrMaxFrames\n0\n0\narnrStrength\n0\n0\narnrType\nCENTERED\nCENTERED\nencodingMode\nTWO_PASS\nTHREE_PASS\nframeParallel\ntrue\ntrue\nlagInFrames\n25\n25\nlossless\nfalse\nfalse\nmaxIntraRate\n0\n0\nnoiseSensitivity\nfalse\nfalse\nqpMax\n63\n63\nqpMin\n0\n0\nquality\nGOOD\nGOOD\nrateOvershootPct\n25\n25\nrateUndershootPct\n25\n25\nstaticThresh\n0\n0\ntileColumns\n+\n+\ncpuUsed\n+\n+\ntileRows\n0\n0\n+\nsmart value which is set automatically based on preset and resolution\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.\nVoD Speed Preset Configurations\nVoD Speed Presets\nVOD_SPEED\naqMode\nVARIANCE\narnrMaxFrames\n0\narnrStrength\n0\narnrType\nCENTERED\nencodingMode\nSINGLE_PASS\nframeParallel\ntrue\nlagInFrames\n25\nlossless\nfalse\nmaxIntraRate\n0\nnoiseSensitivity\nfalse\nqpMax\n63\nqpMin\n0\nquality\nGOOD\nrateOvershootPct\n25\nrateUndershootPct\n25\nstaticThresh\n0\ntileColumns\n+\ncpuUsed\n+\ntileRows\n0\n+\nsmart value which is set automatically based on preset and resolution\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-aws-s3-role-based-outputs",
    "title": "Using AWS S3 Role-Based Outputs",
    "text": "S3 role-based Outputs\nare an alternative way to access your AWS S3 bucket to be used as an Input (Encoding) and/or Output (Encoding/Analytics). Instead of requiring an Access/Secret key pair, we provide you with an AWS IAM user name, to which you then grant specific access rights in your account so it can access your desired S3 bucket.\nHave a look at our tutorials for\ncreating S3 role-based outputs with the Bitmovin API\nor\nfor our Analytics service and how to define them there as outputs for data exports",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/h264-presets-live",
    "title": "H264 Presets",
    "text": "Live Quality Preset Configurations\nLive Quality Presets\nLIVE_STANDARD\nLIVE_HIGH_QUALITY\nLIVE_VERYHIGH_QUALITY\nLIVE_ULTRAHIGH_QUALITY\nadaptiveQuantizationMode\nVARIANCE\nVARIANCE\nVARIANCE\nVARIANCE\nadaptiveQuantizationStrength\n1\n1\n1\n1\nadaptiveSpatialTransform\nTRUE\nTRUE\nTRUE\nTRUE\nbAdaptiveStrategy\nFAST\nFAST\nFAST\nFULL\nbPyramid\nNORMAL\nNORMAL\nNORMAL\nNORMAL\nbframes\n3\n3\n3\n3\ncabac\nTRUE\nTRUE\nTRUE\nTRUE\nencodingMode\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nTWO_PASS\nfastSkipDetectionPFrames\nTRUE\nTRUE\nTRUE\nTRUE\nmacroblockTreeRatecontrol\nTRUE\nTRUE\nTRUE\nTRUE\nmixedReferences\nFALSE\nTRUE\nTRUE\nTRUE\nmotionEstimationMethod\nHEX\nHEX\nHEX\nUMH\nmvPredictionMode\nSPATIAL\nSPATIAL\nSPATIAL\nAUTO\nmvSearchRangeMax\n16\n16\n16\n16\nnalHrd\nNONE\nNONE\nNONE\nNONE\npartitions\n\"I4X4,I8X8,P8X8,B8X8\"\n\"I4X4,I8X8,P8X8,B8X8\"\n\"I4X4,I8X8,P8X8,B8X8\"\n\"I4X4,I8X8,P8X8,B8X8\"\nprofile\nHIGH\nHIGH\nHIGH\nHIGH\nrcLookahead\n20\n30\n40\n50\nrefFrames\n2\n2\n3\n5\nsceneCutThreshold\n0\n0\n0\n0\nsubMe\nQPEL4\nRD_IP\nRD_ALL\nRD_REF_IP\ntrellis\nENABLED_FINAL_MB\nENABLED_FINAL_MB\nENABLED_FINAL_MB\nENABLED_FINAL_MB\nweightedPredictionBFrames\nTRUE\nTRUE\nTRUE\nTRUE\nweightedPredictionPFrames\nSIMPLE\nSIMPLE\nSMART\nSMART\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.\nLive Low Latency Preset Configurations\nLive Low Latency Presets\nLIVE_LOW_LATENCY\nLIVE_LOWER_LATENCY\nLIVE_VERYLOW_LATENCY\nadaptiveQuantizationMode\nVARIANCE\nVARIANCE\nDISABLED\nadaptiveQuantizationStrength\n1\n1\n0\nadaptiveSpatialTransform\nTRUE\nTRUE\nFALSE\nbAdaptiveStrategy\nFAST\nFAST\nNONE\nbPyramid\nNORMAL\nNORMAL\nNORMAL\nbframes\n3\n3\n0\ncabac\nTRUE\nTRUE\nFALSE\nencodingMode\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nfastSkipDetectionPFrames\nTRUE\nTRUE\nTRUE\nmacroblockTreeRatecontrol\nTRUE\nFALSE\nFALSE\nmixedReferences\nFALSE\nFALSE\nFALSE\nmotionEstimationMethod\nHEX\nDIA\nDIA\nmvPredictionMode\nSPATIAL\nSPATIAL\nSPATIAL\nmvSearchRangeMax\n16\n16\n16\nnalHrd\nNONE\nNONE\nNONE\npartitions\n\"I4X4,I8X8,P8X8,B8X8\"\n\"I4X4,I8X8\"\nNONE\nprofile\nHIGH\nHIGH\nBASELINE\nrcLookahead\n10\n0\n0\nrefFrames\n1\n1\n1\nsceneCutThreshold\n0\n0\n0\nsubMe\nSATD\nSAD\nFULLPEL\ntrellis\nDISABLED\nDISABLED\nDISABLED\nweightedPredictionBFrames\nTRUE\nTRUE\nFALSE\nweightedPredictionPFrames\nSIMPLE\nSIMPLE\nDISABLED\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.\nConformance with H264 Profiles\nThe majority of the profiles listed above set the H264 profile to\nHIGH\n, and use codec capabilities that target decoders that support this profile. The H264 High profile is by far the most common and has been widely supported for many years.\nIf your application requires the use of lower profiles, such as\nBASELINE\nor\nMAIN\n, you will need to overwrite some of the codec configuration parameters after applying the preset. Note however that the output quality may be reduced in doing so, and file sizes and bandwidth requirements may increase.\nTo set the profile to\nMAIN\nOverwrite\nadaptiveSpatialTransform\nto False\nTo set the profile to\nBASELINE\nOverwrite\nadaptiveSpatialTransform\nto False\nOverwrite\nbFrames\nto 0\nOverwrite\ncabac\nto False\nOverwrite\nweightedPredictionPFrames\nto DISABLED\nConversely, if your selected preset sets the profile to\nMAIN\nand you want to ensure that the output uses\nHIGH\n, you can make a simple change:\nTo force the profile to\nHIGH\nOverwrite\nadaptiveSpatialTransform\nto True",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/overview-per-title-encoding",
    "title": "Overview Per-Title-Encoding",
    "text": "A regular encoding simply follows the rules you define in its stream-, codec-, and muxing configurations. A Per-Title Encoding in comparison not only uses the given configuration but evaluates the asset itself. Based on that data, the algorithm adjusts the parameters width, height, and bitrate, optimizing your bitrate ladder to increase quality while reducing the required bandwidth to deliver it. The below graphic compares the so-called Convex Hull (basically an envelope of the regions where certain resolution/bitrate combinations work best) of a standard (non-dynamic) bitrate ladder and the Per-Title one.\nFig.1 - Comparison between Per-Title and Standard Profile in terms of Quality and Bitrate\nAs you can see, the Per-Title Profile is able to achieve a much better quality by using lower bitrates in the human perceptual range (roughly between 35dB and 45dB). Furthermore, it removes the resolution/bitrate tuples above 45dB, which do not lead to a visual quality improvement anymore.\nTry it yourself\nIf you don't have an Bitmovin Acccount yet, simply\nsign up here\nand use our\nPer-Title Benchmark Tool\nfor free as part of the trial of your account. All you have to do is to provide it with an URL to your input file and provided it with the bitrate ladder you are currently using. Then it will compare an encoding processed with your static ladder against the one derived automatically by the Per-Title algorithm.\nCreate a Per-Title Encoding\nWhile you can leave the decisions entirely to the Per-Title Encoding, you can also provide it with some ground rules that have to be met regardless its own results. You might want to do this to accommodate requirements from your clients or content providers who want to have specific resolutions to be present, or to use different encryption keys for specific renditions, and so on. How to do that, and how you adjust the configuration of an Per-Title Encoding is shown in this\ntutorial\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/bee9c72-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-outputs",
    "title": "Live Outputs",
    "text": "Bitmovin supports a range of output options and the following articles provide guidance on how to configure them. This can be configured via the API or Dashboard.\nConfigured outputs can be found under Live Encoding -> Outputs\n🚧\nCurrently both Live and VOD outputs will be shown in this view\nSupported Outputs\nBitmovin CDN\nAkamai NetStorage\nAkamai MSL4\nAzure Blob Storage\nGoogle Cloud Storage\nAWS S3 Access and Secret Keys\nAWS S3 Role Based\nGeneric S3",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/544c1aa-Screenshot_2024-04-05_at_18.41.26.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/nodejs-javascript-sdk",
    "title": "Node.js / JavaScript SDK",
    "text": "The Bitmovin API enables you to build cutting edge video workflows while leveraging emmy-award winning encoding solutions, that automatically optimize your catalog for high quality and cost efficient video streaming.\nQuick Start Guide\nThis Quick Start Video will demonstrate different approaches to setup an encoding workflow using a\nWatch Folder Workflow (No Code Required)\nand a\nfull Python API SDK example\nusing this\nColabortory Notebook\n.\nundefined\nGet Started with your own Project\nThe following steps will walk you through adding the SDK to an new or existing Project step by step.\nOverview\nStarting an encoding triggers a request for encoding instances. Once available, the input file will be downloaded, divided into chunks, and distributed between all of them. Those instances transcode each chunk into segments for all configured video/audio representations (muxings) in parallel and write them to the designated output destination.\nThis approach makes Bitmovin's encoding the fastest in the world.\nIn this tutorial you will implement a simple VOD online streaming scenario, where an input file containing a video stream and stereo audio stream is transcoded into ...\na fixed ladder of 3 video representations at different resolutions and bitrates using the H264 codec, and\nan audio rendition with the AAC codec.\nEach individual stream is wrapped into fragmented MP4 (fMP4) containers. HLS and DASH manifests are generated so the encoded content is stream- and playable on the majority of modern devices and browsers out there today.\nStep 1: Add Bitmovin SDK to Your Project\nTo get started add the Bitmovin SDK to your project.\nFor the latest version please refer to the\nGitHub repository\nor check the\nchangelog\nof the Bitmovin API. Open API SDK versions and the Bitmovin API share the same versioning.\nShell\n// with yarn\nyarn add @bitmovin/api-sdk\n\n// with npm\nnpm install @bitmovin/api-sdk --save\nStep 2: Setup a Bitmovin API Client Instance\nThe Bitmovin API client instance is used for all communication with the Bitmovin REST API, pass your API key to authenticate with the API. You can find your\nAPI_KEY\nin the dashboard in your\nAccount Settings\n.\nJava\nconst { ConsoleLogger,} = require('@bitmovin/api-sdk');\nconst BitmovinApi = require('@bitmovin/api-sdk').default;\n\nconst bitmovinApi = new BitmovinApi({\n    apiKey: '<API_KEY>',\n    logger: new ConsoleLogger()\n});\nStep 3: Create an Input\nCreate Input\nAn input holds the configuration to access a specific file storage, from which the encoder will download the source file.\nWe support various types of input sources including HTTP(S), (S)FTP, Google Cloud Storage (GCS), Amazon Simple Storage Service (S3), Azure Blob Storage, and all cloud storage vendors that offer an S3 compatible interface.\nBelow we are creating an input. You can change the input type on the top right of the code panel.\nJavaScript\nconst input = await bitmovinApi.encoding.inputs.https.create(\n    new HttpsInput({\n        host: '<HTTPS_INPUT_HOST>',\n        name: '<INPUT_NAME>'\n    })\n);\nReuse Inputs\nInputs refer to a location, not a specific file.\nThis makes them easily reusable. Create an input once and reuse it for all your encodings.\nHint: All resources in our API are identified by a unique id which enables you to reuse many resources. So you only have to create one input for each location and can reuse it for all encodings.\nJavaScript\nconst input = await bitmovinApi.encoding.inputs.https.get(\n  '<INPUT_ID>'\n);\nStep 4: Create an Output\nCreate Output\nOutputs define where the manifests and encoded segments will be written to.\nLike inputs, outputs represent a set of information needed to access a specific storage. From this storage players can access the content for playback.\nJavaScript\nconst output = await bitmovinApi.encoding.outputs.gcs.create(\n    new GcsOutput({\n        name: '<GCS_OUTPUT_NAME>',\n        accessKey: '<GCS_ACCESS_KEY>',\n        secretKey: '<GCS_SECRET_KEY>',\n        bucketName: '<GCS_BUCKET_NAME>'\n    })\n);\nconst outputId = output.id;\nReuse Output\nSimilar to inputs, outputs also refer to a location, not a specific file.\nSo they're exactly as reusable as inputs. Create it once and reuse it for all your encodings.\nJavaScript\nconst output = await bitmovinApi.encoding.outputs.gcs.get(\n    '<OUTPUT_ID>'\n);\nStep 5: Create Codec Configurations\nCodec Configurations\nCodec configurations specify the codec and its parameters (eg. media codec, bitrate, frame rate, sampling rate).\nThey are used to encode the input stream coming from your input file.\nVideo Codec Configurations\nBelow we are creating video codec configurations. We support a multitude of codecs, including H264, H265, VP9, AV1 and many more.\nFor simplicity, we use H264 now, but you can easily change that after finishing the getting started.\nJavaScript\nconst videoCodecConfiguration1 = await bitmovinApi.encoding.configurations.video.h264.create(\n    new H264VideoConfiguration({\n        name: 'Getting Started H264 Codec Config 1',\n        bitrate: 1500000,\n        width: 1024,\n        presetConfiguration: PresetConfiguration.VOD_STANDARD\n    })\n);\n\nconst videoCodecConfiguration2 = await bitmovinApi.encoding.configurations.video.h264.create(\n    new H264VideoConfiguration({\n        name: 'Getting Started H264 Codec Config 2',\n        bitrate: 1000000,\n        width: 768,\n        presetConfiguration: PresetConfiguration.VOD_STANDARD\n    })\n);\n\nconst videoCodecConfiguration3 = await bitmovinApi.encoding.configurations.video.h264.create(\n    new H264VideoConfiguration({\n        name: 'Getting Started H264 Codec Config 3',\n        bitrate: 750000,\n        width: 640,\n        presetConfiguration: PresetConfiguration.VOD_STANDARD\n    })\n);\nAudio Codec Configurations\nAs audio codec configuration we are creating one AAC configuration.\nWe also support many more audio codecs including Opus, Vorbis, AC3, E-AC3 and MP2.\nJavaScript\nconst audioCodecConfiguration = await bitmovinApi.encoding.configurations.audio.aac.create(\n    new AacAudioConfiguration({\n        name: 'Getting Started Audio Codec Config',\n        bitrate: 128000\n    })\n);\nReusing Codec Configurations\nAs inputs and outputs, codec configurations can also be reused.\nTo use existing video or audio codec configurations, loading them with their id.\nJavaScript\nconst videoCodecConfiguration = await bitmovinApi.encoding.configurations.video.h264.get(\n  '<H264_CC_ID>'\n);\n\nconst audioCodecConfiguration = await bitmovinApi.encoding.configurations.audio.aac.get(\n  '<AAC_CC_ID>'\n);\nStep 6: Create & Configure an Encoding\nCreate Encoding\nAn encoding is a collection of resources (inputs, outputs, codec configurations etc.), mapped to each other.\nThe cloud region defines in which cloud and region your encoding will be started.\nIf you don't set it, our encoder takes the region closest to your input and output.\nInputs, outputs and codec configurations can be reused among many encodings. Other resources are specific to one encoding, like streams, which we'll create below.\nJavaScript\nconst encoding = await bitmovinApi.encoding.encodings.create(\n    new Encoding({\n        name: 'Getting Started Encoding',\n        cloudRegion: CloudRegion.GOOGLE_EUROPE_WEST_1\n    })\n);\nStreams\nA stream maps an audio or video input stream of your input file (called input stream) to one audio or video codec configuration.\nThe following example uses the selection mode\nAUTO\n, which will select the first available video input stream of your input file.\nYou can choose an input file from 3 predefined examples on the top right of the code panel.\nVideo Stream\nJavaScript\nconst inputPath = '<INPUT_PATH>';\n\nconst videoStreamInput = new StreamInput({\n  inputId: input.id,\n  inputPath: inputPath,\n  selectionMode: StreamSelectionMode.AUTO,\n});\n\nconst videoStream1 = await bitmovinApi.encoding.encodings.streams.create(\n  encoding.id,\n  new Stream({\n    codecConfigId: videoCodecConfiguration1.id,\n    inputStreams: [videoStreamInput],\n  }),\n);\n\nconst videoStream2 = await bitmovinApi.encoding.encodings.streams.create(\n  encoding.id,\n  new Stream({\n    codecConfigId: videoCodecConfiguration2.id,\n    inputStreams: [videoStreamInput],\n  }),\n);\n\nconst videoStream3 = await bitmovinApi.encoding.encodings.streams.create(\n  encoding.id,\n  new Stream({\n    codecConfigId: videoCodecConfiguration3.id,\n    inputStreams: [videoStreamInput],\n  }),\n);\nAudio Stream\nJavaScript\nconst audioStreamInput = new StreamInput({\n    inputId: input.id,\n    inputPath: inputPath,\n    selectionMode: StreamSelectionMode.AUTO\n});\n\nconst audioStream = await bitmovinApi.encoding.encodings.streams.create(\n    encoding.id,\n    new Stream({\n        codecConfigId: audioCodecConfiguration.id,\n        inputStreams: [audioStreamInput]\n    })\n);\nStep 7: Create a Muxing\nMuxings\nA muxing defines which container format will be used for the encoded video or audio files (segmented TS, progressive TS, MP4, FMP4, ...). It requires a stream, an output, and the output path, where the generated segments will be written to.\nFMP4 muxings are used for DASH manifest representations, TS muxings for HLS playlist representations.\nFMP4\nJavaScript\nconst aclEntry = new AclEntry({\n    permission: AclPermission.PUBLIC_READ\n});\n\nconst segmentLength = 4;\nconst outputPath = '<OUTPUT_PATH>';\nconst segmentNaming = 'seg_%number%.m4s';\nconst initSegmentName = 'init.mp4';\n\nconst videoMuxing1 = await bitmovinApi.encoding.encodings.muxings.fmp4.create(\n    encoding.id,\n    new Fmp4Muxing({\n        segmentLength: segmentLength,\n        segmentNaming: segmentNaming,\n        initSegmentName: initSegmentName,\n        streams: [new MuxingStream({streamId: videoStream1.id})],\n        outputs: [\n            new EncodingOutput({\n                outputId: outputId,\n                outputPath: outputPath + '/video/1024_1500000/fmp4/',\n                acl: [aclEntry]\n            })\n        ]\n    })\n);\n\nconst videoMuxing2 = await bitmovinApi.encoding.encodings.muxings.fmp4.create(\n    encoding.id,\n    new Fmp4Muxing({\n        segmentLength: segmentLength,\n        segmentNaming: segmentNaming,\n        initSegmentName: initSegmentName,\n        streams: [new MuxingStream({streamId: videoStream2.id})],\n        outputs: [\n            new EncodingOutput({\n                outputId: outputId,\n                outputPath: outputPath + '/video/768_1000000/fmp4/',\n                acl: [aclEntry]\n            })\n        ]\n    })\n);\n\nconst videoMuxing3 = await bitmovinApi.encoding.encodings.muxings.fmp4.create(\n    encoding.id,\n    new Fmp4Muxing({\n        segmentLength: segmentLength,\n        segmentNaming: segmentNaming,\n        initSegmentName: initSegmentName,\n        streams: [new MuxingStream({streamId: videoStream3.id})],\n        outputs: [\n            new EncodingOutput({\n                outputId: outputId,\n                outputPath: outputPath + '/video/640_750000/fmp4/',\n                acl: [aclEntry]\n            })\n        ]\n    })\n);\nAudio Muxings\nJavaScript\nconst audioMuxing = await bitmovinApi.encoding.encodings.muxings.fmp4.create(\n    encoding.id,\n    new Fmp4Muxing({\n        segmentLength: segmentLength,\n        segmentNaming: segmentNaming,\n        initSegmentName: initSegmentName,\n        streams: [new MuxingStream({streamId: audioStream.id})],\n        outputs: [\n            new EncodingOutput({\n                outputId: outputId,\n                outputPath: outputPath + '/audio/128000/fmp4/',\n                acl: [aclEntry]\n            })\n        ]\n    })\n);\nHint: To optimize your content for a specific use-case you can also provide a custom segment length. Further, you can also customize the naming pattern for the resulting segments and the initialization segment as well.\nStep 8: Create a Manifest\nDASH & HLS Manifests\nIts recommended to use Default Manifests, one each for DASH (\nAPI-Reference\n) and HLS (\nAPI-Reference\n). They are a construct that leaves it to the encoder to determine how to best generate the manifest based on what resources have been created by the encoding. This simplifies the code greatly for simple scenarios like this guide is about.\nCreate a DASH Manifest\nJavaScript\nconst manifestOutput = new EncodingOutput({\n    outputId: outputId,\n    outputPath: outputPath,\n    acl: [\n        new AclEntry({\n            permission: AclPermission.PUBLIC_READ,\n            scope: '*'\n        })\n    ]\n});\nlet dashManifest = new DashManifestDefault({\n    manifestName: 'stream.mpd',\n    encodingId: encoding.id,\n    version: DashManifestDefaultVersion.V2,\n    outputs: [manifestOutput]\n});\n\ndashManifest = await bitmovinApi.encoding.manifests.dash.default.create(dashManifest);\nCreate a HLS manifest\nJavaScript\nlet hlsManifest = new HlsManifestDefault({\n    manifestName: 'stream.m3u8',\n    encodingId: encoding.id,\n    version: HlsManifestDefaultVersion.V1,\n    outputs: [manifestOutput]\n});\n\nhlsManifest = await bitmovinApi.encoding.manifests.hls.default.create(hlsManifest);\nStep 9: Start an Encoding\nLet's recap - We have defined:\nan\ninput\n, specifying the input file host\nan\noutput\n, specifying where the manifest and the encoded files will be written to\ncodec configurations\n, specifying how the input file will be encoded\nstreams\n, specifying which input stream will be encoded using what codec configuration\nmuxings\n, specifying the containerization of the streams\nmanifests\n, to play the encoded content using the HLS or DASH streaming format.\nNow we can start the encoding with one API call, including additional configuration so the DASH and HLS Default-Manifests are created as part of the encoding process too:\nconst startEncodingRequest = new StartEncodingRequest({\n    manifestGenerator: ManifestGenerator.V2,\n    vodDashManifests: [\n        new ManifestResource({manifestId: dashManifest.id})\n    ],\n    vodHlsManifests: [\n        new ManifestResource({manifestId: hlsManifest.id})\n    ]\n});\n\nawait bitmovinApi.encoding.encodings.start(encoding.id, startEncodingRequest);\nWhat happens next:\nAfter the successful start of the encoding, it will change its state from\nCREATED\nto\nSTARTED\n, and shortly after that to\nQUEUED\n.\nRUNNING\nit will be in as soon as the input file starts to get downloaded and processed. Eventually\nFINISHED\nwill indicate a successful encoding and upload of the content to the provided Output destination. More details about each encoding status can be\nfound here\n.\nStep 10: Final Review\nCongratulations!\nYou successfully started your first encoding :) and this is only the beginning. The Bitmovin Encoding API is extremely flexible and allows many customizations and provides with access to the latest codecs and video streaming optimizations and technologies out there. Have a look at the\nAPI Reference\nto learn more.\nMore Examples\nYou can find many more fully functional code examples in our Github Repository:\nGithub",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/53b98d6-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/bitmovin-cdn-output",
    "title": "Bitmovin CDN Output",
    "text": "Overview\nThe Bitmovin Content Delivery Network (CDN) helps set up your workflows faster by simplifying your content distribution. It’s easy to use and removes the need to set up your own output storage. Your content will be automatically distributed around the globe, enabling fast and efficient delivery to all your viewers without any additional effort.\nRequirements\nSDK Version: 1.111.0 or higher\nContent-Type: VoD (1.111.0 or higher), LIVE (1.113.0 or higher)\nAvailable to Bitmovin Accounts created after April 5th, 2022 that have\nStarter or PAYG subscription, and\nUsers during their 30-Day Encoding Trial.\nSupported Features\ncustomized encoding workflows for VoD\nUsage in the Dashboard\nThe Bitmovin CDN Output is listed in the Outputs overview, where you find your existing Output resources as well. So, you can also use it as an Output destination in Step 6 when you\ncreate a new Encoding\nthrough the Dashboard:\nGo to\nhttps://bitmovin.com/dashboard/\nOn the left, open the \"Encoding\" section, and select \"Encodings\"\nIn the upper right corner of the Overview, click \"Create new encoding\"\nGo through each step and select your Input and all desired renditions and manifests for your encoding.\nIn step 5, select \"CDN Output\" in the Existing Output List\nIn step 6 of the Encoding Wizard, click \"Start Encoding\", then \"Go to Encoding\" once the encoding successfully started to open its encoding detail view.\nThere, select the “Manifests” tab. You will find the CDN URL to your encoded content there. Click the “Preview” button to view in the dashboard right away 🙂:\nℹ️\nThe domain name in this URL is unique to your account. It gets set up the first time you use the CDN as output for an encoding. So, it can take a couple of minutes for its DNS entry to become globally available. As soon as it is reachable, you will be able to play your content as expected.\nUsage with the Bitmovin API\nℹ️\nTake a look at our\nGetting Started Guide\nto learn how to use the Bitmovin API to create your first custom encoding workflow.\nUsing the Bitmovin API, you will have to look up the CDN Output in your account to obtain its ID first to use it in the\nEncodingOutput\nConfiguration Objets of your encoding workflow. The simplest way to do that is to use the “List CDN Outputs” endpoint:\nJava\nCdnOutput cdnOutput = bitmovinApi.encoding.outputs.cdn.list().getItems().get(0);\nor, you obtain its details by using its ID. You can look up its ID in the dashboard under “Encoding =>\nOutputs\n“. Click on the \"CDN\" Output there to open its detail view, copy its ID, and replace with it\nyourCdnOutputId\nin the code snippet below:\nJava\nCdnOutput cdnOutput = bitmovinApi.encoding.outputs.cdn.get(\"yourCdnOutputId\");\nOutput paths for CDN Outputs\nWhen you create an encoding you configure it as you do usually. Once an encoding is started, all\nEncodingOutput\nConfiguration Objects that refer to\nCDN Output\nwill be automatically adjusted to meet CDN Output file structure requirements:\nEvery\noutputPath\nof an\nEncodingOutput\nConfiguration Object will be pre-fixed with an\nAsset ID\nwhich is unique within an encoding. Doing so, it is ensured that all resources created by an encoding share the same root folder on your CDN Output.\nACL permissions are set to\nPRIVATE\nto ensure the content on the CDN storage is only accessible through its attached CDN Endpoint URL.\nObtain the CDN URL for your encoded contents\noutputPath\ns only contain relative URLs. By concatenating the\ndomainName\nof your CDN Output with those output paths you will obtain the URL to a particular resource on your CDN storage.\nℹ️\nThe\ndomainName\nof an CDN Output is only available, after it was used at least once by an successful encoding.\nThe following example show how you can programmatically obtain the CDN URL for an output of an Progressive MP4 Muxing:\nJava\n// retrieve updated CDN output details \nCdnOutput cdnOutput = bitmovinApi.encoding.outputs.cdn.get(cdnOutput.getId())\n\n// retrieve updated muxing details (https://bitmovin.com/docs/encoding/api-reference/all#/Encoding/GetEncodingEncodingsMuxingsByEncodingIdAndMuxingId)\nMp4Muxing retrievedMuxing =\n    bitmovinApi.encoding.encodings.muxings.mp4.get(encoding.getId(), muxing.getId());\n\n// find the EncodingOutput of the muxing that refers to the CdnOutput (in case there are more than one)\nEncodingOutput cdnEncodingOutput =\n    retrievedMuxing.getOutputs().stream()\n        .filter(encodingOutput -> encodingOutput.getOutputId().equals(cdnOutput.getId())).findFirst().get();\n\n// build the CDN URL\nString pathOnCdn = String.format(\"https://%s/%s%s\", cdnOutput.getDomainName(), cdnEncodingOutput.getOutputPath(), retrievedMuxing.getFilename());\nThe following example shows how you can programmatically obtain the CDN URL for a Manifest:\nJava\n// retrieve updated output details (domainName is only available after the first usage) (https://bitmovin.com/docs/encoding/api-reference/sections/outputs#/Encoding/GetEncodingOutputsCdnByOutputId)\nCdnOutput cdnOutput = bitmovinApi.encoding.outputs.cdn.get(cdnOutput.getId())\n\n// retrieve updated manifest details (https://bitmovin.com/docs/encoding/api-reference/sections/manifests#/Encoding/GetEncodingManifestsDashByManifestId)\nDashManifest retrievedManifest = \n    bitmovinApi.encoding.manifests.dash.get(manifest.getId());\n\n// find the EncodingOutput of the muxing that refers to the CdnOutput (in case there are more than one)\nString manifestOutputPath =\n    retrievedManifest.getOutputs().stream()\n        .filter(encodingOutput -> encodingOutput.getOutputId().equals(cdnOutput.getId()))\n        .findFirst()\n        .map(EncodingOutput::getOutputPath).get();\n\n// build the absolute path\nString pathOnCdn =\n    String.format(\n        \"https://%s/%s%s\",\n        cdnOutput.getDomainName(),\n        manifestOutputPathOptional.get(),\n        manifest.getManifestName());\nKnown Limitations\nWhen using the Bitmovin CDN Output for the first time, it can take a couple of minutes until the necessary DNS entries for your CDN are propagated. The storage for your CDN is immediately available.\nOnly\nautomatically generated manifests\nthat are created as part of the Encoding Start Request, can be used when using the Bitmovin CDN Output.\nLive Encodings don’t support the Bitmovin CDN Output at the moment.\nCDN Usage Statistics API\nIn order to retrieve the usage statistics of your CDN, a GET request can be made to the following endpoint\nhttps://api.bitmovin.com/v1/encoding/statistics/cdn/usage?from={time-stamp}&to={time-stamp}\n, where the\nfrom\nand\nto\nquery parameters determine the time period for which the usage statistics are retrieved.\nℹ️\nNote: both of the following date formats are accepted in the request:\nyyyy-MM-dd\nand\nyyyy-MM-ddThh:mm:ssZ\n. The maximum retrieval time period is of 100 days.\nShell\ncurl 'https://api.bitmovin.com/v1/encoding/statistics/cdn/usage?from=2022-04-12&to=2022-04-15' \\\n  --header 'x-api-key: YOUR_BITMOVIN_API_KEY' \\\n  --header 'Content-Type: application/json'\nThe request will return to the total amount of transfer usage and storage usage, as well as those usages by day:\nJSON\n{\n    \"from\": \"2022-04-12T00:00:00Z\",\n    \"to\": \"2022-04-15T00:00:00Z\",\n    \"transferUsageTotal\": 260.0,\n    \"storageUsageTotal\": 170.0,\n    \"usage\": \n    [\n        {\n            \"from\":\"2022-04-12T00:00:00Z\",\n            \"to\":\"2022-04-13T00:00:00Z\",\n            \"storageUsage\": 100.0,\n            \"transferUsage\": 120.0\n        }, \n        {\n            \"from\":\"2022-04-13T00:00:00Z\",\n            \"to\":\"2022-04-14T00:00:00Z\",\n            \"storageUsage\": 100.0,\n            \"transferUsage\": 0.0\n        }, \n        {\n            \"from\":\"2022-04-14T00:00:00Z\",\n            \"to\":\"2022-04-15T00:00:00Z\",\n            \"storageUsage\": 50.0,\n            \"transferUsage\": 50.0\n        }, \n        {\n            \"from\":\"2022-04-15T00:00:00Z\",\n            \"to\":\"2022-04-16T00:00:00Z\",\n            \"storageUsage\": 10.0,\n            \"transferUsage\": 0.0\n        }\n    ]\n}\nℹ️\nThe measure unit for storage is GB/month, conveying how many gigabytes per month are consumed for storage, while the measure unit for transfer is GB, which conveys the number of gigabytes transferred.\nDashboard\nIn the dashboard, the CDN usage statistics can be found under the billing statistics page. The data for the last 2 days has a latency, so it may be either incomplete or not available yet.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/158638d-image.png",
      "https://files.readme.io/179b684-image.png",
      "https://files.readme.io/93093e0-image.png",
      "https://files.readme.io/4c84ab8-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/c-sdk",
    "title": "C# SDK",
    "text": "The Bitmovin API enables you to build cutting edge video workflows while leveraging emmy-award winning encoding solutions, that automatically optimize your catalog for high quality and cost efficient video streaming.\nQuick Start Guide\nThis Quick Start Video will demonstrate different approaches to setup an encoding workflow using a\nWatch Folder Workflow (No Code Required)\nand a\nfull Python API SDK example\nusing this\nColabortory Notebook\n.\nundefined\nGet Started with your own Project\nThe following steps will walk you through adding the SDK to an new or existing Project step by step.\nOverview\nStarting an encoding triggers a request for encoding instances. Once available, the input file will be downloaded, divided into chunks, and distributed between all of them. Those instances transcode each chunk into segments for all configured video/audio representations (muxings) in parallel and write them to the designated output destination.\nThis approach makes Bitmovin's encoding the fastest in the world.\nIn this tutorial you will implement a simple VOD online streaming scenario, where an input file containing a video stream and stereo audio stream is transcoded into ...\na fixed ladder of 3 video representations at different resolutions and bitrates using the H264 codec, and\nan audio rendition with the AAC codec.\nEach individual stream is wrapped into fragmented MP4 (fMP4) containers. HLS and DASH manifests are generated so the encoded content is stream- and playable on the majority of modern devices and browsers out there today.\nStep 1: Add Bitmovin SDK to Your Project\nTo get started add the Bitmovin SDK to your project.\nFor the latest version please refer to the\nGitHub repository\nor check the\nchangelog\nof the Bitmovin API. Open API SDK versions and the Bitmovin API share the same versioning.\nShell\n//with Package Manager\nInstall-Package Bitmovin.Api.Sdk\n\n//with .NET CLI\ndotnet add package Bitmovin.Api.Sdk\nStep 2: Setup a Bitmovin API Client Instance\nThe Bitmovin API client instance is used for all communication with the Bitmovin REST API, pass your API key to authenticate with the API. You can find your\nAPI_KEY\nin the dashboard in your\nAccount Settings\n.\nC#\nusing Bitmovin.Api.Sdk;\nusing Bitmovin.Api.Sdk.Common.Logging;\n\nvar bitmovinApi = BitmovinApi.Builder\n    .WithApiKey(API_KEY)\n    .WithLogger(new ConsoleLogger())\n    .Build();\nStep 3: Create an Input\nCreate Input\nAn input holds the configuration to access a specific file storage, from which the encoder will download the source file.\nWe support various types of input sources including HTTP(S), (S)FTP, Google Cloud Storage (GCS), Amazon Simple Storage Service (S3), Azure Blob Storage, and all cloud storage vendors that offer an S3 compatible interface.\nBelow we are creating an input. You can change the input type on the top right of the code panel.\nC#\nvar input = await bitmovinApi.Encoding.Inputs.Https.CreateAsync(new HttpsInput()\n{\n    Name = \"<INPUT_NAME>\",\n    Host = \"<HTTPS_INPUT_HOST>\"\n});\nReuse Inputs\nInputs refer to a location, not a specific file.\nThis makes them easily reusable. Create an input once and reuse it for all your encodings.\nHint: All resources in our API are identified by a unique id which enables you to reuse many resources. So you only have to create one input for each location and can reuse it for all encodings.\nC#\nvar input = await bitmovinApi.Encoding.Inputs.Https.GetAsync(\"<INPUT_ID>\");\nStep 4: Create an Output\nCreate Output\nOutputs define where the manifests and encoded segments will be written to.\nLike inputs, outputs represent a set of information needed to access a specific storage. From this storage players can access the content for playback.\nC#\nvar output = await bitmovinApi.Encoding.Outputs.Gcs.CreateAsync(new GcsOutput\n{\n    AccessKey = \"<GCS_ACCESS_KEY>\",\n    SecretKey = \"<GCS_SECRET_KEY>\",\n    BucketName = \"<GCS_BUCKET_NAME>\",\n    Name = \"<GCS_OUTPUT_NAME>\"\n});\nstring outputId = output.Id;\nReuse Output\nSimilar to inputs, outputs also refer to a location, not a specific file.\nSo they're exactly as reusable as inputs. Create it once and reuse it for all your encodings.\nC#\nvar output = await bitmovinApi.Encoding.Outputs.Gcs.GetAsync(\"<OUTPUT_ID>\");\nStep 5: Create Codec Configurations\nCodec Configurations\nCodec configurations specify the codec and its parameters (eg. media codec, bitrate, frame rate, sampling rate).\nThey are used to encode the input stream coming from your input file.\nVideo Codec Configurations\nBelow we are creating video codec configurations. We support a multitude of codecs, including H264, H265, VP9, AV1 and many more.\nFor simplicity, we use H264 now, but you can easily change that after finishing the getting started.\nC#\nvar codecConfigVideoName1 = \"Getting Started H264 Codec Config 1\";\nvar codecConfigVideoBitrate1 = 1500000;\nvar videoCodecConfiguration1 = await bitmovinApi.Encoding.Configurations.Video.H264.CreateAsync(new H264VideoConfiguration\n{\n    Name = codecConfigVideoName1,\n    PresetConfiguration = PresetConfiguration.VOD_STANDARD,\n    Width = 1024,\n    Bitrate = codecConfigVideoBitrate1,\n    Description = codecConfigVideoName1 + \"_\" + codecConfigVideoBitrate1\n});\n\nvar codecConfigVideoName2 = \"Getting Started H264 Codec Config 2\";\nvar codecConfigVideoBitrate2 = 1000000;\nvar videoCodecConfiguration2 = await bitmovinApi.Encoding.Configurations.Video.H264.CreateAsync(new H264VideoConfiguration\n{\n    Name = codecConfigVideoName2,\n    PresetConfiguration = PresetConfiguration.VOD_STANDARD,\n    Width = 768,\n    Bitrate = codecConfigVideoBitrate2,\n    Description = codecConfigVideoName2 + \"_\" + codecConfigVideoBitrate2\n});\n\nvar codecConfigVideoName3 = \"Getting Started H264 Codec Config 3\";\nvar codecConfigVideoBitrate3 = 750000;\nvar videoCodecConfiguration3 = await bitmovinApi.Encoding.Configurations.Video.H264.CreateAsync(new H264VideoConfiguration\n{\n    Name = codecConfigVideoName3,\n    PresetConfiguration = PresetConfiguration.VOD_STANDARD,\n    Width = 640,\n    Bitrate = codecConfigVideoBitrate3,\n    Description = codecConfigVideoName3 + \"_\" + codecConfigVideoBitrate3\n});\nAudio Codec Configurations\nAs audio codec configuration we are creating one AAC configuration.\nWe also support many more audio codecs including Opus, Vorbis, AC3, E-AC3 and MP2.\nC#\nvar codecConfigAudio = await bitmovinApi.Encoding.Configurations.Audio.Aac.CreateAsync(new AacAudioConfiguration\n{\n    Name = \"Getting Started Audio Codec Config\",\n    Bitrate = 128000\n});\nReusing Codec Configurations\nAs inputs and outputs, codec configurations can also be reused.\nTo use existing video or audio codec configurations, loading them with their id.\nC#\nvar codecConfigVideo = await bitmovinApi.Encoding.Configurations.Video.H264.GetAsync(\"<H264_CC_ID>\");\nvar codecConfigAudio = await bitmovinApi.Encoding.Configurations.Audio.Aac.GetAsync(\"<AAC_CC_ID>\");\nStep 6: Create & Configure an Encoding\nCreate Encoding\nAn encoding is a collection of resources (inputs, outputs, codec configurations etc.), mapped to each other.\nThe cloud region defines in which cloud and region your encoding will be started.\nIf you don't set it, our encoder takes the region closest to your input and output.\nInputs, outputs and codec configurations can be reused among many encodings. Other resources are specific to one encoding, like streams, which we'll create below.\nC#\nvar encoding = await bitmovinApi.Encoding.Encodings.CreateAsync(new Encoding\n{\n    Name = \"Getting Started Encoding\",\n    CloudRegion = CloudRegion.GOOGLE_EUROPE_WEST_1\n});\nStreams\nA stream maps an audio or video input stream of your input file (called input stream) to one audio or video codec configuration.\nThe following example uses the selection mode\nAUTO\n, which will select the first available video input stream of your input file.\nYou can choose an input file from 3 predefined examples on the top right of the code panel.\nVideo Stream\nC#\nstring inputPath = \"<INPUT_PATH>\";\n\nvar videoStreamInput = new StreamInput\n{\n  InputId = input.Id,\n  InputPath = inputPath,\n  SelectionMode = StreamSelectionMode.AUTO\n};\n\nvar videoStream1 = await bitmovinApi.Encoding.Encodings.Streams.CreateAsync(encoding.Id, new Stream\n{\n  InputStreams = new List<StreamInput>\n  {\n    videoStreamInput\n  },\n  CodecConfigId = videoCodecConfiguration1.Id\n});\n\nvar videoStream2 = await bitmovinApi.Encoding.Encodings.Streams.CreateAsync(encoding.Id, new Stream\n{\n  InputStreams = new List<StreamInput>\n  {\n    videoStreamInput\n  },\n  CodecConfigId = videoCodecConfiguration2.Id\n});\n\nvar videoStream3 = await bitmovinApi.Encoding.Encodings.Streams.CreateAsync(encoding.Id, new Stream\n{\n  InputStreams = new List<StreamInput>\n  {\n    videoStreamInput\n  },\n  CodecConfigId = videoCodecConfiguration3.Id\n});\nAudio Stream\nC#\nvar inputStreamAudio = await bitmovinApi.Encoding.Encodings.Streams.CreateAsync(encoding.Id, new Stream\n{\n    InputStreams = new List<StreamInput>\n    {\n        new StreamInput\n        {\n            InputId = input.Id,\n            InputPath = inputPath,\n            SelectionMode = StreamSelectionMode.AUTO\n        }\n    },\n    CodecConfigId = codecConfigAudio.Id\n});\nStep 7: Create a Muxing\nMuxings\nA muxing defines which container format will be used for the encoded video or audio files (segmented TS, progressive TS, MP4, FMP4, ...). It requires a stream, an output, and the output path, where the generated segments will be written to.\nFMP4 muxings are used for DASH manifest representations, TS muxings for HLS playlist representations.\nFMP4\nC#\nvar aclEntry = new AclEntry { Permission = AclPermission.PUBLIC_READ };\n\ndouble segmentLength = 4;\nstring outputPath = \"<OUTPUT_PATH>\";\nstring initSegmentName = \"init.mp4\";\nstring segmentNaming = \"seg_%number%.m4s\";\n\n\nvar videoMuxingOutput1 = new EncodingOutput\n{\n    Acl = new List<AclEntry> { aclEntry },\n    OutputId = outputId,\n    OutputPath = outputPath + \"/video/1024_1500000/fmp4\"\n};\n\nvar videoMuxing1 = new Fmp4Muxing\n{\n    Outputs = new List<EncodingOutput> { videoMuxingOutput1 },\n    Streams = new List<MuxingStream> { new MuxingStream { StreamId = videoStream1.Id } },\n    SegmentLength = segmentLength,\n    InitSegmentName = initSegmentName,\n    SegmentNaming = segmentNaming\n};\nvideoMuxing1 = await bitmovinApi.Encoding.Encodings.Muxings.Fmp4.CreateAsync(encoding.Id, videoMuxing1);\n\nvar videoMuxingOutput2 = new EncodingOutput\n{\n    Acl = new List<AclEntry> { aclEntry },\n    OutputId = outputId,\n    OutputPath = outputPath + \"/video/768_1000000/fmp4\"\n};\n\nvar videoMuxing2 = new Fmp4Muxing\n{\n    Outputs = new List<EncodingOutput> { videoMuxingOutput2 },\n    Streams = new List<MuxingStream> { new MuxingStream { StreamId = videoStream2.Id } },\n    SegmentLength = segmentLength,\n    InitSegmentName = initSegmentName,\n    SegmentNaming = segmentNaming\n};\nvideoMuxing2 = await bitmovinApi.Encoding.Encodings.Muxings.Fmp4.CreateAsync(encoding.Id, videoMuxing2);\n\nvar videoMuxingOutput3 = new EncodingOutput\n{\n    Acl = new List<AclEntry> { aclEntry },\n    OutputId = outputId,\n    OutputPath = outputPath + \"/video/640_750000/fmp4\"\n};\n\nvar videoMuxing3 = new Fmp4Muxing\n{\n    Outputs = new List<EncodingOutput> { videoMuxingOutput3 },\n    Streams = new List<MuxingStream> { new MuxingStream { StreamId = videoStream3.Id } },\n    SegmentLength = segmentLength,\n    InitSegmentName = initSegmentName,\n    SegmentNaming = segmentNaming\n};\nvideoMuxing3 = await bitmovinApi.Encoding.Encodings.Muxings.Fmp4.CreateAsync(encoding.Id, videoMuxing3);\nAudio Muxings\nC#\nvar audioMuxingOutput = new EncodingOutput\n{\n    Acl = new List<AclEntry> { aclEntry },\n    OutputId = outputId,\n    OutputPath = outputPath + \"/audio/128000/fmp4\"\n};\n\nvar audioMuxing = new Fmp4Muxing\n{\n    Outputs = new List<EncodingOutput> { audioMuxingOutput },\n    Streams = new List<MuxingStream> { new MuxingStream { StreamId = inputStreamAudio.Id } },\n    SegmentLength = segmentLength,\n    InitSegmentName = initSegmentName,\n    SegmentNaming = segmentNaming\n};\naudioMuxing = await bitmovinApi.Encoding.Encodings.Muxings.Fmp4.CreateAsync(encoding.Id, audioMuxing);\nHint: To optimize your content for a specific use-case you can also provide a custom segment length. Further, you can also customize the naming pattern for the resulting segments and the initialization segment as well.\nStep 8: Create a Manifest\nDASH & HLS Manifests\nIts recommended to use Default Manifests, one each for DASH (\nAPI-Reference\n) and HLS (\nAPI-Reference\n). They are a construct that leaves it to the encoder to determine how to best generate the manifest based on what resources have been created by the encoding. This simplifies the code greatly for simple scenarios like this guide is about.\nCreate a DASH Manifest\nC#\nvar encodingOutput = new EncodingOutput()\n{\n    OutputPath = outputPath,\n    OutputId = outputId,\n    Acl = new List<AclEntry>() { new AclEntry()\n      {\n          Permission = AclPermission.PUBLIC_READ\n      }\n    }\n};\n\nvar dashManifest = new DashManifestDefault()\n{\n    EncodingId = encoding.Id,\n    ManifestName = \"stream.mpd\",\n    Version = DashManifestDefaultVersion.V1,\n    Outputs = new List<EncodingOutput>() { encodingOutput }\n};\n\nvar dashDefaultManifest = await bitmovinApi.Encoding.Manifests.Dash.Default.CreateAsync(dashManifest);\nCreate a HLS manifest\nC#\nvar hlsManifest = new HlsManifestDefault()\n{\n    EncodingId = encoding.Id,\n    Name = \"stream.m3u8\",\n    Version = HlsManifestDefaultVersion.V1,\n    Outputs = new List<EncodingOutput>() { encodingOutput }\n};\n\nvar hlsDefaultManifest = await bitmovinApi.Encoding.Manifests.Hls.Default.CreateAsync(hlsManifest);\nStep 9: Start an Encoding\nLet's recap - We have defined:\nan\ninput\n, specifying the input file host\nan\noutput\n, specifying where the manifest and the encoded files will be written to\ncodec configurations\n, specifying how the input file will be encoded\nstreams\n, specifying which input stream will be encoded using what codec configuration\nmuxings\n, specifying the containerization of the streams\nmanifests\n, to play the encoded content using the HLS or DASH streaming format.\nNow we can start the encoding with one API call, including additional configuration so the DASH and HLS Default-Manifests are created as part of the encoding process too:\nC#\nvar startEncodingRequest = new StartEncodingRequest()\n{\n    ManifestGenerator = ManifestGenerator.V2,\n    VodDashManifests = new List<ManifestResource>() { new ManifestResource() {\n         ManifestId = dashDefaultManifest.Id\n      }\n    },\n    VodHlsManifests = new List<ManifestResource>() { new ManifestResource() {\n         ManifestId = hlsDefaultManifest.Id\n      }\n    }\n};\nawait bitmovinApi.Encoding.Encodings.StartAsync(encoding.Id, startEncodingRequest);\nWhat happens next:\nAfter the successful start of the encoding, it will change its state from\nCREATED\nto\nSTARTED\n, and shortly after that to\nQUEUED\n.\nRUNNING\nit will be in as soon as the input file starts to get downloaded and processed. Eventually\nFINISHED\nwill indicate a successful encoding and upload of the content to the provided Output destination. More details about each encoding status can be\nfound here\n.\nStep 10: Final Review\nCongratulations!\nYou successfully started your first encoding :) and this is only the beginning. The Bitmovin Encoding API is extremely flexible and allows many customizations and provides with access to the latest codecs and video streaming optimizations and technologies out there. Have a look at the\nAPI Reference\nto learn more.\nMore Examples\nYou can find many more fully functional code examples in our Github Repository:\nGitHub",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/494de3f-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/sdks",
    "title": "SDKs",
    "text": "Github\nAll our SDKs are also open-source. Take a look at their Github repositories.\nPHP\nC#\nPython\nNode.js / JavaScript\nJava\nGo\nExamples\nWe provide examples for many different use-cases, for all our SDKs.\nPHP\nC#\nPython\nNode.js / JavaScript\nJava\nGo",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-encoding-with-hls-scte-35-and-ssai",
    "title": "Live Encoding with HLS, SCTE-35 and SSAI",
    "text": "Overview\nThis tutorial details how Bitmovin's Live Encoder supports SCTE-35. It explains how splice decisions are made and what options can be configured. There is a full Java code example for starting a Live Encoding with SCTE-35 tags in a live HLS manifest. Additionally, there are examples of how to integrate with Server Side Ad Insertion (SSAI) services.\nRequirements\nEncoder Version:\nv2.186.0\nor higher\nBitmovin Encoding API SDK Version:\nv1.183.2\nor higher\nInput Source\nExample in this tutorial:\nsrt-live-transmit\n,\npv (pipe viewer)\nSSAI Service\nExample in this tutorial:\nAWS Mediatailor\nAd decision server\nExample in this tutorial:\nsimple VAST response XML\nSCTE-35\nFor a comprehensive overview of the The\nS\nociety of\nC\nable and\nT\nelecommunications\nE\nngineers and the SCTE-35 standard, you can read our blog post\nThe Essential Guide to SCTE-35\nWhat is SCTE-35\nSCTE-35 signals are used to identify national and local ad breaks as well as program content like intro/outro credits, chapters, blackouts, and extensions when a live program like a sporting event runs long. For modern streaming applications, they are usually included within an MPEG-2 transport stream PID and then converted into metadata that is embedded in HLS and MPEG-DASH manifests.\nSCTE-35 markers and their applications for streaming video\nWhile SCTE-35 markers are primarily used for ad insertion in OTT workflows, they can also signal many other events that allow an automation system to tailor the program output for compliance with local restrictions or to improve the viewing experience. Let’s take a look at some common use cases and benefits of using SCTE-35 markers.\nAd Insertion\nAs mentioned above, inserting advertisements into a video stream is the main use case for SCTE-35 markers. They provide seamless splice points for national, local and individually targeted dynamic ad replacement. This allows for increased monetization opportunities for broadcasters and content providers by enabling segmenting of viewers into specific demographics and geographic locations. When ad content can be tailored for a particular audience, advertisers are willing to pay more, leading to higher revenue for content providers and distributors.\nProgram boundary markers\nAnother common use case is to signal a variety of program boundaries. This includes the start and end of programs, chapters, ad breaks and unexpected interruptions or extensions. Many of these become particularly useful in Live-to-VOD scenarios. Ad break start/end markers can be used as edit points in a post-production workflow to automate the removal of ads for viewers with ad-free subscriptions. A program end marker can be used to trigger the next episode in a series for binge viewing sessions if so desired. All of these markers open new possibilities for improving the user experience and keeping your audience happy and engaged.\nBlackouts and alternate content\nAnother less common, but important use case is to signal blackouts, when a piece of content should be replaced or omitted from a broadcast. This often applies to regional blackouts for sporting events. Respecting blackout restrictions is crucial for avoiding fines and loss of access to future events. Using SCTE-35 allows your automation system to take control and ensure you are compliant.\nBitmovin and SCTE-35\nBitmovin supports the parsing of SCTE-35 triggers from MPEG-TS input streams for Live Encodings, the triggers are shown below as splice decisions. The triggers are then mapped to HLS manifest tags.\nSplice Decisions\nCertain SCTE-35 triggers signal that an advertisement or break (to  from the original content starts or ends. The following table describes how the Live Encoder treats SCTE-35 trigger types and SCTE-35 Segmentation Descriptor types as splice decision points, and the compatibility of those types with the different command types, Spice Insert and Time Signal.\nSegmentation UPID Type(Start/End)\nDescriptor Type Name\nSPLICE_INSERT\nTIME_SIGNAL\n-\n✓\n✖\n0x10, 0x11\nPROGRAM\n✓\n✖\n0x20, 0x21\nCHAPTER\n✓\n✖\n0x22, 0x23\nBREAK\n✓\n✓\n0x30, 0x31\nPROVIDER_ADVERTISEMENT\n✓\n✓\n0x32, 0x33\nDISTRIBUTOR_ADVERTISEMENT\n✓\n✓\n0x34, 0x35\nPROVIDER_PLACEMENT_OPPORTUNITY\n✓\n✓\n0x36, 0x37\nDISTRIBUTOR_PLACEMENT_OPPORTUNITY\n✓\n✓\n0x40, 0x41\nUNSCHEDULED_EVENT\n✓\n✖\n0x42, 0x43\nALTERNATE_CONTENT_OPPORTUNITY\n✓\n✖\n0x44, 0x45\nPROVIDER_AD_BLOCK\n✓\n✖\n0x46, 0x47\nDISTRIBUTOR_AD_BLOCK\n✓\n✖\n0x50, 0x51\nNETWORK\n✓\n✖\nNote:\nWhen the SCTE-35 trigger has the\nauto-return\nflag enabled for a cue-out indication, a cue-in marker will be automatically inserted after the specified\nbreak-duration\n. If a cue-in trigger with the corresponding event ID is received between the cue-out trigger and the automatically generated cue-in, it will override the automatic cue-in insertion.\nStarting with Encoder Version\nv2.231.0\n, this behavior becomes the default whenever a cue-out command includes a configured\nbreak-duration\n. The\nauto-return\nflag is no longer required.\nHLS - Ad Marker Settings\nThe Live Encoder supports a range of different HLS tags that are written when SCTE-35 triggers are parsed from the MPEG-TS input stream. Multiple marker types can be enabled for each HLS manifest. Which marker types to use depends on the consumer of the HLS manifest. An example consumer would be a Server Side Ad Insertion (SSAI) service. They usually state in their documentation which HLS tags they support for signaling SCTE-35 triggers.\nEXT_X_CUE_OUT_IN\n: Ad markers will be inserted using\n#EXT-X-CUE-OUT\nand\n#EXT-X-CUE-IN\ntags.\nEXT_OATCLS_SCTE35\n: Ad markers will be inserted using\n#EXT-OATCLS-SCTE35\ntags. They contain the base64 encoded raw bytes of the original SCTE-35 trigger.\nEXT_X_SPLICEPOINT_SCTE35\n: Ad markers will be inserted using\n#EXT-X-SPLICEPOINT-SCTE35\ntags. They contain the base64 encoded raw bytes of the original SCTE-35 trigger.\nEXT_X_SCTE35\n: Ad markers will be inserted using\n#EXT-X-SCTE35\ntags. They contain the base64 encoded raw bytes of the original SCTE-35 trigger. This type requires Encoder Version:\nv2.188.0\nor higher and Bitmovin Encoding API SDK Version:\nv1.183.2\nor higher.\nEXT_X_DATERANGE\n: Ad markers will be inserted using\n#EXT-X-DATERANGE\ntags as specified in the\nHLS specification\n. They contain the ID, start timestamp, and hex-encoded raw bytes of the original SCTE-35 trigger. (\nNote:\nThis type requires Encoder Version:\nv2.188.0\nor higher and Bitmovin Encoding API SDK Version:\nv1.183.2\nor higher.\nTo enable HLS ad markers for the Live Encoder, they need to be configured for the\nLive Encoding start call\n. For an object of the\nhlsManifests\nthe\nadMarkerSettings\ncan be added. A code example can be found in the section Live Encoder Configuration below.\nInput Preparation\nThe Live Encoder supports parsing of SCTE-35 triggers that are present in an MPEG-TS input stream. This stream can be ingested using SRT or Zixi. The incoming MPEG-TS program must have a SCTE-35 data stream with the MPEG-TS Packetized Elementary Stream (PES) type 0x86.\nffprobe\ncan be used to verify if the MPEG-TS input contains an SCTE-35 stream like this:\nffprobe srt://<source-ip>:2088\nor\nffprobe livestream.ts\nThe following stream should show up in the command line output:\nData: scte_35\nEven though the stream is present, it could still be the case that there are no SCTE-35 triggers present or being sent. This depends on the MPEG-TS source provider and the insertion of SCTE-35 triggers might be triggered manually.\nExample ingest using srt-live-transmit\nThe Live Encoder currently supports a live MPEG-TS ingest stream via SRT or Zixi input protocols. If you already have access to an SRT or Zixi source with a SCTE-35 stream, it can be used for this example. Alternatively, the\nsrt-live-transmit\ntool in combination with\npv (pipe viewer)\ncan be used in combination with an MPEG-TS file, to ingest the file to the Live Encoder via SRT in real-time. These tools are available for Mac OS and most Linux distributions.\nYou need to following information for the command pipeline:\nPath of the local MPEG-TS file:\n<mpeg-ts-input-file>\nThe overall bitrate of the file (in kiloBytes/s):\n<bitrate>\nThe SRT ingest URI:\n<srt-ingest-uri>\nThe command:\ncat <mpeg-ts-input-file> | pv -L <bitrate> | srt-live-transmit file://con <srt-ingest-uri>\nSample ingest with a demo file\nThis example uses a demo MPEG-TS file with SPLICE-INSERT SCTE-35 triggers. The triggers signal a 60-second ad break every two minutes. The overall bitrate of the file is determined using\nffprobe\n. To reproduce this on Ubuntu, follow these commands:\nInstall srt-live-transmit and pv:\nsudo apt-get update && sudo apt-get install -y pv srt-tools\nDownload the demo file:\nhttps://storage.googleapis.com/bitmovin-content-cdn-origin/content/assets/scte35/scte35_spliceInsert_2hour_demo.ts\nStart the Live Encoder with SRT Input with\nSrtMode.LISTENER\n(more details in the Live Encoder Configuration section) and wait for the Live Encoder to get into the state\nWAITING_FOR_FIRST_CONNECT\n. Obtain the\n<srt-ingest-uri>\nfrom the Bitmovin Dashboard or the Bimtovin API. (e.g.\nsrt://341.140.55.228:2088\n)\nExecute the command pipeline:\ncat scte35_spliceInsert_2hour_demo.ts | pv -L 19K | srt-live-transmit file://con srt://341.140.55.228:2088\nIf you are using your own MPEG-TS file for this test, make sure to update the\npv -L xxxK\npart with the kiloBytes/s rate of your MPEG-TS file. You can use\nffprobe\nto get the kiloBit/s from the file and then divide it by 8. This method works best with assets that have a constant bitrate.\nLive Encoder Configuration\nA full Java code example can be found here:\nhttps://github.com/bitmovin/bitmovin-api-sdk-examples/blob/main/java/src/main/java/tutorials/Scte35HlsLiveEncoding.java\nTo set up SCTE-35 support for your Live Encoder, it's essential to activate the relevant marker types within your\nLiveHlsManifest\nconfiguration. For instance, incorporating both markers involves the following steps:\nJava\nLiveHlsManifest liveHlsManifest = new LiveHlsManifest();\nHlsManifestAdMarkerSettings scteMarkerSettings = new HlsManifestAdMarkerSettings();\nscteMarkerSettings.addEnabledMarkerTypesItem(HlsManifestAdMarkerType.EXT_X_CUE_OUT_IN);\nscteMarkerSettings.addEnabledMarkerTypesItem(HlsManifestAdMarkerType.EXT_X_SPLICEPOINT_SCTE35);\nliveHlsManifest.setAdMarkerSettings(scteMarkerSettings);\nOnce you enable at least one marker type, the SCTE35 pipeline becomes active.\nHint:\nThe forthcoming marker type,\n#EXT-X-DATERANGE\n, will necessitate\nProgramDateTime\nactivation and configuration. This is crucial for synchronization with a universal time source. To configure it using our local system clock as the reference, utilize the following snippet:\nJava\nvar pdts = new ProgramDateTimeSettings();\npdts.setProgramDateTimeSource(ProgramDateTimeSource.SYSTEM_CLOCK);\nliveHlsManifest.setInsertProgramDateTime(true);\nliveHlsManifest.setProgramDateTimeSettings(pdts);\nMost SSAI services only support\nTsMuxing\n. Make sure your Live Encoding uses this for the live HLS manifest.\nVerification in the dashboard\nThe following methods show how to check the\nBitmovin Dashboard\nif the SCTE-35 processing is working correctly.\nDashboard Encoding Log\nTo verify if SCTE-35 triggers have been parsed from the input stream and converted into HLS tags can be seen in the Encoding Log section of the Live Encoding dashboard.\nExample of a processed SCTE-35 trigger in the Bitmovin Dashboard\nHint:\nThe Base64 or Hex-encoded SCTE-35 triggers can be parsed and inspected by an online tool like\nVideo Tools\n.\nDashboard Input Monitoring\nIn the “Input Monitoring” tab of the Live Encoding dashboard, there is a graph for “Bits SCTE35” and “Samples SCTE35”. Use this graph to verify the presence of an SCTE-35 stream in the ingested MPEG-TS stream and if samples have been processed.\nExample of the Bits per second graph for a SCTE-35 stream in the Input tab of a running live encoding, in the Bitmovin Dashboard\nVerification in the HLS manifest\nOne way to verify if the SCTE-35 parsing is working is to check the HLS variant playlist’s content.\nE.g.: The content of\nvideo_0.m3u8\n:\nThis example signals the end of an ad segment.\nM3U8\n#EXTM3U\n#EXT-X-VERSION:5\n#EXT-X-MEDIA-SEQUENCE:356\n#EXT-X-TARGETDURATION:4\n#EXT-X-PROGRAM-DATE-TIME:2024-01-15T14:44:35.130+0000\n#EXTINF:4.0,\nvideo/1080p/segment_356.ts\n#EXTINF:4.0,\nvideo/1080p/segment_357.ts\n#EXT-X-SPLICEPOINT-SCTE35:/DAqAAAAA3XwAP/wDwUAAABvf0/+B7mKAAABEv8ACgAIQ1VFSQAAABJdRndD\n#EXT-X-CUE-IN\n#EXTINF:4.0,\nvideo/1080p/segment_358.ts\n#EXTINF:4.0,\nvideo/1080p/segment_359.ts\n#EXT-X-ENDLIST\n#EXT-X-SPLICEPOINT-SCTE35:/DAqAAAAA3XwAP/wDwUAAABvf0/+B7mKAAABEv8ACgAIQ1VFSQAAABJdRndD\nis inserted by using the\nEXT_X_SPLICEPOINT_SCTE35\nHlsManifestAdMarkerType.\n#EXT-X-CUE-IN\nis inserted by using the\nEXT_X_CUE_OUT_IN\nHlsManifestAdMarkerType.\nSupported SSAI (Server Side Ad Insertion) services\nThe following table maps the Bitmovin API HlsManifestAdMarkerTypes to SSAI services.\n✓ - tested and supported\nO - not compatible\nempty - unknown\nTags\nAWS Elemental MediaTailor\nGoogle DAI\nYoSpace\nbroadpeak.io\nEXT_X_CUE_OUT_IN\n✓\n✓\n✓\nEXT_X_SCTE35\n✓\nEXT_OATCLS_SCTE35\nO\nEXT_X_SPLICEPOINT_SCTE35\n✓\nO\nEXT_X_DATERANGE\n✓\n✓\n✓\nHow to guides\nAWS MediaTailor\nLive Encoding with AWS Elemental MediaTailor\nLive Encoding with broadpeak.io\nLive Encoding with YoSpace\nTroubleshooting\nIf there are no HLS tags in the manifest or no ads are being inserted by the SSAI there are several parameters you can look into.\nWrong Muxing Type\nMake sure to set the correct muxing type for the Live Encoder. This depends on the consumer of the output HLS manifest, e.g. an SSAI service. The most common muxing types are\nTsMuxing\nand\nFmp4Muxing\n.\nNo or wrong SCTE-35 stream in MPEG-TS\nThe Live Encoder supports parsing of SCTE-35 triggers that are present in an MPEG-TS input stream. The incoming MPEG-TS program must have a SCTE-35 data stream with the MPEG-TS Packetized Elementary Stream (PES) type 0x86. Make sure that the stream is present by checking the MPEG-TS stream with\nffprobe\n. The Live Monitoring in the\nLive Encoder Dashboard\ncan also be used to check if the SCTE-35 stream is present.\nNo SCTE-35 triggers can be parsed\nThe SCTE-35 stream is present, but there are no SCTE-35 triggers being parsed (no SCTE-35 events in the Encoding Log). The Live Monitoring graph in the\nLive Encoder Dashboard\nshows if there are SCTE-35 samples read from the input. If no samples are showing up for the graph this means that there were no SCTE-35 triggers found in the input stream yet. Sometimes manual triggers are needed for an MPEG-TS source to insert SCTE-35 events. If SCTE-35 samples are showing in the graph but no SCTE-35 events are logged, please contact Bitmovin Support.\nNo Ads are shown in the SSAI’s output manifest\nMake sure that the configured HLS ad marker tags are present in the Live Encoder output HLS manifest. Which HLS ad markers are supported by the SSAI varies. SSAI can also have different support of SCTE-35 triggers that are carried in the HLS tags as Base64 or Hex format. Consult the SSAI service’s documentation. Try out different supported HLS ad marker types.\nThe issue with ingesting SRT/MPEG-TS using ffmpeg\nFFmpeg does not support SCTE-35 streams. When transmuxing an SCTE-35 stream with ffmpeg it is converted to a binary data (\nbin_data\n) stream. The Live Encoder does not support SCTE-35 triggers that are signaled using a\nbin_data\nstream. Alternatively, use the workflow from the section “Sample ingest with a demo file” to stream an MPEG-TS file via SRT to the Live Encoder.\nUseful tools and resources\nThis section provides links to tools and other resources that help with working with SCTE-35.\nSSAI (Server-Side Ad Insertion): What Is It?\n- Blogpost about SSAI\nVideo Tools\n- Base64, Hex parser for SCTE-35 trigger\nMiddleman Software, Inc. - Tools - SCTE 35 Parser\n- Base64, Hex parser for SCTE-35 trigger\nscte35-threefive\n- SCTE-35 parser, encoder and decoder tools and libraries",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/95b142b-SCTE_Ad_Decision_Markers.png",
      "https://files.readme.io/3cf752b-SCTE_Blackouts_and_Programme_markers.webp",
      "https://files.readme.io/afbf93c-SCTE-35_HLS_tags_Bitmovin_Dashboard.png",
      "https://files.readme.io/e1c124c-SCTE-35_Live_Monitoring_Bitmovin_Dashboard.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/changing-your-login-credentials",
    "title": "Changing Your Login Credentials",
    "text": "Forgot your password?\nYou can reset your password\nhere\nin case you forgot it.\nChanging your login email\nIf you want to change your login email, please\ncontact our support team\nvia\n[email protected]\nor via the chat which you can find in the bottom right corner in your\nBitmovin Dashboard\nand\nlet us know which email address\nyou'd like to use going forward.\nYou've signed in via GitHub, Google, Slack or LinkedIn? (SSO)\nIf you have used one of our SSO options to sign up for an account and want to change your password you can do this by entering your password of the used SSO account and defining a new one.\nYou can now manually log in with the newly defined password and your email or still use the SSO option to get access to our dashboard.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/required-permissions-for-s3-buckets-for-encoding-input-and-output",
    "title": "Required Permissions for S3 Buckets for Encoding Input and Output",
    "text": "An encoding uses Input and Output resources to define where to get source files from and where to write result files. When you use S3 bucket, you need to provide the IAM user accessing the bucket a set of permissions.\nLearn more in these tutorials about creating AWS S3 Inputs/Outputs using\nAccess/Secret keypairs\nor\nrole-based authentication\n.\nFull Access\nIf you want a quick solution, for example for quick evaluations or development environments, you can simply allocate the\nAmazonS3FullAccess\npolicy will give the IAM user unrestricted access to your bucket.\nRestricted Access\nFor most applications, you will want to tighten permissions to the strict set required. With AWS IAM, you have granular control to create a custom policy that only defines certain permissions.\nThe minimum set required (and why each permission is needed) is listed below:\nFor Input buckets\nAction\nResource Level\nJustification\ns3:GetBucketLocation\nBucket\nTo determine the location of the input bucket to resolve the correct encoding region for mode AUTO.\ns3:GetObject\nObject\nTo read the file from the S3 bucket\nFor Output buckets\nAction\nResource Level\nJustification\ns3:GetBucketLocation\nBucket\nTo determine the location of the output bucket to resolve the correct region for mode\nAUTO\ns3:ListBucket\nBucket\nTo verify if all files are present at the output location (i.e., check if all uploaded segments are present)\ns3:PutObject\nObject\nTo write the file to the S3 Bucket\ns3:PutObjectAcl\n(1)\nObject\nTo update the ACL for an object on a S3 Bucket (i.e., to allow public access to a file)\ns3:GetObject\nObject\nTo check that the file was successfully uploaded\n(1) Access control list (ACL) enable you to manage access to buckets and objects. In our API it is possible to set the output of muxings and manifests to\nPUBLIC_READ\nor\nPRIVATE\n.\nPublic access for Output buckets\nFor Encodings started in our\nBitmovin Dashboard\n, the output bucket’s policy needs to allow\npublic access\nto your Amazon S3 storage. The\nPUBLIC_READ\nsetting requires also that the correct bucket ownership policies are set. Here is a FAQ on how to do this:\nHow can I configure an AWS S3 Bucket to test playback of my content?\nJSON Custom Policy\nAssuming that you are using the same IAM user and a single policy for both Input and Output buckets, you can use the following JSON payload to create your custom policy in AWS IAM.\nThe following placeholders need to be replaced with the name of your bucket(s):\n<INPUT_BUCKET_NAME>\n<OUTPUT_BUCKET_NAME>\nJSON\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"BitmovinInputBucketPermissions\",\n            \"Principal\": \"*\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::<INPUT_BUCKET_NAME>\"\n            ]\n        },\n        {\n            \"Sid\": \"BitmovinInputObjectPermissions\",\n            \"Principal\": \"*\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::<INPUT_BUCKET_NAME>/*\"\n            ]\n        },\n        {\n            \"Sid\": \"BitmovinOutputBucketPermissions\",\n            \"Principal\": \"*\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::<OUTPUT_BUCKET_NAME>\"\n            ]\n        },\n        {\n            \"Sid\": \"BitmovinOutputObjectPermissions\",\n            \"Principal\": \"*\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::<OUTPUT_BUCKET_NAME>/*\"\n            ]\n        }\n    ]\n}",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/drm-removal-from-a-stream",
    "title": "DRM Removal from a Stream",
    "text": "No. Digital Rights Management (DRM) systems provide you with the ability to control how people can consume your content. Protecting a stream with DRM means encrypting the different files (DASH / HLS segments) with the media key of one or several DRM provider. This encryption of the content is part of the encoding process when using our service.\nDRM protected content isn't decrypted by the player itself, but by the browser, using the appropriate CDM (Content Decryption Module), specific to the DRM solution (Widevine, Playready, Fairplay), if available and/or supported. The decryption itself is happening in a secured environment where no one but the browser and the CDM have access to it. The player is utilising the EME API of the browser, in order to establish a secure connection between the DRM backend and the browser, and the CDM. During this process it also get validated if a particular user is permitted to receive a DRM license to watch the content. So, player never has access, nor the viewer, to the keys needed to decrypt the content, which is why DRM is a powerful tool to protect and control access to your content.\nMore information regarding DRM can be found on\nour website\nas well as\nin our documentation\n, where it gets explained in more detail.\nIn order to Start generating DRM protected content, just create an account in our\nBitmovin Dashboard\nand\nGet Started with our Encoding Service\nright away.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/h264-presets",
    "title": "H264 Presets",
    "text": "VoD Quality Preset Configurations\nVoD Quality Presets\nVOD_STANDARD\nVOD_QUALITY\nVOD_HIGH_QUALITY\nadaptiveQuantizationMode\nVARIANCE\nVARIANCE\nVARIANCE\nadaptiveQuantizationStrength\n1\n1\n1\nadaptiveSpatialTransform\nFALSE\nFALSE\nTRUE\nbAdaptiveStrategy\nFAST\nFULL\nFULL\nbPyramid\nNORMAL\nNORMAL\nNORMAL\nbframes\n3\n3\n8\ncabac\nTRUE\nTRUE\nTRUE\ndeblockAlpha\n0\n0\n0\ndeblockBeta\n0\n0\n0\nencodingMode\nTWO_PASS\nTHREE_PASS\nTHREE_PASS\nfastSkipDetectionPFrames\nTRUE\nTRUE\nTRUE\nmacroblockTreeRatecontrol\n-\nTRUE\nTRUE\nmixedReferences\nTRUE\nTRUE\nTRUE\nmotionEstimationMethod\nHEX\nHEX\nUMH\nmvPredictionMode\nSPATIAL\nSPATIAL\nSPATIAL\nmvSearchRangeMax\n16\n24\n24\nnalHrd\nNONE\nNONE\nNONE\npartitions\n\"P8X8,B8X8,I8X8,I4X4\"\n\"I4X4,I8X8,P8X8,B8X8,P4X4\"\n\"I4X4,I8X8,P8X8,B8X8,P4X4\"\nprofile\nMAIN\nMAIN\nHIGH\npsyRateDistortionOptimization\n1\n1\n1\npsyTrellis\n0\n0\n0\nquantizerCurveCompression\n0.6\n0.6\n0.6\nrcLookahead\n40\n60\n60\nrefFrames\n3\n4\n16\nsceneCutThreshold\n0\n0\n0\nslices\n-\n1\n1\nsubMe\nRD_ALL\nRD_ALL\nRD_REF_ALL\ntrellis\nENABLED_FINAL_MB\nENABLED_ALL\nENABLED_ALL\nweightedPredictionBFrames\nTRUE\nTRUE\nTRUE\nweightedPredictionPFrames\nSMART\nSMART\nSMART\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.\nVoD Speed Preset Configurations\nVoD Speed Presets\nVOD_SPEED\nVOD_HIGH_SPEED\nVOD_VERYHIGH_SPEED\nVOD_EXTRAHIGH_SPEED\nVOD_SUPERHIGH_SPEED\nVOD_ULTRAHIGH_SPEED\nadaptiveQuantizationMode\nVARIANCE\nVARIANCE\nVARIANCE\nVARIANCE\nVARIANCE\nDISABLED\nadaptiveQuantizationStrength\n1\n1\n1\n1\n1\n0\nadaptiveSpatialTransform\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nbAdaptiveStrategy\nFAST\nFAST\nFAST\nFAST\nFAST\nNONE\nbPyramid\nNORMAL\nNORMAL\nNORMAL\nNORMAL\nNORMAL\nNORMAL\nbframes\n3\n3\n3\n3\n3\n0\ncabac\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\ndeblockAlpha\n0\n0\n0\n0\n0\n0\ndeblockBeta\n0\n0\n0\n0\n0\n0\nencodingMode\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nfastSkipDetectionPFrames\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nmacroblockTreeRatecontrol\nTRUE\n-\n-\n-\n-\n-\nmixedReferences\nTRUE\nTRUE\nFALSE\nFALSE\nFALSE\nFALSE\nmotionEstimationMethod\nHEX\nHEX\nHEX\nHEX\nDIA\nDIA\nmvPredictionMode\nSPATIAL\nSPATIAL\nSPATIAL\nSPATIAL\nSPATIAL\nSPATIAL\nmvSearchRangeMax\n16\n16\n16\n16\n16\n16\nnalHrd\nNONE\nNONE\nNONE\nNONE\nNONE\nNONE\npartitions\n\"I4X4,I8X8,P8X8,B8X8\"\n\"P8X8,B8X8,I8X8,I4X4\"\n\"P8X8,B8X8,I8X8,I4X4\"\n\"P8X8,B8X8,I8X8,I4X4\"\n\"I8X8,I4X4\"\nNONE\nprofile\nHIGH\nHIGH\nHIGH\nHIGH\nHIGH\nHIGH\npsyRateDistortionOptimization\n1\n1\n1\n1\n1\n1\npsyTrellis\n0\n0\n0\n0\n0\n0\nquantizerCurveCompression\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\nrcLookahead\n50\n30\n10\n20\n0\n0\nrefFrames\n5\n2\n1\n2\n1\n1\nsceneCutThreshold\n0\n0\n0\n0\n0\n0\nslices\n1\n-\n-\n-\n-\n-\nsubMe\nRD_IP\nRD_IP\nSATD\nQPEL4\nSAD\nFULLPEL\ntrellis\nENABLED_ALL\nENABLED_FINAL_MB\nDISABLED\nENABLED_FINAL_MB\nDISABLED\nDISABLED\nweightedPredictionBFrames\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nweightedPredictionPFrames\nSMART\nSIMPLE\nSIMPLE\nSIMPLE\nSIMPLE\nDISABLED\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.\nVoD Hardware Preset Configurations\nVoD Speed Presets\nVOD_HARDWARE_SHORTFORM\nadaptiveQuantizationMode\n-\nadaptiveQuantizationStrength\n-\nadaptiveSpatialTransform\n-\nbAdaptiveStrategy\nFULL\nbPyramid\nNORMAL\nbframes\n3\ncabac\nTRUE\ndeblockAlpha\n-\ndeblockBeta\n-\nencodingMode\n-\nfastSkipDetectionPFrames\n-\nmacroblockTreeRatecontrol\n-\nmixedReferences\n-\nmotionEstimationMethod\nUMH\nmvPredictionMode\nAUTO\nmvSearchRangeMax\n16\nnalHrd\nNONE\npartitions\n\"ALL\"\nprofile\nMAIN\npsyRateDistortionOptimization\n-\npsyTrellis\n-\nquantizerCurveCompression\n-\nrcLookahead\n50\nrefFrames\n5\nsceneCutThreshold\n-\nslices\n-\nsubMe\nRD_REF_IP\ntrellis\nENABLED_FINAL_MB\nweightedPredictionBFrames\n-\nweightedPredictionPFrames\n-\n-\nmeans that the hardware encoder doesn't support this configuration.\nConformance with H264 Profiles\nThe majority of the profiles listed above set the H264 profile to\nHIGH\n, and use codec capabilities that target decoders that support this profile. The H264 High profile is by far the most common and has been widely supported for many years.\nIf your application requires the use of lower profiles, such as\nBASELINE\nor\nMAIN\n, you will need to overwrite some of the codec configuration parameters after applying the preset. Note however that the output quality may be reduced in doing so, and file sizes and bandwidth requirements may increase.\nTo set the profile to\nMAIN\nOverwrite\nadaptiveSpatialTransform\nto False\nTo set the profile to\nBASELINE\nOverwrite\nadaptiveSpatialTransform\nto False\nOverwrite\nbFrames\nto 0\nOverwrite\ncabac\nto False\nOverwrite\nweightedPredictionPFrames\nto DISABLED\nConversely, if your selected preset sets the profile to\nMAIN\nand you want to ensure that the output uses\nHIGH\n, you can make a simple change:\nTo force the profile to\nHIGH\nOverwrite\nadaptiveSpatialTransform\nto True",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/glossary",
    "title": "Glossary",
    "text": "Are you encountering complex terms while exploring video encoding? Our Video Glossary breaks down all the technical jargon into simple, easy-to-understand definitions. Whether you’re looking to understand formats, codecs, or advanced streaming terms, this glossary has you covered! Check it out here:\nVideo Glossary\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/python-sdk",
    "title": "Python SDK",
    "text": "The Bitmovin API enables you to build cutting edge video workflows while leveraging emmy-award winning encoding solutions, that automatically optimize your catalog for high quality and cost efficient video streaming.\nQuick Start Guide\nThis Quick Start Video will demonstrate different approaches to setup an encoding workflow using a\nWatch Folder Workflow (No Code Required)\nand a\nfull Python API SDK example\nusing this\nColabortory Notebook\n.\nundefined\nGet Started with your own Project\nThe following steps will walk you through adding the SDK to an new or existing Project step by step.\nOverview\nStarting an encoding triggers a request for encoding instances. Once available, the input file will be downloaded, divided into chunks, and distributed between all of them. Those instances transcode each chunk into segments for all configured video/audio representations (muxings) in parallel and write them to the designated output destination.\nThis approach makes Bitmovin's encoding the fastest in the world.\nIn this tutorial you will implement a simple VOD online streaming scenario, where an input file containing a video stream and stereo audio stream is transcoded into ...\na fixed ladder of 3 video representations at different resolutions and bitrates using the H264 codec, and\nan audio rendition with the AAC codec.\nEach individual stream is wrapped into fragmented MP4 (fMP4) containers. HLS and DASH manifests are generated so the encoded content is stream- and playable on the majority of modern devices and browsers out there today.\nStep 1: Add Bitmovin SDK to Your Project\nTo get started add the Bitmovin SDK to your project.\nFor the latest version please refer to the\nGitHub repository\nor check the\nchangelog\nof the Bitmovin API. Open API SDK versions and the Bitmovin API share the same versioning.\nShell\npip install git+https://github.com/bitmovin/bitmovin-api-sdk-python.git\nStep 2: Setup a Bitmovin API Client Instance\nThe Bitmovin API client instance is used for all communication with the Bitmovin REST API, pass your API key to authenticate with the API. You can find your\nAPI_KEY\nin the dashboard in your\nAccount Settings\n.\nPython\nfrom bitmovin_api_sdk import BitmovinApi, BitmovinApiLogger\n\nbitmovin_api = BitmovinApi(api_key='<API_KEY>', logger=BitmovinApiLogger()))\nStep 3: Create an Input\nCreate Input\nAn input holds the configuration to access a specific file storage, from which the encoder will download the source file.\nWe support various types of input sources including HTTP(S), (S)FTP, Google Cloud Storage (GCS), Amazon Simple Storage Service (S3), Azure Blob Storage, and all cloud storage vendors that offer an S3 compatible interface.\nBelow we are creating an input. You can change the input type on the top right of the code panel.\nPython\ninput = HttpsInput(\n  name='<INPUT_NAME>',\n  host='<HTTPS_INPUT_HOST>'\n)\ninput = bitmovin_api.encoding.inputs.https.create(https_input=input)\ninput_id = input.id\nReuse Inputs\nInputs refer to a location, not a specific file.\nThis makes them easily reusable. Create an input once and reuse it for all your encodings.\nHint: All resources in our API are identified by a unique id which enables you to reuse many resources. So you only have to create one input for each location and can reuse it for all encodings.\nPython\ninput = bitmovin_api.encoding.inputs.https.get(input_id='<INPUT_ID>')\ninput_id = input.id\nStep 4: Create an Output\nCreate Output\nOutputs define where the manifests and encoded segments will be written to.\nLike inputs, outputs represent a set of information needed to access a specific storage. From this storage players can access the content for playback.\nPython\noutput = GcsOutput(\n  access_key='<GCS_ACCESS_KEY>',\n  secret_key='<GCS_SECRET_KEY>',\n  bucket_name='<GCS_BUCKET_NAME>',\n  name='<GCS_OUTPUT_NAME>'\n)\noutput = bitmovin_api.encoding.outputs.gcs.create(gcs_output=output)\noutput_id = output.id\nReuse Output\nSimilar to inputs, outputs also refer to a location, not a specific file.\nSo they're exactly as reusable as inputs. Create it once and reuse it for all your encodings.\nPython\noutput = bitmovin_api.encoding.outputs.gcs.get(output_id='<OUTPUT_ID>')\noutput_id = output.id\nStep 5: Create Codec Configurations\nCodec Configurations\nCodec configurations specify the codec and its parameters (eg. media codec, bitrate, frame rate, sampling rate).\nThey are used to encode the input stream coming from your input file.\nVideo Codec Configurations\nBelow we are creating video codec configurations. We support a multitude of codecs, including H264, H265, VP9, AV1 and many more.\nFor simplicity, we use H264 now, but you can easily change that after finishing the getting started.\nPython\nvideo_configuration_1 = H264VideoConfiguration(\n  name=\"Getting Started H264 Video Configuration 1\",\n  bitrate=1500000,\n  width=1024,\n  preset_configuration=PresetConfiguration.VOD_STANDARD\n)\nvideo_configuration_1 = bitmovin_api.encoding.configurations.video.h264.create(h264_video_configuration=video_configuration_1)\n\nvideo_configuration_2 = H264VideoConfiguration(\n  name=\"Getting Started H264 Video Configuration 2\",\n  bitrate=1000000,\n  width=768,\n  preset_configuration=PresetConfiguration.VOD_STANDARD\n)\nvideo_configuration_2 = bitmovin_api.encoding.configurations.video.h264.create(h264_video_configuration=video_configuration_2)\n\nvideo_configuration_3 = H264VideoConfiguration(\n  name=\"Getting Started H264 Video Configuration 2\",\n  bitrate=750000,\n  width=640,\n  preset_configuration=PresetConfiguration.VOD_STANDARD\n)\nvideo_configuration_3 = bitmovin_api.encoding.configurations.video.h264.create(h264_video_configuration=video_configuration_3)\nAudio Codec Configurations\nAs audio codec configuration we are creating one AAC configuration.\nWe also support many more audio codecs including Opus, Vorbis, AC3, E-AC3 and MP2.\nPython\naudio_configuration = AacAudioConfiguration(\n  name='Getting Started Audio Configuration',\n  bitrate=128000\n)\naudio_configuration = bitmovin_api.encoding.configurations.audio.aac.create(aac_audio_configuration=audio_configuration)\nReusing Codec Configurations\nAs inputs and outputs, codec configurations can also be reused.\nTo use existing video or audio codec configurations, loading them with their id.\nPython\nvideo_configuration = bitmovin_api.encoding.configurations.video.h264.get(configuration_id='<H264_CC_ID>')\naudio_configuration = bitmovin_api.encoding.configurations.video.aac.get(configuration_id='<AAC_CC_ID>'')\nStep 6: Create & Configure an Encoding\nCreate Encoding\nAn encoding is a collection of resources (inputs, outputs, codec configurations etc.), mapped to each other.\nThe cloud region defines in which cloud and region your encoding will be started.\nIf you don't set it, our encoder takes the region closest to your input and output.\nInputs, outputs and codec configurations can be reused among many encodings. Other resources are specific to one encoding, like streams, which we'll create below.\nPython\nencoding = Encoding(\n  name='Getting Started Encoding',\n  cloud_region=CloudRegion.GOOGLE_EUROPE_WEST_1\n)\nencoding = bitmovin_api.encoding.encodings.create(encoding=encoding)\nStreams\nA stream maps an audio or video input stream of your input file (called input stream) to one audio or video codec configuration.\nThe following example uses the selection mode\nAUTO\n, which will select the first available video input stream of your input file.\nYou can choose an input file from 3 predefined examples on the top right of the code panel.\nVideo Stream\nPython\ninput_path = '<INPUT_PATH>'\n\nvideo_stream_input = StreamInput(\n    input_id=input_id,\n    input_path=input_path,\n    selection_mode=StreamSelectionMode.AUTO\n)\n\nvideo_stream_1 = Stream(\n    input_streams=[video_stream_input],\n    codec_config_id=video_configuration_1.id\n)\nvideo_stream_1 = bitmovin_api.encoding.encodings.streams.create(encoding_id=encoding.id, stream=video_stream_1)\n\nvideo_stream_2 = Stream(\n    input_streams=[video_stream_input],\n    codec_config_id=video_configuration_2.id\n)\nvideo_stream_2 = bitmovin_api.encoding.encodings.streams.create(encoding_id=encoding.id, stream=video_stream_2)\n\nvideo_stream_3 = Stream(\n    input_streams=[video_stream_input],\n    codec_config_id=video_configuration_3.id\n)\nvideo_stream_3 = bitmovin_api.encoding.encodings.streams.create(encoding_id=encoding.id, stream=video_stream_3)\nAudio Stream\nPython\naudio_stream_input = StreamInput(\n  input_id=input_id,\n  input_path=input_path,\n  selection_mode=StreamSelectionMode.AUTO\n)\naudio_stream = Stream(\n  input_streams=[audio_stream_input],\n  codec_config_id=audio_configuration.id\n)\naudio_stream = bitmovin_api.encoding.encodings.streams.create(encoding_id=encoding.id, stream=audio_stream)\nStep 7: Create a Muxing\nMuxings\nA muxing defines which container format will be used for the encoded video or audio files (segmented TS, progressive TS, MP4, FMP4, ...). It requires a stream, an output, and the output path, where the generated segments will be written to.\nFMP4 muxings are used for DASH manifest representations, TS muxings for HLS playlist representations.\nFMP4\nPython\nacl_entry = AclEntry(permission=AclPermission.PUBLIC_READ)\nsegment_length = 4.0\noutput_path = '<OUTPUT_PATH>'\n\nvideo_muxing_stream_1 = MuxingStream(stream_id=video_stream_1.id)\nvideo_muxing_output_1 = EncodingOutput(\n  output_path=output_path + '/video/1024_1500000/fmp4/',\n  output_id=output_id,\n  acl=[acl_entry]\n)\nvideo_muxing_1 = Fmp4Muxing(\n  segment_length=segment_length,\n  outputs=[video_muxing_output_1],\n  streams=[video_muxing_stream_1]\n)\nvideo_muxing_1 = bitmovin_api.encoding.encodings.muxings.fmp4.create(encoding_id=encoding.id, fmp4_muxing=video_muxing_1)\n\nvideo_muxing_stream_2 = MuxingStream(stream_id=video_stream_2.id)\nvideo_muxing_output_2 = EncodingOutput(\n  output_path=output_path + '/video/768_1000000/fmp4/',\n  output_id=output_id,\n  acl=[acl_entry]\n)\nvideo_muxing_2 = Fmp4Muxing(\n  segment_length=segment_length,\n  outputs=[video_muxing_output_2],\n  streams=[video_muxing_stream_2]\n)\nvideo_muxing_2 = bitmovin_api.encoding.encodings.muxings.fmp4.create(encoding_id=encoding.id, fmp4_muxing=video_muxing_2)\n\nvideo_muxing_stream_3 = MuxingStream(stream_id=video_stream_3.id)\nvideo_muxing_output_3 = EncodingOutput(\n  output_path=output_path + '/video/640_750000/fmp4/',\n  output_id=output_id,\n  acl=[acl_entry]\n)\nvideo_muxing_3 = Fmp4Muxing(\n  segment_length=segment_length,\n  outputs=[video_muxing_output_3],\n  streams=[video_muxing_stream_3]\n)\nvideo_muxing_3 = bitmovin_api.encoding.encodings.muxings.fmp4.create(encoding_id=encoding.id, fmp4_muxing=video_muxing_3)\nAudio Muxings\nPython\naudio_muxing_stream = MuxingStream(stream_id=audio_stream.id)\naudio_muxing_output = EncodingOutput(\n  output_path=output_path + '/audio/128000/fmp4/',\n  output_id=output_id,\n  acl=[acl_entry]\n)\naudio_muxing = Fmp4Muxing(\n  segment_length=segment_length,\n  outputs=[audio_muxing_output],\n  streams=[audio_muxing_stream]\n)\naudio_muxing = bitmovin_api.encoding.encodings.muxings.fmp4.create(encoding_id=encoding.id, fmp4_muxing=audio_muxing)\nHint: To optimize your content for a specific use-case you can also provide a custom segment length. Further, you can also customize the naming pattern for the resulting segments and the initialization segment as well.\nStep 8: Create a Manifest\nDASH & HLS Manifests\nIts recommended to use Default Manifests, one each for DASH (\nAPI-Reference\n) and HLS (\nAPI-Reference\n). They are a construct that leaves it to the encoder to determine how to best generate the manifest based on what resources have been created by the encoding. This simplifies the code greatly for simple scenarios like this guide is about.\nCreate a DASH Manifest\nPython\nmanifest_output = EncodingOutput(output_id=output_id,\n                                 output_path=output_path,\n                                 acl=[AclEntry(scope='*', permission=AclPermission.PUBLIC_READ)])\ndash_manifest = DashManifestDefault(manifest_name=\"stream.mpd\",\n                                    encoding_id=encoding.id,\n                                    version=DashManifestDefaultVersion.V2,\n                                    outputs=[manifest_output])\ndash_manifest = bitmovin_api.encoding.manifests.dash.default.create(dash_manifest)\nCreate a HLS manifest\nPython\nhls_manifest = HlsManifestDefault(manifest_name=\"stream.m3u8\",\n                                  encoding_id=encoding.id,\n                                  version=HlsManifestDefaultVersion.V1,\n                                  outputs=[manifest_output])\nhls_manifest = bitmovin_api.encoding.manifests.hls.default.create(hls_manifest)\nStep 9: Start an Encoding\nLet's recap - We have defined:\nan\ninput\n, specifying the input file host\nan\noutput\n, specifying where the manifest and the encoded files will be written to\ncodec configurations\n, specifying how the input file will be encoded\nstreams\n, specifying which input stream will be encoded using what codec configuration\nmuxings\n, specifying the containerization of the streams\nmanifests\n, to play the encoded content using the HLS or DASH streaming format.\nNow we can start the encoding with one API call, including additional configuration so the DASH and HLS Default-Manifests are created as part of the encoding process too:\nPython\nstart_request = StartEncodingRequest(\n    manifest_generator=ManifestGenerator.V2,\n    vod_dash_manifests=[ManifestResource(manifest_id=dash_manifest.id)],\n    vod_hls_manifests=[ManifestResource(manifest_id=hls_manifest.id)]\n)\n\nbitmovin_api.encoding.encodings.start(encoding_id=encoding.id, start_encoding_request=start_request)\nWhat happens next:\nAfter the successful start of the encoding, it will change its state from\nCREATED\nto\nSTARTED\n, and shortly after that to\nQUEUED\n.\nRUNNING\nit will be in as soon as the input file starts to get downloaded and processed. Eventually\nFINISHED\nwill indicate a successful encoding and upload of the content to the provided Output destination. More details about each encoding status can be\nfound here\n.\nStep 10: Final Review\nCongratulations!\nYou successfully started your first encoding :) and this is only the beginning. The Bitmovin Encoding API is extremely flexible and allows many customizations and provides with access to the latest codecs and video streaming optimizations and technologies out there. Have a look at the\nAPI Reference\nto learn more.\nMore Examples\nYou can find many more fully functional code examples in our Github Repository:\nGitHub",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/d268774-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/optimizing-your-h264-codec-configuration-for-different-use-cases",
    "title": "Optimizing Your H.264 Codec Configuration for Different Use Cases",
    "text": "Overview\nCodec Configurations\nare a collection of settings to control the behaviour of a codec supported by our service. As most of its settings have a default value, those are chosen to provide good results for multitude of use-cases. However, most of time you will find yourself to optimize your Live or VoD workflow to a specific use case. As Codecs have in general a broad set of settings, finding the right mix for your Codec Configurations and use-case can be very time consuming and requires a deep knowledge of how a particular codec works as well.\nPreset Configurations\nmake it easy to achieve this optimizations and let you select between several configuration templates optimized to improve encoding speed, quality and more. If you know what you are doing, you can also use them as a base configuration template, and simply overwrite specific settings with your own values to perfectly optimize them to your needs.\nIf you are just looking for a table listing all H264 Preset Configurations and their settings, please\nclick here\n:)\nCreate a H264 Codec Configuration with Preset Configuration\nPreset Configurations\nare templates, a codec configuration can be based on. You are still able to define your own values for certain settings as usual by defining them specifically when you create a codec configuration, so they will simply overwrite the template value.\nJava\nBitmovinApi bitmovinApi = BitmovinApi.builder()\n           .withApiKey(\"YOUR_BITMOVIN_API_KEY\")\n           .build();\n\nH264VideoConfiguration h264VideoConfiguration = new H264VideoConfiguration();\nh264VideoConfiguration.setName(\"Your first CodecConfig with Presets\");\nh264VideoConfiguration.setProfile(ProfileH264.HIGH);\nh264VideoConfiguration.setBitrate(4500000L);\nh264VideoConfiguration.setPresetConfiguration(PresetConfiguration.VOD_HIGH_QUALITY);\n\nh264VideoConfiguration = bitmovinApi.encoding.configurations.video.h264.create(h264VideoConfiguration);\nAs of now, 10 preset configurations are available, which are either optimized to achieve higher speeds, quality or low-latency when it comes to live-encodings.\nFor Live Encodings:\nLIVE_HIGH_QUALITY\nLIVE_LOW_LATENCY\nFor VoD Encodings:\nVOD_HIGH_QUALITY\nVOD_STANDARD\nVOD_SPEED\nVOD_HIGH_SPEED\nVOD_VERYHIGH_SPEED\nVOD_EXTRAHIGH_SPEED\nVOD_SUPERHIGH_SPEED\nVOD_ULTRAHIGH_SPEED\nUse-Cases explained\nAs you can tell already from their names, Preset Configurations are designed to optimize a codec configuration for particular use-cases. Their requirements can be typically reduced to either quality, speed or a mix of both. Therefore lets look at some common use-cases and which Preset Configuration would fit best.\nHigh quality video content\nYour VoD platform offers movies, or TV shows which are released based on fixed schedule. You most likely have enough time to process this video content and therefore strive to provide high quality video content to your user base. So, as speed is not crucial, and your focus will be on creating high quality outputs with your encodings, the Preset Configuration\nVOD_HIGH_QUALITY\nis a good configuration to start.\nHint:\nWhile the codec config is tweaked, the bitrate is still a static value and therefore prone to be set too low or too high, which either results in quality loss, or increased CDN delivery costs if too much bitrate is used. Per-Title encoding enables you to perfectly optimize for quality and bitrate consumption on a \"per title\" basis.\nGive it a try!\nNews clips\nSomething happened somewhere in the world and you want to be the first to show it to everyone, so turn around time is critical. While other factors have an impact on that time as well, using Preset Configurations like\nVOD_HIGH_SPEED\n, or\nVOD_ULTRAHIGH_SPEED\nenable you to lower the time needed to encode the content and therefore upload it to your storage as soon as possible.\nLive Sports / Interactive Streaming / eSports\nAll these use-cases are low latency scenarios. Their requirements are still a bit different therefore make sure you know your own requirements and what latency you actually need. Using the Preset Configuration\nLIVE_LOW_LATENCY\nis a first step to optimize your live-encoding workflow.\nUser generated Content\nUser generated content comes in all kind of formats, resolutions and qualities. Depending on your use-case, either a quick turn around time or general good quality output is relevant, probably both :) If you don't enforce certain minimum requirements, like resolutions, bitrates, or formats this can get really tricky. Therefore going with a mid range preset configuration like\nVOD_STANDARD\nor\nVOD_SPEED\nwould be a good first step to generate good quality content in a faster manner.\nThose are all common use-cases, however they are also just the tip of the iceberg of use-cases you can handle with integrating our services. So, if you are wondering how your use-case can be covered effectively,\nlet us know\n:)\nOther codecs\nThe examples above used the H264 codec. The majority of the profiles are also usable with H265 and VP9. For a full list of the presets of each codec, and the detail of the codec parameters they set, please follow these links\nH264 Presets\nH265 Presets\nVP9 Presets",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/h265-presets",
    "title": "H265 Presets",
    "text": "VoD Quality Preset Configurations\nVoD Quality Presets\nVOD_STANDARD\nVOD_HIGH_QUALITY\nadaptiveQuantizationMode\nAUTO_VARIANCE\nVARIANCE\nadaptiveQuantizationMotion\nFALSE\nFALSE\nadaptiveQuantizationStrength\n1\n1\nallowedRADLBeforeIDR\n0\n0\nanalyzeSourceFramePixels\nFALSE\nFALSE\nasymetricMotionPartitionsAnalysis\nFALSE\nFALSE\nbAdapt\nFULL\nFULL\nbframeBias\n0\n0\nbframes\n4\n4\nblurComplexity\n20\n20\nblurQuants\n0.5\n0.5\ncodingUnitLossless\nFALSE\nFALSE\nconstrainedIntraPrediction\nFALSE\nFALSE\ncopyPicture\nTRUE\nTRUE\ncutree\nTRUE\nTRUE\ndynamicRateDistortionStrength\n0\n0\nearlySkip\nFALSE\nTRUE\nencodingMode\nTWO_PASS\nTHREE_PASS\nevaluationOfIntraModesInBSlices\nFALSE\nFALSE\nfastSearchForAngularIntraPredictions\nFALSE\nFALSE\nforceFlush\nDISABLED\nDISABLED\ngopLookahead\n0\n0\ngrainOptimizedRateControl\nFALSE\nFALSE\nipRatio\n1.4\n1.4\nlevelHighTier\nTRUE\nTRUE\nlimitModes\nFALSE\nTRUE\nlimitReferences\nDEPTH_AND_CU\nDEPTH_AND_CU\nlimitSao\nFALSE\nFALSE\nlimitTransformUnitDepthRecursion\nDISABLED\nLEVEL_4\nlookaheadSlices\n8\n0\nlowpassDct\nFALSE\nFALSE\nmaxCTUSize\n64\n64\nmaxMerge\n2\n4\nmaximumTransformUnitSize\nMTU_32x32\nMTU_32x32\nminCodingUnitSize\nMCU_8x8\nMCU_8x8\nmotionSearch\nHEX\nHEX\nmotionSearchRange\n57\n57\nnoiseReductionInter\n0\n0\npbRatio\n1.3\n1.3\npixelFormat\nYUV420P\nYUV420P\nprofile\nmain\nmain\npsyRateDistortionOptimization\n2\n2\npsyRateDistortionOptimizedQuantization\n0\n1\nqpOffsetChromaCb\n0\n0\nqpOffsetChromaCr\n0\n0\nqpStep\n4\n4\nquantizationGroupSize\nmaxCTUSize(QGS_64x64) / QGS_32x32\nQGS_32x32\nquantizerCurveCompressionFactor\n0.6\n0.6\nrateDistortionLevelForModeDecision\n3\n4\nrateDistortionLevelForQuantization\nDISABLED\nLEVELS_AND_CODING_GROUPS\nrateDistortionPenalty\nDISABLED\nDISABLED\nrcLookahead\n20\n25\nrectangularMotionPartitionsAnalysis\nFALSE\nFALSE\nrecursionSkip\nTRUE\nTRUE\nrefFrames\n3\n5\nrefineRateDistortionCost\nFALSE\nFALSE\nsao\nTRUE\nTRUE\nsaoNonDeblock\nFALSE\nFALSE\nsceneCutThreshold\n40\n40\nscenecutBias\n5\n5\nsignHide\nTRUE\nTRUE\nskipSplitRateDistortionAnalysis\nFALSE\nFALSE\nslices\n1\n1\nssimRateDistortionOptimization\nFALSE\nFALSE\nstrongIntraSmoothing\nTRUE\nTRUE\nsubMe\n2\n4\ntemporalMotionVectorPredictors\nTRUE\nTRUE\ntransformSkip\nNONE\nNONE\ntuInterDepth\n1\n1\ntuIntraDepth\n1\n1\nwavefrontParallelProcessing\nTRUE\nTRUE\nweightPredictionOnBSlice\nFALSE\nFALSE\nweightPredictionOnPSlice\nTRUE\nTRUE\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.\nVoD Speed Preset Configurations\nVoD Speed Presets\nVOD_SPEED\nVOD_HIGH_SPEED\nVOD_VERYHIGH_SPEED\nVOD_EXTRAHIGH_SPEED\nVOD_SUPERHIGH_SPEED\nVOD_ULTRAHIGH_SPEED\nadaptiveQuantizationMode\nAUTO_VARIANCE\nAUTO_VARIANCE\nAUTO_VARIANCE\nAUTO_VARIANCE\nDISABLED\nDISABLED\nadaptiveQuantizationMotion\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nadaptiveQuantizationStrength\n1\n1\n1\n1\n1\n1\nallowedRADLBeforeIDR\n0\n0\n0\n0\n0\n0\nanalyzeSourceFramePixels\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nasymetricMotionPartitionsAnalysis\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nbAdapt\nFULL\nNONE\nNONE\nNONE\nNONE\nNONE\nbframeBias\n0\n0\n0\n0\n0\n0\nbframes\n4\n4\n4\n4\n3\n3\nblurComplexity\n20\n20\n20\n20\n20\n20\nblurQuants\n0.5\n0.5\n0.5\n0.5\n0.5\n0.5\ncodingUnitLossless\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nconstrainedIntraPrediction\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\ncopyPicture\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\ncutree\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\ndynamicRateDistortionStrength\n0\n0\n0\n0\n0\n0\nearlySkip\nFALSE\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nencodingMode\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nSINGLE_PASS\nevaluationOfIntraModesInBSlices\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nfastSearchForAngularIntraPredictions\nFALSE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nforceFlush\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\ngopLookahead\n0\n0\n0\n0\n0\n0\ngrainOptimizedRateControl\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nipRatio\n1.4\n1.4\n1.4\n1.4\n1.4\n1.4\nlevelHighTier\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nlimitModes\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nlimitReferences\nDEPTH_AND_CU\nDEPTH_AND_CU\nDEPTH_AND_CU\nDEPTH_AND_CU\nDISABLED\nDISABLED\nlimitSao\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nlimitTransformUnitDepthRecursion\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nlookaheadSlices\n8\n8\n8\n8\n8\n8\nlowpassDct\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nmaxCTUSize\n64\n64\n64\n64\n32\n32\nmaxMerge\n2\n2\n2\n2\n2\n2\nmaximumTransformUnitSize\nMTU_32x32\nMTU_32x32\nMTU_32x32\nMTU_32x32\nMTU_32x32\nMTU_32x32\nminCodingUnitSize\nMCU_8x8\nMCU_8x8\nMCU_8x8\nMCU_8x8\nMCU_8x8\nMCU_16x16\nmotionSearch\nHEX\nHEX\nHEX\nHEX\nHEX\nDIA\nmotionSearchRange\n57\n57\n57\n57\n57\n57\nnoiseReductionInter\n0\n0\n0\n0\n0\n0\npbRatio\n1.3\n1.3\n1.3\n1.3\n1.3\n1.3\npixelFormat\nYUV420P\nYUV420P\nYUV420P\nYUV420P\nYUV420P\nYUV420P\nprofile\nmain\nmain\nmain\nmain\nmain\nmain\npsyRateDistortionOptimization\n2\n2\n2\n2\n2\n2\npsyRateDistortionOptimizedQuantization\n0\n0\n0\n0\n0\n0\nqpOffsetChromaCb\n0\n0\n0\n0\n0\n0\nqpOffsetChromaCr\n0\n0\n0\n0\n0\n0\nqpStep\n4\n4\n4\n4\n4\n4\nquantizationGroupSize\nmaxCTUSize(QGS_64x64) / QGS_32x32\nmaxCTUSize(QGS_64x64) / QGS_32x32\nmaxCTUSize(QGS_64x64) / QGS_32x32\nmaxCTUSize(QGS_64x64) / QGS_32x32\nmaxCTUSize(QGS_64x64) / QGS_32x32\nmaxCTUSize(QGS_64x64) / QGS_32x32\nquantizerCurveCompressionFactor\n0.6\n0.6\n0.6\n0.6\n0.6\n0.6\nrateDistortionLevelForModeDecision\n3\n2\n2\n2\n2\n2\nrateDistortionLevelForQuantization\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nrateDistortionPenalty\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nDISABLED\nrcLookahead\n20\n15\n15\n15\n10\n5\nrectangularMotionPartitionsAnalysis\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nrecursionSkip\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nrefFrames\n3\n3\n2\n2\n1\n1\nrefineRateDistortionCost\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nsao\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\nsaoNonDeblock\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nsceneCutThreshold\n40\n40\n40\n40\n40\n0\nscenecutBias\n5\n5\n5\n5\n5\n5\nsignHide\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nskipSplitRateDistortionAnalysis\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nslices\n1\n1\n1\n1\n1\n1\nssimRateDistortionOptimization\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nstrongIntraSmoothing\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nsubMe\n2\n2\n1\n2\n1\n0\ntemporalMotionVectorPredictors\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\ntransformSkip\nNONE\nNONE\nNONE\nNONE\nNONE\nNONE\ntuInterDepth\n1\n1\n1\n1\n1\n1\ntuIntraDepth\n1\n1\n1\n1\n1\n1\nwavefrontParallelProcessing\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nTRUE\nweightPredictionOnBSlice\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\nweightPredictionOnPSlice\nTRUE\nTRUE\nTRUE\nTRUE\nFALSE\nFALSE\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.\nVoD Hardware Preset Configurations\nVoD Quality Presets\nVOD_HARDWARE_SHORTFORM\nadaptiveQuantizationMode\n-\nadaptiveQuantizationMotion\n-\nadaptiveQuantizationStrength\n-\nallowedRADLBeforeIDR\n-\nanalyzeSourceFramePixels\n-\nasymetricMotionPartitionsAnalysis\nFULL\nbAdapt\n-\nbframeBias\n-\nbframes\n4\nblurComplexity\n-\nblurQuants\n-\ncodingUnitLossless\n-\nconstrainedIntraPrediction\n-\ncopyPicture\n-\ncutree\n-\ndynamicRateDistortionStrength\n-\nearlySkip\n-\nencodingMode\nSINGLE_PASS\nevaluationOfIntraModesInBSlices\n-\nfastSearchForAngularIntraPredictions\n-\nforceFlush\n-\ngopLookahead\n-\ngrainOptimizedRateControl\n-\nipRatio\n-\nlevelHighTier\n-\nlimitModes\n-\nlimitReferences\n-\nlimitSao\n-\nlimitTransformUnitDepthRecursion\n-\nlookaheadSlices\n-\nlowpassDct\n-\nmaxCTUSize\n64\nmaxMerge\n-\nmaximumTransformUnitSize\n-\nminCodingUnitSize\n-\nmotionSearch\nSTAR\nmotionSearchRange\n57\nnoiseReductionInter\n-\npbRatio\n-\npixelFormat\nYUV420P\nprofile\nmain\npsyRateDistortionOptimization\n-\npsyRateDistortionOptimizedQuantization\n-\nqpOffsetChromaCb\n-\nqpOffsetChromaCr\n-\nqpStep\n-\nquantizationGroupSize\n-\nquantizerCurveCompressionFactor\n-\nrateDistortionLevelForModeDecision\n-\nrateDistortionLevelForQuantization\n-\nrateDistortionPenalty\n-\nrcLookahead\n25\nrectangularMotionPartitionsAnalysis\n-\nrecursionSkip\n-\nrefFrames\n4\nrefineRateDistortionCost\n-\nsao\nTRUE\nsaoNonDeblock\n-\nsceneCutThreshold\n-\nscenecutBias\n-\nsignHide\n-\nskipSplitRateDistortionAnalysis\n-\nslices\n-\nssimRateDistortionOptimization\n-\nstrongIntraSmoothing\n-\nsubMe\n3\ntemporalMotionVectorPredictors\n-\ntransformSkip\n-\ntuInterDepth\n1\ntuIntraDepth\n1\nwavefrontParallelProcessing\n-\nweightPredictionOnBSlice\nFALSE\nweightPredictionOnPSlice\nTRUE\n-\nmeans that the hardware encoder doesn't support this configuration.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/reducing-turnaround-time-for-vod-encodings",
    "title": "Reducing Turnaround Time in VOD Encodings",
    "text": "Overview\nThere are cases for which it is most important to achieve fast turnaround of your encodings, when the speed of the encoding comes before any other concern. This page provides you with a number of pointers and makes recommendations, whilst also highlighting the compromises you may have to make.\nWe will assume that your baseline is a “standard” encoding, as you would have executed by using one of our SDK examples. The type of configuration tuning highlighted in the rest of this document requires you to use the full API, and will not be accessible if you are using our other encoding mechanisms (such as dashboard or Simple Encoding API).\nThe techniques presented below can all be combined (unless indicated otherwise). The increase in speed you will get from any combination of them is difficult to quantify, as it depend (among others) on the duration and complexity of the asset.\nYou will need to perform some experimentation with your own content to determine what works best for your use case…\nInputs\nYou may not be able to control the specification of the source files you will encode, but if you do, the following recommendations apply:\nAvoid unnecessarily high bitrates. High bitrates files take a long time to download, analyse and decode.\nAvoid complex codecs (such as JPEG2000). They take a significant amount of time to decode.\nEncoding Configuration\nA fast codec\nFirst, to ensure that the encoding is quick, choose a “simpler” codec. H264 is going to be by far the fastest to encode with.\nWhat compromise you may have to make\nMore advanced codecs such as H265, VP9 or AV1 will be much better at achieving higher quality for given bitrates. Therefore selecting H264 instead may compromise the quality, or force you to use higher bitrates.\nA speed-focused preset\nAll our codec configurations allow you to use a preset. Some of those presets are focused on achieving speed.\nH264:\nVoD Speed Preset Configurations\nH265:\nVoD Speed Preset Configurations\nVP9:\nVoD Speed Preset Configurations\nAV1: available presets are stated in the\nAPI reference\nIt’s a simple question of choosing the correct one (maybe the fastest one?) and setting it in your codec configuration:\nJava\nH264VideoConfiguration config = new H264VideoConfiguration();\n...\nconfig.setPresetConfiguration(PresetConfiguration.VOD_ULTRAHIGH_SPEED);\n...\nreturn bitmovinApi.encoding.configurations.video.h264.create(config);\nWhat compromise you may have to make\nThe more a preset is targetted towards achieving speed, the worse it will be at getting good quality for a given bitrate, compared to a quality-focused preset. You will therefore have to compromise on quality or bitrate.\nA fast encoding mode\nThe Encoding Mode determines how the encoder will perform its rate control. Multiple passes (Bitmovin offers 2 or 3) will naturally make the encoding slower. You could therefore choose to use a single pass instead, which is again simply done in the codec configuration.\nJava\nH264VideoConfiguration config = new H264VideoConfiguration();\n...\nconfig.setEncodingMode(EncodingMode.SINGLE_PASS);\n...\nreturn bitmovinApi.encoding.configurations.video.h264.create(config);\nWhat compromise you may have to make\nWith single-pass, the encoder only has a very crude mechanism for rate control, and you will therefore be more likely to have suboptimal quality in some scenes, and possibly an output bitrate that deviates significantly from your target bitrate.\nSegmented Muxing\nDue to the distributed nature of our encoder, it pays to avoid having to “stitch” separately encoded chunks into a single file again (if your requirements downstream allow you to do so). You should therefore use our segmented muxings instead of our “progressive” (single-file) ones. Which means:\nPrefer the Fmp4Muxing over an Mp4Muxing\nPrefer the TsMuxing over a ProgressiveTsMuxing\nPrefer the WebmMuxing over a ProgressiveWebmMuxing\nInfrastructure\nThe correct storage and region\nYou will want to ensure first that no unnecessary delay is caused by transfer of input and output files. Our encoder will run in your selected cloud provider (AWS, GCP or Azure), and you should therefore first and foremost ensure that your source files are stored in the corresponding storage solution (respectively S3, GCS and Blob Storage) and that your outputs are sent to the same type of storage.\nYour Input and Output storage should also be in the same cloud region, and you should configure your encoding to run in that same cloud region.\nJava\nEncoding encoding = new Encoding();\n...\nencoding.setCloudRegion(CloudRegion.AWS_AP_SOUTH_1);\n...\nencoding = bitmovinApi.encoding.encodings.create(encoding);\nPre-Warming\nWhen you start an encoding, it will first be queued whilst an encoder instance is spun up and configured, as described in\nWhat do the different encodings state mean?\nThe queue time can be a significant portion of the turnaround time, in particular for short source files. In most circumstances, you will be able to reduce that time with the use of pre-warmed pools (described in detail at\nHow to use Pre-warmed Encoder Pools\n). A pre-warmed encoder pool can be\nstatic\nor\ndynamic\nin size, based on your workflow requirements and resource demands.\nWhat compromise you may have to make\nPre-warmed pools may increase your encoding costs, in particular if they’re not used with care. Only configure the pools with as many instances as you may reasonably require, and don’t forget to shut them down when not needed to avoid incurring costs.\nEncoding Slots\nYour account is configured to let you perform a number of encodings concurrently (typically 25, but this may depend on your own commercial agreement). If you start more than this number of encodings in a short time span, the additional encodings above that threshold will be added to a queue, waiting for a slot to be available.\nIf you have a need to be able to run a large number of encodings in a short amount of time, we may be able to increase your encoding slots temporarily. Reach to your account manager or to our support team to make that request.\nWorkflow\nThe final set of recommendations relates to your whole workflow.\nIf your desired output is a complex ladder, with mutiple renditions (with a range of different resolutions and bitrates), you may want to consider splitting it in a 2-stage process:\nConfigure and start an encoding with a single rendition (or a small set of renditions) at a relatively low resolution and bitrate (just enough not to be of terrible visual quality)\nIn parallel to it, configure and start the encoding with the full ladder.\nThe first encoding will finish first and therefore the output will be available sooner. When the second encoding finishes, viewers can be re-directed to it for the full ABR experience.\nHow you go about doing this will mainly depend on the upstream and downstream components in your workflow. Some considerations:\nare the renditions of the first encoding also added to the second encoding, or is the full set requiring outputs of both encodings (with a multi-encoding manifest generation)?\nwill both encodings use the same or different output folders (and will files get overwritten in the process)?\nWhat compromise you may have to make\nThe workflow will become significantly more complex, and the orchestration layer will need to be able to track the status of multiple encodings.\nEncoding costs may be raised as well (albeit in a minor way) if the same renditions are produced by multiple encodings.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/incompatible-output-frame-rates",
    "title": "Incompatible Output Frame Rates",
    "text": "Motivation\nOn 2.186.0 we changed our internal rate change logic to always produce output segments of constant length. This means that in any configuration, every segment of an output video stream will have the exact same length in frames e.g. 24 fps & 4 seconds --> 96 frames segment length. This behaviour is required by some downstream workflows, e.g. packetizers.\nBut with this new rate change behaviour we have some new requirements for the frame rate values on multiple video rendition output configuration.\nNew Restrictions and how to resolve them\nThere are 3 cases that will lead to a new fail fast error by our encoder which are described below and how to avoid them:\n1. Output fps compatibility on different video renditions\nIt is not possible anymore to configure combinations of fractional and non fractional output fps on different output renditions. The solution is to either change the non fractional ones to fractional output fps values or the fractional ones to non fractional fps values.\n--> all videoConfigurations must have fractional fps values or all videoConfigurations must have non-fractional fps values\nThe most common fractional fps values are 23.976 fps (24000 / 1001), 29.97 fps (30000 / 1001) or 59.94 fps (60000 / 1001). Non-fractional fps are those without a decimal point like 24 fps, 25 fps or 30 fps.\nExample (java)\nvar videoConfig1 = new H264VideoConfiguration();\nvar videoConfig2 = new H264VideoConfiguration();\nvar videoConfig3 = new H264VideoConfiguration();\nvideoConfig1.setRate(24.0);\n❌\nvideoConfig2.setRate(24000.0 / 1001.0);\n❌\nvideoConfig3.setRate(30000.0 / 1001.0);\n❌\nvideoConfig1.setRate(24.0);\n✅\nvideoConfig2.setRate(24.0);\n✅\nvideoConfig3.setRate(30.0);\n✅\nor\nvideoConfig1.setRate(24000.0 / 1001.0);\n✅\nvideoConfig2.setRate(24000.0 / 1001.0);\n✅\nvideoConfig3.setRate(30000.0 / 1001.0);\n✅\nIf it is required for you to have both a fractional and a non fractional output fps. It is necessary since 2.186.0 to do two separate encodings to achieve all the outputs you need. It is still possible to fallback to a version older than 2.185.0 but then at least one of the output renditions will show a non-constant segment length behavior (for more details on this check \"Background\" below).\n2. Fps values are not set on some videoConfigurations\nWhen the fps values are set only on some of your videoConfigurations our encoder will take over the input fps for the videoConfigurations that don't have an fps value set. Depending on your values for the configured fps and for the value of the input fps a configuration like this might work or just run into Case 1.) (compare above). To prevent this behaviour it is recommended to:\n--> for all videoConfigurations a value for fps should be set.\nExample (java) with input fps 23.976:\nvideoConfig1.setRate(24.0);\n❌\nvideoConfig2.setRate(24.0);\n❌\nvideoConfig3.setRate(null);\n❌ (not set)\nthis config for an input of 23.976 fps would automatically lead to the following setup:\nvideoConfig1.setRate(24.0);\n❌\nvideoConfig2.setRate(24.0);\n❌\nvideoConfig3.setRate(23.976);\n❌\nwhich is invalid by case 1.) and should be resolved by just setting a value to all videoConfigurations e.g.:\nvideoConfig1.setRate(24.0);\n✅\nvideoConfig2.setRate(24.0);\n✅\nvideoConfig3.setRate(24.0);\n✅\n3. Fractional segment length\nwith fractional segment length values, e.g. 1.8 seconds segment legnth. It might be that even different non-fractional values are incompatible to each other. It is not possible to reach exactly 1.8 seconds with some fps values like e.g. 24 fps but with e.g. 25 fps it is possible to reach exactly 1.8 seconds. This case of two different segment length configurations is not supported.\n--> all fps values need to be chosen to fit the configured segment duration.\nassuming the segment duration is configured to 1.8 seconds:\nvideoConfig1.setRate(24.0);\n❌\nvideoConfig2.setRate(25.0);\n❌\nvideoConfig1.setRate(24.0);\n✅\nvideoConfig2.setRate(48.0);\n✅ (all streams would result in 1.8333 seconds output segment length)\nor\nvideoConfig1.setRate(25.0);\n✅\nvideoConfig2.setRate(50.0);\n✅\nvideoConfig3.setRate(30.0);\n✅ (all streams result in 1.8 seconds segment length)\n4. Wrong rounding\nIn case when fractional frames rates are rounded wrongly, the new fail fast will trigger.\nFractional frame rates (besides the common ones described above) are constructed by an integer value multiplied by 1000 and devided by 1001. E.g.: 24000.0 / 1001.0 = 23.976023976. We configured the new fail fast to let rounded values pass that are similar to 23.976 in case of 24000.0 / 1001. But roundings like 23.975 or 23.98 for the value of 24000.0 / 1001.0 will not pass.\n--> fractional frame rate configurations need to have some precision. It is recommended to just configure them with the division:\nExample (java)\nvideoConfig1.setRate(24000.0 / 1001.0);\n✅\nvideoConfig2.setRate(23.975);\n❌\nvideoConfig3.setRate(29.97);\n✅\nvideoConfig1.setRate(24000.0 / 1001.0);\n✅\nvideoConfig2.setRate(23.976);\n✅\nvideoConfig3.setRate(29.97);\n✅\nbetter\nvideoConfig1.setRate(24000.0 / 1001.0);\n✅\nvideoConfig2.setRate(24000.0 / 1001.0);\n✅\nvideoConfig3.setRate(30000.0 / 1001.0);\n✅\nBackground\nOn 2.186.0 we changed our internal rate change logic to always produce output segments of constant length. This means that in any configuration of (e.g. 4 seconds segment length and 24 fps output), every segment will have a length of 96 frames. This is different from the previous behavior (before version 2.186.0) of the rate change logic. In cases where a rate change of 23.976 fps (fractional) to 24 fps was done in the former implementation, some of the segments would be 97 frames in length to correct the drift error between the framerates.\nWe are now correcting this error in a different way which enables us to consistently output 96 frames for 24 fps outputs no matter the input fps provided to our encoder. But with this new way of error correction comes a new requirement for it to work correctly for the configured output fps of multiple output streams:\nWith the new rate change behavior in 2.186.0 the encoder will produce segments with a fixed number of frames. However, this comes with a restriction to the timing drift correction between the input and\nall\noutputs. For example, for a rate change from 24 fps to 23.976 fps, the rate change logic will take the input frames and modify their presentation times to the new framerate of 23.976 fps. Every repositioned frame adds a small drift error to the original input. This error will accumulate over time until it reaches a value of exactly 1 frame in duration (1 frame duration = 1 / fps). At this point in the video stream, the error will be corrected by dropping the corresponding input frame resulting in the expected output frame rate of 23.976 fps.\nSince only one error correction from the input to all outputs is supported, it is no longer possible to e.g. change the frame rate from 24 to 23.976\nand\non a second output stream convert the input frame rate from 24 to 25. The conversion from 23.976 to 24 requires a drift error correction but the conversion from 24 to 25 requires no drift error correction. The conversion from 24 to 25 requires only the duplication of 4 frames from the 96 input frames to reach the 100 output frames required for 25 fps and this doesn't require a drift error correction.\nThe most common use case of this error correction vs. no error correction is the usage of fraction fps and non-fractional fps.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/understanding-media-err-decode-in-google-chrome-with-encoded-content",
    "title": "Understanding MEDIA_ERR_DECODE in Google Chrome with Encoded Content",
    "text": "If your source content has a frame rate of 24.02, 25.02, or 30.02 the encoded content will have the same frame rate if not specified other in your encoding profile. Using such frame rates can introduce problems with the Google Chrome MSE. However, the content will play fine on IE9, Edge, Firefox.\nYou can resolve this issue also for Google Chrome if you set the frame rate of your encoded content to a fixed value of, e.g., 24, 25, or 30. You can easily do that by specifying the rate of the video streams for your codec configuration.\nOpen API SDK Java Example - Create an H264 Codec Configuration:\nJava\nH264VideoConfiguration config = new H264VideoConfiguration();\nconfig.setName(\"H264 - 1080p - 30 FPS\");\nconfig.setPresetConfiguration(PresetConfiguration.VOD_STANDARD);\nconfig.setRate(30.0);\nconfig.setHeight(1080L);\nconfig.setBitrate(4800000L);\n\nreturn bitmovinApi.encoding.configurations.video.h264.create(config);\nThis example is based on our\nOpen API SDK for Java\n, which is available on Github. Please see our\nOpen API SDK overview\nfor all our supported programming languages, and our\nGH API SDK Example Repository\nfor further examples.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/obs-studio-contribution-encoder",
    "title": "OBS Studio Contribution Encoder",
    "text": "Introduction\nOBS Studio\nis free, open source, easy to use encoder software to produce rich live content with lots of features. The software is popular among our customers for encoding live contribution streams.\nOBS Studio\nis available on this page:\nOpen Broadcaster Software\nNote:\nAt the time this tutorial was written, the OBS Studio version was 27.2.4. Other versions may differ slightly, but the principles are the same.\nTo use\nOBS Studio\nas a Contribution Encoder for Bitmovin, you will need to perform two steps:\nSet up the Live Encoding in Bitmovin Encoder\nSend the Contribution Stream from\nOBS Studio\nto Bitmovin Encoder.\nNote:\nIn this tutorial, we use OBS Studio to send an RTMP stream to Bitmovin Encoder. OBS Studio and Bitmovin Encoder also support the SRT protocol, which is out of scope of this tutorial.\nSetting up the Live Encoding in Bitmovin Encoder\nOn your local machine, open a browser and log into your Bitmovin account.\nSet up a Live Encoding in Bitmovin Encoder. You have multiple options to do this:\nFor a\nQuick Start\n, just open your Bitmovin Dashboard, go to\nENCODING->Live Encodings\nand go through the wizard by clicking on the blue\nCreate new live encoding\nbutton.\nIf you are\nalready familiar with Bitmovin Encoder\n, you can also proceed according to this tutorial:\nCreate a Live Encoding from an RTMP stream\nNow we need the\nStream URL\n(e.g.\nrtmp://a.b.c.d/live\n) and the\nStream Key\nof the newly started Live Encoding. To get them, in the dashboard, go to\nENCODING->Live Encodings\nand click on the newest encoding in the list. This opens the encoding’s page with the Live Encoding Ingest section, in which you see the\nStream URL\nand the\nStream Key\nto be used going forward.\nSend a Contribution Stream from OBS Studio to Bitmovin Encoder\nOn your local machine, start\nOBS Studio\n. (If you start OBS Studio after a fresh install, it automatically opens the auto-configuration wizard. If this happens, just close the wizard).\nOpen\nPreferences\n(or click on\nSettings\non the bottom right). This opens the Settings Dialog.\nIn the Settings Dialog, Click on\nStream\n. This opens a dialog with three configuration options and one checkbox, see screenshot.\nIn this dialog, choose the following:\nAs\nService\n, choose\nCustom\n.\nAs\nServer\n, type in or paste the\nStream URL\n(e.g. rtmp://a.b.c.d/live) defined by the Bitmovin encoding.\nAs\nStream Key\n, choose the Stream Key that you got from your Live Encoding page.\nLeave the\nUse authentication\ncheckbox unchecked (RTMP authentication is currently not supported by Bitmovin Encoder. If authentication is required for your production setup, we recommend using the SRT protocol).\nClick OK to close the dialog\nNow configure OBS to generate a Live Stream according to the manual, e.g. this Quickstart: Wiki - OBS Studio Quickstart | OBS . There are lots of tutorials and YouTube videos online that will be helpful learning OBS Studio.\nClick\nStart Streaming\nto begin sending the OBS stream to Bitmovin Encoder.\nIf OBS shows a green light and the bitrate it sends (in its bottom right corner), then the stream is successfully sent to Bitmovin Encoder.\nCheck in Bitmovin Encoder that the stream is succesfully received. Look at the\nLive Encoding Ingest\nsection and verify that the\nInput Stream Status\nis\nConnected\n.\nNote:\nIf OBS Studio does not show a green light and the live stream is not received by Bitmovin Encoder, make sure the Stream URL is correct and you can reach the IP address from the computer running OBS Studio.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/5fc996f-image.png",
      "https://files.readme.io/4953be0-image.png",
      "https://files.readme.io/82c3ae0-image.png",
      "https://files.readme.io/5b37152-image.png",
      "https://files.readme.io/6991759-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoder-1243-1560",
    "title": "Encoder 1.24.3 - 1.56.0",
    "text": "1.56.0\nReleased 2018-10-01\nAdded\nAdded flag to ignore duration trimming for input files shorter than the specified duration\nAdded support for timecode trimming for MXF files\nImprove error message reporting when an internal encoding error occurs\nCut segments for HLS manifests and insert custom tags\nAdded Audio Leveling support\nFixed\nFixed an issue with bitrate conformance when using HLSv3 and AES128 encryption\nFixed handling of per title encodings with PAR other than 1:1\nFixed a bug where encoding from MXF source failed\n1.55.3\nReleased 2018-09-17\nAdded\nRestart of running live encodings\nBurnt-In Time Code (BITC), taken from the SEI of the source file for every frame\nFixed\nImproved handling with large input files\nImproved playback on Roku and Tizen devices\nFixed a bug where the encoding failed with the resync at start feature\nFixed playback issue in chrome when adding custom timecode to MP4 muxing\nImproved handling of watermark image download\nImproved feedback for unsupported input file types\nImproved handling of legacy S3 buckets\nImproved support of MXF input files\nImproved handling of download from Azure storage with special characters\nFixed handling of interlaced source files packaged in MP4 containers\n1.54.0\nReleased 2018-09-03\nAdded\nCreate multi-period DASH manifests with an on-demand profile\nCreate multi-period DASH manifests with keyframes\nImproved handling of input paths that contain URL encoded characters\nFixed\nFixed an issue where the framerate might be doubled when the deinterlace filter is used\nIssue fixed where an audio that is encoded with a sample rate below the input sample rate caused an error\nFixed an issue where the passthrough of 608 captions may not work as expected\nKnown Issues\nEncoding job might fail for 4k high bitrate files\n1.53.0\nReleased 2018-08-19\nAdded\nAdded support for\nHEVC\nwith\nFairPlay DRM\nAllow usage of\npre-warmed\ninstances\nAdded\nunsharp\n,\ninterlace\nand\nscaling\nfilter\nAdd\nmultiple audio groups\nto an\nHLS\nstream to be used together with\nstream conditions\nIntroduced new\naudio video sync mode\nthat is useful if the source file was cut out of already encoded content.\nFixed\nFixed an issue where\nHTTP Basic authentication\nwas not working for some endpoints\nFixed a bug where audio in\nMP4\nfiles could not be decoded when multiple input files have been used\nFixed an issue where\nHLSv3\nin combination with\nAES-128\nencryption failed for a few input files\n1.52.1\nReleased 2018-07-30\nAdded\nSupport for multiple input files (e.g., one video file and multiple audio files)\nSupport\nS3 storage\nwith\nrole-based authentication\nfor\nprogressive muxings\ntypes\nFixed\nFixed an issue where\nHTTP basic authentication\nmight not work for some hosts\nImproved status messages when creating\nBroadcast TS muxings\n1.51.0\nReleased 2018-07-16\nAdded\nSupport for\nPer-Title\nencoding with\n2pass\nand\n3pass\nencoding\nAdded option to remove all position information when converting\nSCC\nto\nWebVtt\nAdded success/error message when creating\npreview manifests\nAdded timecode support for\nMP4 muxing\nAdded\naudio language metadata\nto\nprogressive muxings\nFixed\nFixed a bug where\nvariable input fps\nhandling might not work as expected\nFixed a bug where valid video files may be recognized incorrectly as\ninput file format not supported\n1.50.0\nReleased 2018-07-03\nAdded\nSupport for\nAV1\nSupport for\nPer-Title Encoding\nSupport for\nS3 V4\nsignature buckets\nAdded\nnumberOfFrames\nand\naspectRatio\nto\nmuxing information\nFixed\nFixed a bug where\nwatermark filters\ncould not be applied for certain configurations\nKnown Issues\nPer-Title\ndoes not work yet with\n2pass\nand\n3pass\nencoding\n1.49.0\nReleased 2018-06-25\nAdded\nSupport for\ntwo-pass\nand\nthree-pass\nencoding\nSupport for\nSCC\nto\nWebVtt\nconversion\nSupport for\nSCC\nto\n608/708\nconversion\nFixed\nImproved transcoding resilience\nFixed an issue where manifest creation fails when\nHEVC\n,\nTS\n, and\nHLS\nis used\nFixed an issue where the\nFPS\nfrom a live stream might be detected incorrectly\n1.48.0\nReleased 2018-06-18\nAdded\n608/708 Passthrough\nsupport\nDASH On Demand\nProfile\nNew Fonts for\nText Filter\nMulti-Channel Audio\nFixed\nFixed a bug where the encoding fails when validating the output if\nVP8\nand\nH264\nare encoded at the same time\nFixed a bug where the analysis of an input file might fail\n1.47.3\nReleased 2018-05-29\nAdded\nAdded\nMP3 muxing\nAdded\nEnhanced Watermark Filter\nAdded SBR signaling to\nHE-AAC\ncodec\nAdded\ndynamic font size\nfor\ntext filter\nAdded custom port for SSH when using\nAWS connect\nAdded property to create multiple files for\nsprites\nChanged\nImproved download speed from\nS3 Inputs\nFixed\nFixed an issue that might cause wrong audio duration for\nBroadcast TS Muxing\nFixed an error where the download step might stall\n1.46.1\nReleased 2018-05-15\nAdded\nAdded\nBroadcast TS\nMuxing\nFixed\nFixed an error where watermark download fails for certain\nHTTP\nstorages\nFixed an audio sync issue that could occur when using\nMXF\nfiles and\nXDCAM\nFixed an issue that the encoding may fail when the video input codec is\nqtrle\n1.45.0\nReleased 2018-05-06\nAdded\nSupport for\nHE-AACv1\nUp to 3x faster download of files from\nS3 Inputs\nFixed\nFixed an issue where an encoding might fail when\naudio stream conditions\nare used\nLive stream fails fast now if files cannot be written to the output storage\nFix for sub-second GOP length for progressive outputs.\nFixed a bug for timecode trimming which occurred for some files.\n1.44.0\nReleased 2018-04-22\nAdded\nAdded\nmuxing information\nfor MOV muxings\nSet\nDTS offset\nfor\nTS muxings\n1.43.3\nReleased 2018-03-29\nAdded\nSupport for new AWS regions:\nUS_EAST_2\n,\nCA_CENTRAL_1\n,\nEU_WEST_2\n,\nEU_WEST_3\n1.42.0\nReleased 2018-03-29\nAdded\nCreate\npreview manifest\nof running VoD encodings\nImproved validation to check if watermark file is downloadable\n1.41.0\nReleased 2018-03-25\nAdded\nAdded new\nstream condition\nto check for the\nduration\nof the input file\nImproved error message when all streams are removed due to\nstream conditions\nAdded port to\nHTTP\nand\nHTTPS\ninput\nFixed\nFixed a bug where thumbnails created at 100 percents may fail\n1.40.0\nReleased 2018-03-15\nAdded\nAdded\nText\nFilter and\nBurn In Timecode\nFilter\nAdded unit `percents for specifying sprite distance\nImproved error messages for invalid input file types\nFixed\nFixed a bug that might cause an error for specific input files when checking for the\nfps\nin the\nstream conditions\n1.39.0\nReleased 2018-03-05\nAdded\nAdded support for\nS3 v2\nSignature for input\nAdded\nstream condition\nfor\ntotal stream count\n,\nvideo stream count\nand\naudio stream count\nFixed\nFixed a bug where muxing to\nProgressive TS\nwith video only may fail\n1.38.0\nReleased 2018-03-01\nAdded\nCustom\nsample aspect ratio\ncan now be applied\nImproved scheduling of\non-premise\nencodings to increase speed and cluster utilization\nFixed\nFixed an issue where frames might get dropped if the input file is\nOpen GOP\nFixed a bug where\nthumbnails\nmight not be transferred to\nakamai\nendpoints\nFixed a bug which caused encodings with\nVP9\ninput files to fail\n1.37.0\nReleased 2018-02-19\nAdded\nImproved speed for\nVP8\nencoding\nFixed\nFixed an issue where frames might get dropped if the input file is\nOpen GOP\nFixed an issue where output duration might be wrong for\nMP4\nmuxings\nFixed an issue where output duration of manifest might not match the duration of the video stream for\nHLSv3\nmuxings\n1.36.0\nReleased 2018-02-11\nAdded\nIncreased speeds for progressive muxings\nFixed\nImproved stability for H264 encodings\nImproved acquiring of instances to speed up cluster creation\n1.35.0\nReleased 2018-01-25\nAdded\nStream conditions can now be configured to only remove the steam instead of the whole muxing if it evaluates to false\nAdded option to provide input color space and range\nAdded support for\naspera token authentication\nFixed\nFixed an issue where thumbnail creation fails with some MXF files\n1.34.0\nReleased 2018-01-14\nAdded\nAdd\nrelative position\nsupport for\ncrop filter\nAdd\nMD5 Hash Header\nfor progressive outputs written to S3\nFixed\nBugfix for an internal race condition occurring with some H264 input files which caused the encoding to fail\nEnhanced open GOP detection to avoid frame glitches and audio/video sync issues\n1.33.1\nReleased 2018-01-04\nAdded\nAdded\nDenoise Hqdn3d\nfilter\nFixed frame rate\nfor\nprogressive MP4\nand\nprogressive TS\nFixed\nBugfix for\non-premise\nwhere the worker nodes have not registered on the master node in certain conditions\n1.32.0\nReleased 2017-12-27\nAdded\nAdded\nprogressive MOV\nmuxing\n1.31.0\nReleased 2017-12-18\nAdded\nIntroduced\npre-warmed\npods for\non-premise\nencoding to reduce enqueue time\nTransfer checks can be disabled\nFixed\nFixed a bug where the encoding fails when\noutputPath\nwas not set\n1.30.0\nReleased 2017-12-11\nAdded\nDefault settings for ACL were different for segmented and progressive output\n1.29.0\nReleased 2017-11-29\nAdded\nAdded\nKeyframe Archive\n1.28.1\nReleased 2017-11-23\nAdded\nAdded\nscenecut threshold\noption for\nH264\nand\nH265\ncodec configurations\nFixed\nEncoding failed with an MXF input and an output audio sample rate of 44.1kHz\nSegment size was ignored for TS muxings\n1.27.0\nReleased 2017-11-15\nAdded\nAdded new video codec:\nVP8\nAdded new audio codecs:\nAC3\n,\nE-AC3\n,\nMP2\n,\nMP3\n,\nOpus\n,\nVorbis\nAdded\nprogressive WebM\nmuxing\n1.26.3\nReleased 2017-11-07\nAdded\nAdded API endpoint to get muxing information for\nprogressive TS\nand\nprogressive MP4\nFixed\nEncoding failed with certain MXF files due to an invalid handling\nImproved acquiring of\nMicrosoft Azure Cloud\ninstances\n1.25.3\nReleased 2017-10-23\nAdded\nEncodings can now run on Azure cloud\nAdded audio stream conditions for:\nBitrate\n,\nLanguage\n,\nChannel Format\n,\nChannel Layout\nFixed\nChannel Format for stream conditions has been changed to a Long value\nEncoding failed when using audio merge in common with MXF input",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/get-started-with-the-bitmovin-api",
    "title": "Get started with the Bitmovin API",
    "text": "Introduction\nAs a developer the Bitmovin API will become the center of your video infrastructure universe, as it allows you to manage everything about your encodings, player, and analytics, as well as your Bitmovin account itself. In this tutorial we will guide to you through the first steps on how to communicate with the Bitmovin API, so you can start to create your first encoding right away.\nCreate an Bitmovin Account\nIf you haven't already, please create an Bitmovin account by clicking on \"Sign Up\" in the upper right corner or visit\nhttps://bitmovin.com/dashboard/signup\n. Once activated you will have access to your own dashboard where you can easily manage your encodings, player/analytics licenses, team members and more. Further, you will be able to manage your API Key, which is needed to interact with our Bitmovin API, which allows you to control your complete encoding workflow programmatically.\nGet your Bitmovin API Key\nOnce logged in your Bitmovin account at\nhttps://bitmovin.com/dashboard\ngo to your\naccount settings\n. There you will find your Bitmovin API key which is needed in order to communicate with our REST API.\nSend your first request to the Bitmovin API\nAt this point you created an Bitmovin account, chose an API client, and found your API key in your account. So by now you have everything needed to send your first API request.\nEvery request is sent to your API as to our API endpoint at\nhttps://api.bitmovin.com/v1/\nusing GET, POST or DELETE as method. All responses are formatted using JSON and follow a specific\nresponse message format\n.\nCURL\nThe following request will list all encodings that are available for your account. As you might have not done an encoding yet, the response will look like the following:\nRequest\nShell\ncurl -X GET https://api.bitmovin.com/v1/encoding/encodings -H 'x-api-key: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxx'\nResponse\nJSON\n{\n    \"requestId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxx\",\n    \"status\": \"SUCCESS\",\n    \"data\": {\n        \"result\": {\n            \"totalCount\": 0,\n            \"previous\": \"https://api.bitmovin.com/v1/encoding/encodings?offset=0&limit=25\",\n            \"next\": \"https://api.bitmovin.com/v1/encoding/encodings?offset=25&limit=25\",\n            \"items\": []\n        }\n    }\n}\nChoose an API Client\nDue to the vast amount of features in our API there are as many REST API requests available, that are documented in our\nAPI reference\n.\nIn order to ease and speed up your development, and to save you from spending your valuable time on repetitive tasks, we offer our Open API SDK for multiple programming languages like Java, Javascript, Python and\nmore\n.\nE.g. using our\nOpen API SDK for Java\nto perform the same request as in the previous example getting a List about all encodings, it would look like the following:\nJava\nBitmovinApi bitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_API_KEY_HERE\").build();\nbitmovinApi.encoding.encodings.list();\nIf you are new to encoding then Python may be the easiest to start with, our\nGetting Started Guide for Python\ncontains a walkthrough video, code samples and other tricks for quick deployment.\nIts that simple ! These SDK's allow you to quickly start using our Services as well as integrate them into your new or existing applications/projects, to get your video encoding workflow setup in no time.\nAn overview about all available API clients can be found\nhere\nin our documentation, including links to their respective Github repository. See our\nexamples on Github\nshowcasing how to use each of our Open API SDK's!",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rest-api-services-1500-1990",
    "title": "REST API Services 1.50.0 - 1.99.0",
    "text": "1.99.0\nAdded\nPlaceholders in the output path can now be used with the\nSimple Encoding API\n:\n{uuid}\nor\n{asset}\nwill be replaced either with a random UUID or with the name of the asset provided as input (for VIDEO or DEFAULT input type). More examples can be found\nhere\nInterlaced video input to the\nSimple Encoding API\nwill now be deinterlaced automatically.\n1.98.0\nAdded\nA brand-new way to interact with our encoder - the\nSimple Encoding API\nWith the Simple Encoding API you only need a single\nAPI call\nproviding an input source and an output destination to start an encoding.\nYou can find more information about the template and configuration possibilities\nhere\n.\nThis API is especially meant for everybody looking to quickly prepare videos for OTT streaming (DASH and HLS) following industry best practices. Sensible defaults, no micro-optimizations.\nUnder the hood, this API is utilizing the same high performance encoding engine used by leading media brands, automatically creating the best ladder for each asset with our award-winning per-title.\n1.97.0\nAdded\nHLS Default Manifests\nuse the language of the audio stream language (\nstream.metadata.language\n) if defined.\nFixed\nFixed a potential encoding stall for HDR to SDR conversions with files below 10 seconds.\nFixed a bug where Prewarmed Encoder Pools in combination with Cloud Connect could lead to very slow encodings.\n1.96.0\nFixed\nFixed a bug for the\nVoD\nand\nlive\nstatistics endpoints with time ranges where offsets led to an empty result.\n1.95.0\nAdded\nSupport for adding\nCustomTags\nto\nSubtitle Media Infos\nwhich reference a\nChunkedTextMuxing\n.\nChanged\nAV1 encodings:\nSpeed - Encodings run now\n5x faster\n.\nQuality - Encodings now show appr.\n30% higher quality\nwhen compared to HEVC/VP9 encodings.\nCost - Encodings are now significantly more cost effective. See our\nEncoding Minute Calculation Methodology\nfor detailed pricing information.\nAdded\nstartOffset\nto TextMuxing and ChunkedTextMuxing. This is used for setting the MPEGTS value for the RFC 8216 (HLS Specification) X-TIMESTAMP-MAP for WebVtt outputs (e.g.:\nX-TIMESTAMP-MAP=MPEGTS:900000,LOCAL:00:00:00.000\n).\nWebVttConfigurations with CueIdentifierPolicy INCLUDE_IDENTIFIERS will now generate CueIds starting from\n1\ninstead of\n0\nif the FileInputStreamType is WEBVTT.\nCueIdentifierPolicy default for WebVttConfigurations for inputs with FileInputStreamType TTML and SRT. The default is now INCLUDE_IDENTIFIERS instead of OMIT_IDENTIFIERS.\nFixed\nAdded support for adding DASH\nLabel\nelements to\nVideo\n-,\nAudio\n-\nSubtitle\n- and\nImage\nAdaptation Sets.\nFixed an issue where streams that were ignored by stream conditions are not indicated as being ignored (i.e., the\nignoredBy\nis empty).\nStreams with WebVttConfigurations will now be trimmed with the same offset and duration as the video stream of the same Encoding if the FileInputStreamType is WEBVTT (This works now same as TTML and SRT FileInputStreamTypes already used to).\nFixed an issue where output subtitle cues' timestamps were shifted by the value of the first video PTS, if FileInputStream is used for the subtitle stream\nKeyFrames are now working for streams with WebVttConfigurations\n1.94.0\nAdded\nPreset configurations for setting\ndynamicRangeFormat\n:\nSDR\nfor\nH264\nHLG\nand\nSDR\nfor\nVP9\nDeprecated\nRemoved deprecated\nAV1 API parameters\nwhich had no effect on visual quality but lead to higher encoding times.\n1.93.0\nAdded\nSupport for adding an\nAAC\nor\nDolby Atmos\naudio stream along with a Dolby Vision video stream to a\nMP4 muxing\n.\nCloud regions\nAZURE_GERMANY_WESTCENTRAL\n,\nAZURE_EUROPE_NORTH\nand\nAZURE_UAE_NORTH\nare now supported.\nAZURE_UAE_CENTRAL\nis only accessible for UAE North customers requiring in-country disaster recovery (as per Microsoft), so it can only be used for Azure connect by those customers.\nFixed\nCorrected the documentation for\nBIF distance\nwhich was not marked as required.\n1.92.0\nAdded\nSupport\nDolbyVisionInputStream\ns together with other input stream types like\nIngestInputStream\n,\nAudioMixInputStream\n, etc.\n1.91.0\nFixed\nInvalid private keys are now reported correctly when using Cloud Connect on GCP\n1.90.0\nAdded\nWith this release we've improved the encoding capabilities with regards to HDR conversions for H265 output. From now on we support\nDolby Vision to HDR10\nDolby Vision to SDR\nHDR10 to HLG\nHLG to HDR10\nSDR to HDR10\nSDR to HLG\nThe encoder detects the applicable conversion based on the input and the configured output color settings. To make it easier to correctly configure the output we have added preset configurations for the different dynamic range formats. These can be configured on the\nH265\nresource via the setting\ndynamicRangeFormat\n.\nDolby Vision input can be configured with a\nDolby Vision input stream\n1.89.0\nAdded\nSupport for setting\nduration=null\nfor\nTime-based Trimming\nto indicate that the whole stream should be used (considering offset).\noffset=null\nwill be defaulted to zero now. We also fixed and issue when setting\nduration=0\nwhich lead to different results for video and audio. Setting\nduration=0\nwill now lead to 0 frames for video and audio.\nManifest Generation now supports HDR10 and HLG for DASH for progressive muxings.\nFixed\nFixed the return type for the endpoint\nList prewarmed encoder pool schedules\n1.88.0\nAdded\nDash Manifest generation for segmented HDR Output is now setting\nessentialProperties\nand\nsupplementalProperties\ncorrectly for HDR10 and HLG compatible settings\n1.87.0\nAdded\nAdded EXPLICIT_PS option to HeAacV2Signaling. The option EXPLICIT_SBR now signals Parametric Stereo (PS) implicitly.\nImproved descriptions for HeAacV1Signaling and HeAacV2Signaling.\nAutomatic shutdown of a live encoding after a certain period of time, configurable via the\nautoShutdownConfiguration.streamTimeoutMinutes\nconfiguration when starting a\nLive Encoding\n.\nAutomatic shutdown of a live encoding if input is lost and does not reconnect within a certain time period, configurable via the\nautoShutdownConfiguration.bytesReadTimeoutSeconds\nconfiguration when starting a\nLive Encoding\n.\nFail fast if not all video streams use the same input configuration.\nFail fast if video streams are configured to have more than one stream input (except in the case of CEA 608/708 captions passthrough, where this is expected)\nFixed\nWebhook\nfor\nENCODING_STATUS_CHANGED\nnow can be provided with\ncustomData\n(similar to webhook\nENCODING_FINISHED\nor\nENCODING_ERROR\n).\n1.86.0\nAdded\nAdded new property for DASH manifests which defines the compatibility of the manifest with the Standard DASH Edition. Setting the dash edition compatibility to V4 will support the endNumber attribute for SegmentTemplate DASH manifest, specifying the last available segment. This feature is supported when creating a new DASH manifest via the\nstartEncoding call\nand configuring the V2 ManifestGenerator OR when creating the manifest via the\nDASH manifest creation endpoint\nFixed\nGO API SDK\nEndpoints with optional request bodies like the\nStart Encoding endpoint\nare now generated as two methods. One will not accept a request body, the second one does accept a request body parameter and will have the suffix\nWithRequestBody\nin the name.\nJava API SDK\nFor endpoints with optional request bodies like the\nStart Encoding endpoint\nan additional method overload will be generated which does not accept a request body parameter.\n1.85.0\nAdded\nDRM is now supported for\nMP4 muxings\nwith\nDolby Digital\nand\nDolby Digital Plus\naudio codecs.\n1.84.0\nAdded\nRegion\nus-west2\nis now supported in GCE cloud (\nGOOGLE_US_WEST_2\n)\nChanged\nRemoved the deprecated manifest generator\nV2_BETA\noption from our API SDKs. For generating manifests\nduring the encoding process\nplease use\nV2\n.\nFixed\nEncoding cancelation will now trigger the webhook\nEncoding Status Changed\n1.83.0\nAdded\nAdded fail fast when using\nAC3\nor\nE-AC3\nwith\nTS muxing\nand DRM as this is not supported\nImplemented support for the DTS:HD and DTS:X audio codecs. The openAPI can be found\nhere\n. This feature is compatible starting with the 2.88.0 encoder version. Only MP4 and internally fragmented muxings are supported for DTS:HD/DTS:X. DRM configurations are not supported with DTS:HD/DTS:X codecs.\n1.82.0\nAdded\nAdded a new AudioVideoSyncMode\nRESYNC_AT_START_AND_END\nto the\nStartEncodingRequest\n. This mode pads audio streams with silence, if the audio streams are shorter than the video stream. This prevents DASH clients trying to download non-existent audio segments, if the mediaPresentationDuration is longer than the duration of the audio stream. This new mode is now also the default value.\nChanged\nAdded Fail Fast for manifestGenerator\nV2\nwith encoder versions < 2.70.0 (using it with encoder version >= 2.86.0 is recommended).\nManifestGenerator\nV2\nis the default for new customers going forward.\n1.81.0\nAdded\nManifest generator\nV2\nis now generally available and supports all manifest features of our API.\nIt can be used by directly setting manifest IDs in the\nstart encoding request\nand setting\nmanifestGenerator\nto\nV2\n.\nWe are therefore deprecating the manifest generator\nV2_BETA\noption and will remove it from our API SDKs in release\nv2.89.0\n.\nFor more information see our tutorial\nhere\n.\nImproved validation for Per-Title configurations using multiple templates with fixed resolution, fixed bitrate and bitrate selection mode\nCOMPLEXITY_RANGE\n: Encodings now fail fast if the difference between the minimum or the maximum bitrate of consecutive streams is smaller than the configured minimum bitrate step size.\n1.80.0\nAdded\nAdded support for Thumbnail Letter and Pillarboxing. Supported aspect modes:\nCROP\n,\nPAD\n,\nSTRETCH\n. More Details described in the API Calls:\nBIF\nSprite\nThumbnail\nFixed\nWhen\nretrieving encodings\nand using the parameter\nsearch\nin combination with sorting by\nfinishedAt\na 500 status code was returned\nWhen using an incorrect manifest ID (or one of a different type) in the\nEncoding Start\na proper error message is returned now instead of an Internal Server Error response.\n1.79.0\nAdded\nIMSC as subtitle output format\nfrom TTML and SRT inputs. For TTML styling passthrough is available.\nWarning when creating or using a\nH264 Configuration\nwith settings or a preset that is not compatible with the selected profile.\nadaptiveSpatialTransform=true\nonly works with profile HIGH.\nbframes > 0\n,\ncabac = true\n, and\nweightedPredictionPFrames = true\nare only working with MAIN or HIGH.\nFixed\nFor\nDASH manifests\n, containing Dolby audio renditions, the\ncodecs\nattribute is not set on\nAdaptationSet\nlevel anymore but only on the\nRepresentation\nlevel. This also fixes a rare case of duplicate entries in the\ncodecs\nattribute which could have led to device compatibility issues.\n1.78.0\nAdded\nDolby Digital\nand\nDolby Digital Plus\ncodec configurations are now available as replacement for AC3 and EAC3.\nWe are therefore deprecating\nAC3\nand\nEAC3\nas the two new codec configurations are producing output, which conforms to Dolby's high certification standards.\nOur\nmulti-codec example\nnow uses Dolby Digital instead of AC3.\nFor more information see our tutorial\nhere\n.\nSprite\ngeneration no longer requires both width and height to be set, as long as one of the two values is configured, the other one is automatically computed based on the aspect ratio of the video.\nThumbnail creation\nhas been extended to allow the same options with regards to height and width as sprite generation.\nChanged\nWe improved the error handling and retry behaviour in case of scheduling failed. Previously unrecoverable errors could take 40 minutes to detect, which is no longer the case.\nFixed\nFixed LIVE encodings reporting too low (occasionally negative) statistics after a restart\nFixed a bug that could cause billing address changes to fail.\nKnown Issues\nAdaptationSets\nin\nDASH manifests\ncould contain the same codec multiple times in the\ncodecs\nattribute, when the manifest contains a Dolby Digital, Dolby Digital Plus or a Dolby Atmos rendition.\n1.77.0\nAdded\ntargetDurationRoundingMode\nproperty on\nHLS manifest\n, which defines the rounding mode for the target duration (normal or upward rounding) for manifests generated during the encoding.\n1.76.0\nAdded\nIf we cannot\nrequest\nVM instances the api returns now more detailed error messages regarding the root cause of a \"Scheduling failed\" for Cloud Connect customers .\n1.75.0\nAdded\nIf we cannot\nconfigure\nVM instances the api returns now more detailed error messages regarding the root cause of a \"Scheduling failed\" for Cloud Connect customers .\nChanged\nWhen creating a\nConcatenationInputStream\n, the\nposition\nof each contained\nConcatenationInputConfiguration\nnow needs to have a unique value\nPHP API SDK\n:\nNow supports PHP 8\nFixed\nImproved the SCTE35 cue tags insertion so the time frame between the cue tags is always bigger or equal to the configured cue duration.\nPreviously it was possible, that encodings started with prewarmed encoder pools did not use a prewarmed instance even though it was available\nFixed a concurrency issue which could lead to invalid state when calling\nstart on prewarmed encoder pools\nin quick succession\n1.74.0\nAdded\nAdded support for SCTE-35. When running a live encoding, calling this\nendpoint\nwill trigger the insertion of cue tags in the provided HLS manifests and ads will be inserted.\nNote: this is an experimental feature.\nFixed\nFixed an issue which led to a cascade of scheduling failed error\n1.73.0\nAdded\nAdded\ntargetDurationRoundingMode\nproperty on\nHLS manifest\n, which defines the rounding mode for the target duration (normal or upward rounding).\nFixed\nA validation for live edge offset in HLS Manifests introduced with v1.72.0 could falsely fail, if a CC caption has been used as media stream.\nFixed incorrect audio manifest relative path when generating audio-only HLS manifest with MP4 muxing.\n1.72.0\nChanged\nImproved stability for encodings on Google Cloud.\nFixed\nAdded validation for Live Edge Offset in HLS Manifests: Live Edge Offset now has to be >= than the segment length of their assigned muxing\nFixed concurrency issue for manifest status updates from encoders (introduced in\nEncoder v2.75.0\n) that could lead to inconsistent status\nFixed manifest status endpoints (\nDASH\n,\nHLS\nand\nSmooth\n) to return a spec conform\nservicetask\nproperty.\n1.71.0\nAdded\nAdded support for additional\nSprite\nfeatures so that they can be used for trick-play with DASH manifests. It's now possible to additionally specify the tile format (\nhTiles\n/\nvTiles\n), JPEG quality and a creation mode.\nAdded\nImage adaptation sets\nfor DASH manifests with\nSprite representations\nto enable video players to provide tiled thumbnails based on\nSprites\n.\n1.70.0\nFixed\nStability improvements for Azure encoding clusters.\nChanged the\nrate\nproperty of\nStream Infos of Live Statistics\nin the API specification and SDKs from\ninteger\nto its actual type\ndouble\nPHP API SDK\nFixed an issue in\nErrorHandler\nclass that prevented an update to PHP 7.2\n1.69.0\nAdded\nUpdates in the internals of the TTML to WebVTT subtitle conversion:\nAdded support for basic styling in TTML to WebVTT subtitle conversion, configurable via\nWebVttConfiguration\nAdded support for\nChunkedTextMuxing\nin TTML to WebVTT subtitle conversion: allows to output segmented WebVTT files\n1.68.0\nAdded\nVP9 Codec Configurations\nnow support 10-bit\npixelFormat\nChanged\nInstead of failing during the encoding process\nthe\nAdd Dolby Vision Metadata\ncall will fail if\nenableHrdSignaling\nof the Stream's Codec Configuration is not set to\ntrue\nthe\nCreate VP9 Codec Configuration\ncall will fail if\npixelFormat\nis not any of\nYUV420P, YUV422P, YUV440P, YUV444P, YUVJ420P, YUVJ422P, YUVJ440P, YUVJ444P, YUV420P10LE, YUV422P10LE, YUV440P10LE, YUV444P10LE\nFor\nTTML\nto\nWebVTT subtitle\nconversion with\nChunkedTextMuxing\nsetting\ncueIdentifierPolicy\nto\nOMIT_IDENTIFIERS\nis now mandatory\nFixed\nImproved reliability of encoding startups\n1.67.0\nChanged\nImproved resiliance against server or network errors for manifest uploads\nDRM keys are now hidden in the response as default.\nFor\nSRT\nto\nWebVTT subtitle\nconversion with\nTextMuxing\nsetting\ncueIdentifierPolicy\nto\nOMIT_IDENTIFIERS\nis now mandatory\n1.66.0\nAdded\nRegion\neurope-west3\nis now supported in GCE cloud\nChanged\nEncoder Versions below 2.53.0 are outdated for Azure cloud and will automatically be set to STABLE when executing an encoding.\nFixed\nSDKs deserializing a WebVttSidecarFile as SidecarFile when retrieved via\nList Sidecars\n1.65.0\nAdded\nBroadcast TS muxings\ncan now configure the rate for subtitle streams\nBroadcast TS muxings\ncan now configure the PID for subtite streams\nAfter multiple, failed, consecutive attempts to deliver a pending notification to the endpoint of the configured Webhook, the Webhook will get auto-muted and no further notifications will be sent until the Webhook becomes reachable again and the customer has un-muted the notifications (see\nFAQ\n). When a Webhook is auto-muted, the customer will be immediately informed through an email.\nV2 manifest generator (BETA)\nThe V2 manifest generator is a new implementation that is used for DASH and HLS manifests created as part of the encoding process (SMOOTH manifests are unchanged and supported as before).\nThe new manifest generator can be used by setting the ManifestGenerator parameter in the\nStartEncodingRequest\nto V2_BETA. The new manifest generator is currently in BETA and will receive new features frequently. To make use of the new generator, list the manifests you want to have generated automatically in the\nvodDashManifests\nand\nvodHlsManifests\nproperties of the\nStartEncodingRequest\n.\nVarious bugs have been fixed when manifests were generated during the encoding.\nHLS\nAdded support for Widevine and CENC Drm\nImproved TARGET-DURATION calculation\nAVERAGE-BANDWIDTH, CLOSED-CAPTIONS attributes are now set correctly\nImproved precision of BANDWIDTH attribute\nDASH\nFixed a bug where the channel layout was incorrectly chosen for some Audio AdaptationSets for DASH manifests.\n1.64.0\nFixed\nStability improvement for Azure Encoding Cluster startup and shutdown\n1.63.0\nChanged\nWebhook calls\nnow have a read timeout of 10s and a connection timeout of 5s and are retried 10 times at most.\n1.62.0\nAdded\nExpose the\nanalysisDetails\nproperty of Stream\nStreamInput\nresources in the SDKs\n1.61.0\nChanged\nImproved handling of provisioning and unprovisioning of machine instances for encodings in Google Cloud regions when facing rate limiting from the Google Compute API.\nFixed\nList all Muxings\nendpoint not returning\ntype\nproperty for\nPACKED_AUDIO\n,\nTEXT\n,\nCHUNKED_TEXT\nand\nMXF\nmuxings\n1.60.0\nAdded\nOn encodings that fail with licensing errors, the corresponding retry hint is set in the\nstatus response\n.\nChanged\nDecreased response times for\nencoding get list calls\nby up to 90%\nFixed\nWhen using\ncolorConfig\non\nH264\n,\nH265\nor\nVP9\nconfigurations with a codec preset, it will not be classified as a custom preset anymore\nFixed an error when creating an\nHLS\nor\nDASH default manifest\nwithout specifying the\nversion\nproperty. The default\nV1\nwill now be set.\n1.59.0\nAdded\nPacked Audio\nnow supports segment naming templates.\nChanged\nLimited retries of firing of a webhook to a maximum of 5 minutes.\nOn-demand DASH manifests\nwill fail with a proper error message if no representations are configured.\n1.58.0\nAdded\nSecuring your assets with\nNagra NexGuard FileMarker A/B Watermarking\nis now available\nSupport for\nStatic IPs\nfor Live Encodings running on Google Cloud regions.\nCloud Connect Support for\nStatic IPs\nfor Live Encodings running on Google Cloud and AWS regions.\nSegmented WebVTT subtitles now can be\nconfigured\nto either omit or include WebVTT cue identifiers (included by default). Please note that the following restrictions apply:\nSRT to WebVTT conversion workflow does not support cue identifiers as of now. Therefore it needs to be set to\nOMIT_IDENTIFIERS\n.\nCue identifier can not be configured for\nnon-segmented\nWebVTT outputs.\nAPI SDK\nexamples\nAdded support for setting the tenant organisation ID into all examples, for multi-tenant or sub-organisation scenarios\nNew\nJava examples\nin support of the tutorial on\nseparating and combining audio streams\nChanged\nChanged default branch\nmain\nfor all API SDK repositories\nWe decided to rename the default branch for all API SDKs to\nmain\n. See\nGitHub's support page\nfor more information and guidance on how to update a local clone of these repositories.\nFixed\nSupport for\nGenericS3 output\nwith SSL and signature version V2 for manifests uploaded via the manifest start endpoints:\nDASH\n,\nHLS\n, and\nSmooth\n.\nFixed bug that can lead to stuck manifest generations.\nRemoved security critical fields from the response in the\nQuery SPEKE configuration\nendpoint.\n1.57.0\nAdded\nSupport for\nPacked Audio Segments\n: Allows to create Packed Audio Segments containing encoded audio samples and ID3 tags that are packed together with minimal framing and no per-sample timestamps.\nAdded configuration of PCR interval for\nBroadcast TS\n.\nFixed\nVersion check when using SRT file input to prevent using an incompatible version prior to 2.60.0.\nFixed SPEKE issue where the initialization vector was incorrectly encoded during the key exchange leading to an invalid initialization vector for stream encryption.\nFixed an issue with failed encodings when using\nchannelLayout\nin\naudio mix input stream\n1.56.0\nAdded\nIntroduced new default encoder error for encodings that failed because of an unknown reason. We now will return the\nretryHint\nUNDEFINED\nfor these encodings at the\nencoding status endpoint\nThe\nstatus\nof the static IP is being exposed now\nAdded fail-fast when clear and DRM muxings are configured with the same output\nFixed\nAvailable encoding slots may have been calculated incorrectly when encoding in multiple sub-organizations at the same time\nWhen using\nSPEKE\nthe signature of a key exchange request during the last week of a year included the upcoming year.\n1.55.0\nAdded\nSupport up to 16\nPCM streams\nfor\nMXF muxing\nAdded DVB-SUB subtitle passthrough for BroadcastTsMuxing.\nAdded SRT To DVB-SUB subtitle conversion for BroadcastTsMuxing.\nAdded support for\nSRT file\nto\nWebVtt subtitle encoding\nwith\ntext\n/\nchunked-text muxing\n.\nEarly Access for\nStatic IPs\nfeature which allows reusing of IP addresses for consecutive live streams in AWS (please get in touch to try it)\nChanged\nMade Manifest Generation more resilient by adding some retries.\nFixed\nChanged crop filter to allow only zero or positive values.\n1.54.1\nAdded\nAdded new\nencoding status changed webhooks\n. For\nconditions\n, the following attributes are possible:\ntype\n: 'Input file download', 'Input file analysis', 'Per-Title analysis', 'Encoding', 'Progressive Muxing'\nprogress\n: number in range of 0-100\nstatus\n: 'RUNNING', 'FINISHED', 'ERROR'\nExamples:\nTo only get notified about the encoding process, create a Condition object and set attribute='type', value='Encoding', operator=EQUAL\nTo only get notified if a workflow step is over 50%, create a Condition object and set attribute='progress', value='50', operator=GREATER_THAN\nTo only get notified if a workflow step is finished, create a Condition object and set attribute='status', value='FINISHED', operator=EQUAL\n1.54.0\nAdded\nEncodings\ncan now be filtered and sorted by the property\nfinishedAt\n1.53.0\nAdded\nAdded\nexternalIdMode\nfor\nS3RoleBasedOutput\n,\nS3RoleBasedInput\nand\nSpeke Provider\n.\nCustomers using their own\nAzure account\ncan now opt-in, so that all SSH communication comes from one specific IP address. This enables a more rigid network security policy where only one IP is allowlisted for SSH connections.\nEarly access for\npre-warmed encoder pools\nwhich eliminate queuing times for scheduled encodings (please get in touch to try it).\nFixed\nHLS Manifests can now be\nconfigured\nto add the channels attribute in a standard conform way.\nRemoved unused\nencryption\nproperty from\nwebhook resources\n.\nDAR settings for\nH265\nare now applied correctly.\n1.52.1\nFixed\nGo API SDK\nCode formatting was off in release\n1.52.0\n, this is now fixed again\nPython API SDK\nAdded missing dependency\npytz\nto requirements.txt and setup.py\n1.52.0\nAdded\nAdded handling of\nSMPTE timecode\nwith three flavours -\nNON_DROP_FRAME\n,\nDROP_FRAME\n, and\nAUTO\n.\nThe\nlist all encodings\nendpoint supports now additional time-based filter query parameters.\nAdded\nlive input stream changed webhook\nnotifications\nFixed\nFixed unnecessary\n&\nin the links of the response for the\nlist all inputs\nendpoint\n1.51.0\nAdded\nA new\nEnhanced Deinterlace Filter\nis now available\nAzure Connect: Encoding on\ncustomer Azure infrastructure\nis now available for VoD and Live encodings.\nChanged\nConsolidated manifest status endpoint for\nDash\n,\nHLS\n, and\nSmooth\n. After starting the manifest generations, manifests will be in status QUEUED, once processing starts they will transition into status RUNNING and finally into FINISHED/ERROR depending on the result of the generation.\nFixed\nFixed the\ncache-control\nheader for manifest uploads in both Live and VoD workflows to contain the correct\nmax-age:value\ninstead of\nmax-age=value\nstring\nEncodings\nwith a name longer than 255 characters do no longer result in an internal server error.\nWhen calling the\nstream filter endpoint\nwithout setting the\nposition\nattribute, a\n400\nerror is now correctly returned instead of an internal server error.\nThe correct value\nfalse\nfor the\nadaptiveSpatialTransform\nfor H264\nin the\nVOD_STANDARD\npreset is now applied.\n1.50.0\nAdded\nWhen using the\nlist endpoint for encodings\nyou can now use\nselectedCloudRegion\n,\nselectedEncoderVersion\n, and\nselectedEncodingMode\nas filter parameters\nChanged\nEncodings in status\nERROR\n,\nCANCELED\n,\nFINISHED\n, and\nTRANSFER_ERROR\nwill now always set the\nfinishedAt\ntimestamp in the\nlist encoding\n,\nget encoding\nand\nget encoding status\ncalls. The\nerrorAt\ntimestamp which has been previously set on error encodings is deprecated but will still be populated to ensure backwards compatibility. Note that for encodings prior to this release the finishedAt timestamp might be inaccurate in case of a cancelled encoding.\nC# API SDK\nUp until now, enum value names were missing underscores in most cases. To be more consistent with our API and other SDKs we corrected this mistake.\nGo API SDK\nThe name of 3 enum values was corrected (\nM3U8_URL\n,\nDESEDE\n,\nUHD_8K\n)\nPHP API SDK\nThe name of 3 enum values was corrected (\nM3U8_URL\n,\nDESEDE\n,\nUHD_8K\n)",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/h265-presets-live",
    "title": "H265 Presets",
    "text": "Live Quality Preset Configurations\nLive Quality Presets\nLIVE_STANDARD\nLIVE_HIGH_QUALITY\nadaptiveQuantizationMode\nVARIANCE\nVARIANCE\nadaptiveQuantizationMotion\nFALSE\nFALSE\nadaptiveQuantizationStrength\n1\n1\nallowedRADLBeforeIDR\n0\n0\nanalyzeSourceFramePixels\nFALSE\nFALSE\nasymetricMotionPartitionsAnalysis\nFALSE\nFALSE\nbAdapt\nFULL\nFULL\nbframeBias\n0\n0\nbframes\n4\n4\nblurComplexity\n20\n20\nblurQuants\n0.5\n0.5\ncodingUnitLossless\nFALSE\nFALSE\nconstrainedIntraPrediction\nFALSE\nFALSE\ncopyPicture\nTRUE\nTRUE\ncutree\nTRUE\nTRUE\ndynamicRateDistortionStrength\n0\n0\nearlySkip\nTRUE\nTRUE\nencodingMode\nSINGLE_PASS\nTWO_PASS\nevaluationOfIntraModesInBSlices\nFALSE\nFALSE\nfastSearchForAngularIntraPredictions\nFALSE\nFALSE\nforceFlush\nDISABLED\nDISABLED\ngopLookahead\n0\n0\ngrainOptimizedRateControl\nFALSE\nFALSE\nipRatio\n1.4\n1.4\nlevelHighTier\nTRUE\nTRUE\nlimitModes\nTRUE\nTRUE\nlimitReferences\nDEPTH_AND_CU\nDEPTH_AND_CU\nlimitSao\nFALSE\nFALSE\nlimitTransformUnitDepthRecursion\nLEVEL_4\nLEVEL_4\nlookaheadSlices\n8\n8\nlowpassDct\nFALSE\nFALSE\nmaxCTUSize\n64\n64\nmaximumTransformUnitSize\nMTU_32x32\nMTU_32x32\nmaxMerge\n2\n2\nminCodingUnitSize\nMCU_8x8\nMCU_8x8\nmotionSearch\nHEX\nHEX\nmotionSearchRange\n57\n57\nnoiseReductionInter\n0\n0\npbRatio\n1.3\n1.3\npixelFormat\nYUV420P\nYUV420P\nprofile\nmain\nmain\npsyRateDistortionOptimization\n2\n2\npsyRateDistortionOptimizedQuantization\n0\n0\nqpOffsetChromaCb\n0\n0\nqpOffsetChromaCr\n0\n0\nqpStep\n4\n4\nquantizationGroupSize\nQGS_32x32\nQGS_32x32\nquantizerCurveCompressionFactor\n0.6\n0.6\nrateDistortionLevelForModeDecision\n3\n3\nrateDistortionLevelForQuantization\nLEVELS_AND_CODING_GROUPS\nLEVELS_AND_CODING_GROUPS\nrateDistortionPenalty\nDISABLED\nDISABLED\nrcLookahead\n20\n20\nrectangularMotionPartitionsAnalysis\nFALSE\nFALSE\nrecursionSkip\nTRUE\nTRUE\nrefFrames\n3\n3\nrefineRateDistortionCost\nFALSE\nFALSE\nsao\nTRUE\nTRUE\nsaoNonDeblock\nFALSE\nFALSE\nscenecutBias\n5\n5\nsceneCutThreshold\n40\n40\nsignHide\nTRUE\nTRUE\nskipSplitRateDistortionAnalysis\nFALSE\nFALSE\nslices\n1\n1\nssimRateDistortionOptimization\nFALSE\nFALSE\nstrongIntraSmoothing\nTRUE\nTRUE\nsubMe\n2\n2\ntemporalMotionVectorPredictors\nTRUE\nTRUE\ntransformSkip\nNONE\nNONE\ntuInterDepth\n1\n1\ntuIntraDepth\n1\n1\nwavefrontParallelProcessing\nTRUE\nTRUE\nweightPredictionOnBSlice\nFALSE\nFALSE\nweightPredictionOnPSlice\nTRUE\nTRUE\nLive Low Latency Preset Configurations\nLive Low Latency Presets\nLIVE_LOW_LATENCY\nadaptiveQuantizationMode\nVARIANCE\nadaptiveQuantizationMotion\nFALSE\nadaptiveQuantizationStrength\n1\nallowedRADLBeforeIDR\n0\nanalyzeSourceFramePixels\nFALSE\nasymetricMotionPartitionsAnalysis\nFALSE\nbAdapt\nFAST\nbframeBias\n0\nbframes\n3\nblurComplexity\n20\nblurQuants\n0.5\ncodingUnitLossless\nFALSE\nconstrainedIntraPrediction\nFALSE\ncopyPicture\nTRUE\ncutree\nTRUE\ndynamicRateDistortionStrength\n0\nearlySkip\nTRUE\nencodingMode\nSINGLE_PASS\nevaluationOfIntraModesInBSlices\nFALSE\nfastSearchForAngularIntraPredictions\nTRUE\nforceFlush\nDISABLED\ngopLookahead\n0\ngrainOptimizedRateControl\nFALSE\nipRatio\n1.4\nlevelHighTier\nTRUE\nlimitModes\nFALSE\nlimitReferences\nDEPTH_AND_CU\nlimitSao\nFALSE\nlimitTransformUnitDepthRecursion\nDISABLED\nlookaheadSlices\n8\nlowpassDct\nFALSE\nmaxCTUSize\n64\nmaxMerge\n2\nmaximumTransformUnitSize\nMTU_32x32\nminCodingUnitSize\nMCU_8x8\nmotionSearch\nHEX\nmotionSearchRange\n57\nnoiseReductionInter\n0\npbRatio\n1.3\npixelFormat\nYUV420P\nprofile\nmain\npsyRateDistortionOptimization\n2\npsyRateDistortionOptimizedQuantization\n0\nqpOffsetChromaCb\n0\nqpOffsetChromaCr\n0\nqpStep\n4\nquantizationGroupSize\nQGS_32x32\nquantizerCurveCompressionFactor\n0.6\nrateDistortionLevelForModeDecision\n2\nrateDistortionLevelForQuantization\nDISABLED\nrateDistortionPenalty\nDISABLED\nrcLookahead\n10\nrectangularMotionPartitionsAnalysis\nFALSE\nrecursionSkip\nTRUE\nrefFrames\n2\nrefineRateDistortionCost\nFALSE\nsao\nTRUE\nsaoNonDeblock\nFALSE\nsceneCutThreshold\n40\nscenecutBias\n5\nsignHide\nTRUE\nskipSplitRateDistortionAnalysis\nFALSE\nslices\n1\nssimRateDistortionOptimization\nFALSE\nstrongIntraSmoothing\nTRUE\nsubMe\n2\ntemporalMotionVectorPredictors\nTRUE\ntransformSkip\nNONE\ntuInterDepth\n1\ntuIntraDepth\n1\nwavefrontParallelProcessing\nTRUE\nweightPredictionOnBSlice\nFALSE\nweightPredictionOnPSlice\nTRUE\n-\nmeans that the default value of the codec configuration is used. Please see the\nAPI reference\nfor the respective value.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-encoding-captions-generation-with-azure-speech-services",
    "title": "Live Encoding captions generation with Azure Speech Services",
    "text": "Overview\nAzure Speech Services\nprovides a transcribing service with speech-to-text capabilities with high accuracy. Bitmovin Live encoding service integrates Azure Speech Services to create auto-generated captions in realtime with a simple setup. A full Java code example for starting a Live Encoding with HLS manifest is linked.\nRequirements\nEncoder Version:\nv2.200.0\nor higher\nBitmovin Encoding API SDK Version:\nv1.195.0\nor higher\nAzure Speech Services account\nSubscription key and region\nHow to Setup\nUsing the Dashboard UI\n📘\nThe Live Wizard will allow you to create a simple manifest with some defaults applied, for more customised options we recommend using the API.\nOpen the Live Encoding Wizard and in Step 3 select\nAdd Filter\n.\nFrom the available list select\nAZURE_SPEECH_TO_CAPTIONS\nNext you will see the configuration options, you will need to use your Subscription key for the Azure Cognitive Services with this, then match the settings with the service configuration such as the\nregion\nand\nlanguage\n.\nTesting without a subscription\nFor testing purposes, to modify the style options for example, we offer the option to use Dummy Captions.\nTo enable this, in the subscription key field, just enter\ndummy-captions\ninstead.\nUsing the API\nStart a Bitmovin Live encoder using the Java API code example from\nSRT Live Encoding HLS with Azure Speech To Captions Filter\nYou can configure the Live Input in any of the supported formats detailed in\nLive Inputs\nAfter adding the properties needed to create an encoding detailed in other tutorials, it’s time to add the different streams and codec configurations\nAdd Video and Audio Codec Configurations\nas usual\nFollow the instructions in the following section to  [Add a subtitle stream with AzureSpeechToCaptionsFilter](#Add a subtitle stream with AzureSpeechToCaptionsFilter)\nStart the Live Encoding and start streaming into it once it is ready\nAdd a subtitle stream with AzureSpeechToCaptionsFilter\nAs detailed in the example\nSRT Live Encoding HLS with Azure Speech To Captions Filter\n, in order to use Azure Speech Services to create auto-generated captions we need to configure first a subtitle stream for our encoding.\nCreate WebVTT subtitle configuration\nIn this example we are going to create a WebVTT subtitle configuration for our encoding to store the captions produced by the live encoder. We are setting the default values\ncueIdentifierPolicy\nto\nINCLUDE_IDENTIFIERS\nand appending zeroes when\nhours = 0\nby setting the\nappendOptionalZeroHour\nto\nTrue\n. For more details check the\nWebVTT configuration\ndocumentation.\nJava\nprivate static SubtitleConfiguration createWebVttConfig(String name) {\n    WebVttConfiguration webVttConfiguration = new WebVttConfiguration();\n    webVttConfiguration.setName(name);\n    webVttConfiguration.setCueIdentifierPolicy(WebVttCueIdentifierPolicy.INCLUDE_IDENTIFIERS);\n    webVttConfiguration.setAppendOptionalZeroHour(Boolean.TRUE);\n\n    return bitmovinApi.encoding.configurations.subtitles.webvtt.create(webVttConfiguration);\n}\nAdd AzureSpeechToCaptionsFilter to a subtitle stream\nIn order to create the AzureSpeechToCaptionsFilter we first need to gather the credentials from the Azure Speech Services. This is an example of what we can find in the azure portal:\nWith this information we can configure the filter as follows:\nJava\nprivate static AzureSpeechToCaptionsFilter createAzureSpeechToCaptionsFilter() {\n     AzureSpeechServicesCredentials azureSpeechServicesCredentials = new AzureSpeechServicesCredentials();\n     azureSpeechServicesCredentials.setSubscriptionKey(configProvider.getAzureSpeechSpeechServicesSubscriptionKey());\n\n     AzureSpeechToCaptionsSettings azureSpeechToCaptionsSettings = new AzureSpeechToCaptionsSettings();\n     azureSpeechToCaptionsSettings.setAzureSpeechServicesCredentials(azureSpeechServicesCredentials);\n     azureSpeechToCaptionsSettings.setRegion(configProvider.getAzureSpeechSpeechServicesRegion());\n     azureSpeechToCaptionsSettings.setLanguage(\"en-US\");\n     azureSpeechToCaptionsSettings.setCaptionDelay(100L);\n     azureSpeechToCaptionsSettings.setCaptionRemainTime(1000L);\n     azureSpeechToCaptionsSettings.setCaptionMaxLineLength(40L);\n     azureSpeechToCaptionsSettings.setCaptionLines(2L);\n     azureSpeechToCaptionsSettings.setProfanityOption(AzureSpeechToCaptionsProfanity.MASKED);\n\n     AzureSpeechToCaptionsFilter azureSpeechToCaptionsFilter = new AzureSpeechToCaptionsFilter();\n     azureSpeechToCaptionsFilter.setAzureSpeechToCaptionsSettings(azureSpeechToCaptionsSettings);\n\n     return bitmovinApi.encoding.filters.azureSpeechToCaptions.create(azureSpeechToCaptionsFilter);\n}\nThe\nconfigProvider\ngets the\nsubscriptionKey\nand\nregion\nfor the filter\nThe\nlanguage\nis set to\nen-US\n(IETF BCP 47 language tag) as documented in the list of supported languages of\nAzure's official documentation\n.\nThe\ncaptionDelay\nis set to\n100\nMILLISECONDS to delay the display of each caption, to mimic a real-time experience\nThe\ncaptionTemainTime\nis set to remain\n1 second\non screen\nThe\nprofanityOption\nis configured so it replaces letters in profane words with asterisk\n(*)\ncharacters.\nNow the filter can be added to the created subtitle stream\nJava\nStream subtitleStream = createStream(encoding, input, webVttConfig);\naddFiltersToStream(encoding, subtitleStream, getStreamFilterList(Collections.singletonList(azureSpeechToCaptionsFilter)));\nCreate Chunked Text Muxing\nIn order to have the webVTT subtile packaged correctly we create a\nChunked Text muxing\nwhich is creating a segmented webVTT segmented output where the manifest playlist will load the segments from.\nJava\nprivate static ChunkedTextMuxing createChunkedTextMuxing(\n      Encoding encoding,\n      Output output,\n      String outputPath,\n      Stream stream,\n      Double segmentLength,\n      Integer startOffset) {\n   \n    MuxingStream muxingStream = new MuxingStream();\n    muxingStream.setStreamId(stream.getId());\n\n    ChunkedTextMuxing chunkedTextMuxing = new ChunkedTextMuxing();\n    chunkedTextMuxing.addOutputsItem(buildEncodingOutput(output, outputPath));\n    chunkedTextMuxing.addStreamsItem(muxingStream);\n    chunkedTextMuxing.setSegmentLength(segmentLength);\n    chunkedTextMuxing.setSegmentNaming(\"webvtt_segment_%number%.vtt\");\n    chunkedTextMuxing.setStartOffset(startOffset);\n   \n    return bitmovinApi.encoding.encodings.muxings.chunkedText.create(encoding.getId(), chunkedTextMuxing);\n  }\nIn this example we set the parameters to have a\nchunkLenght\nof\n4.0\nseconds, which matches the one of Video and Audio. Also the segments will have a segment naming\nwebvtt_segment_%number%.vtt\nwhich will translate to\nwebvtt_segment_0.vtt\n,\nwebvtt_segment_1.vtt\n,\nwebvtt_segment_2.vtt\n…\nJava\ncreateChunkedTextMuxing(encoding, output, \"/subtitles\", subtitleStream, 4.0, 10);\nSummary\nIn this page we learned how to configure a Live encoding using SRT as an input, adding a subtitle stream with the Azure Speech Services Filter and creating auto-generated captions with an HLS manifest output using our default HLS manifest creation.\nThis powerful combination ensures your live streams are more accessible and engaging for all viewers with a seamless integration of Azure's highly accurate speech-to-text capabilities and Bitmovin's live encoding. So now you can provide real-time captions, making your content more inclusive and professional.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/4148833ad6a7597debc4f2b9bb9c82a2a9ecaae10d464e02d1885fa8620d2ced-Screenshot_2024-09-09_at_15.26.45.png",
      "https://files.readme.io/423b8daeb4cfa1d44c9b4df680a1c333002b509bc925fe3407b4c31cfc7ba458-Screenshot_2024-09-09_at_15.34.04.png",
      "https://files.readme.io/1dc074c5529916fa3ab4705a776c0229c28a23e92e4267b241d9805b9a3163f0-Screenshot_2024-09-09_at_15.35.09.png",
      "https://files.readme.io/c521e38-Screenshot_2024-06-17_at_13.48.52.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/azure-buckets-for-live-outputs",
    "title": "Azure Buckets for Live Outputs",
    "text": "Overview\nBitmovin can not only run in Microsoft Azure, deploying the Live Encoder to a range of regions, but it can also output to a MS Azure bucket as an origin server for distribution directly or via a CDN.\nConfiguring Azure buckets\nWhen using Azure buckets as an output for the Live Encoder you will need to configure them to grant the appropriate roles and access control policies.\nOutput Buckets\nFor more information on how to configure Azure buckets please see the\ndocumentation from Microsoft\n📘\nAuthorization using Microsoft Extra ID\nBitmovin currently only supports using Token based authorization, with the new role based method planned for a future update.\nFor more information you can follow the conversation and join in here in our\ncommunity\n.\nConfiguring Live Encoding Outputs\nIn the UI navigate to Live Encoding in the left side panel, and then select Outputs.\n📘\nYou will need to enter the Access key and Secret Key provided via the MS Azure console or your MS Azure system administrator.\nUsing the Live Output\nThe bucket will appear in the Outputs list, and in the Wizard under Microsoft Azure.\nYou can confirm the bucket is created in the API by using\nList Azure Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/beec332-Screenshot_2024-04-05_at_18.41.26.png",
      "https://files.readme.io/5a0309f-Screenshot_2024-04-05_at_19.04.18.png",
      "https://files.readme.io/eb61b20-Screenshot_2024-04-06_at_11.46.30.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/protecting-your-content-with-bitmovin-and-vualto-drm",
    "title": "Protecting Your Content with Bitmovin and Vualto DRM",
    "text": "Introduction\nAs an OTT service provider who wants to monetize content, you need to make sure your valuable content can be only played by authorized users conforming to the rights they bought.\nDigital Rights Management (DRM)\nis a commonly used means to protect your content and ensure your revenue.\nAs your users play back your content on a wide range of devices - from Smart TVs to smartphones, running on different platforms - you will need to use multiple DRM systems to cover all devices that are popular with your users.\nManaging multiple DRM systems can be difficult, but service providers like\nVualto\noffer help and facilitate the complex matter.\nThis tutorial explains how to encode and protect your content using Vualto as your multi-DRM management service.\nDRM Systems\nWhen we talk about multiple DRM systems, in this tutorial we mean to cover the most used DRM systems which are:\nWidevine\nThis DRM system by Google is used for playback on related platforms such as Chrome(cast), Android (TV), or Google TV.\nPlayready\nThis DRM system by Microsoft is used for playback on related platforms such as Edge, IE11, XBox, or Windows Phone.\nFairplay\nThis DRM System by Apple is used for playback on related platforms such as Safari, iOS, or Apple TV.\nOther platforms that are not directly related to Google, Microsoft or Apple typically support one or more of the above DRM systems.\nPlease find an elaborate list of platform-DRM compatibility\nhere\n.\nUse Cases in this Tutorial\nIn this tutorial, we will focus on a selected list of combinations of\nadaptive streaming formats\nand DRM systems.\nThe content, packaged in these streaming formats and encrypted with the associated DRM systems, will be playable on most market relevant devices.\nDASH with Widevine\n, tested with Chrome on the Mac\nDASH with Playready\n, tested with Edge and IE11 on Windows 10\nHLS with Fairplay\n, tested with Safari on the Mac and with iOS\nSmooth Streaming with Playready\n, tested with Edge and IE11 on Windows 10\nPrerequisites\nTo protect your content with\nVualto\n, you need to collect some values that are required to configure encoding and playback.\nFirst,\ngive your content an ID\nwhich must be different for each piece of content that you want to encode. In this tutorial, we use\nsomecontentid\nas the ID of your content.\nTo get the the remaining required values, you need to have an account with Vualto or at least get the values from your Vualto contact:\nNote:\nValues that are specified for a certain DRM system (\nWidevine\n,\nPlayready\nor\nFairplay\n) are needed for that DRM system only.\nThe Vualto token\n, called \"VU DRM Token\". This is a long string divided in four sections, which are separated by a \"|\". The first section is the name of the Vualto account owner, the second is a date when the token was created. The token looks like\nyourname|2020-07-02T16:03:55Z|kQI7t+b1Kd51CfzfnQ5qzx==|cb556f86d4453a6abf5381966ec543979fcb656f\n.\nThe Fairplay Certificate URL\n, looking like\nskd://fairplay-license.vudrm.tech/certificate/yourname\nThe following values will be given to the Bitmovin encoding process during configuration.\nDRM_KEY\n: The key which will be used to encrypt your content, looking like\nB01F8C032F5564B6C81DCD340F480978\nDRM_FAIRPLAY_IV\n: The Initialization Vector for Fairplay, looking like\n656636323963441663544463864376663\nDRM_FAIRPLAY_URI\n: The license URL for Fairplay, looking like\nskd://fairplay-license.vudrm.tech/v2/license/somecontentid\n. Note that this URL includes your content's ID, in this example\nsomecontentid\n.\nDRM_WIDEVINE_KID\n: The Key ID for Widevine, looking like\nE920E5426157291C177F8BBD34026696\nDRM_WIDEVINE_PSSH\n: The PSSH data (not the whole box) for Widevine, looking like\nIg2zb21lZ37udKVudGfhSOPclZsG\nDRM_PLAYREADY_PSSH\n: The PSSH data (not the whole box) for Playready. This is a very long string looking like\nkAMAAAEAAQCGAzwBVwBSAE1ASABAEFFAR...RQCCAEQARQBTAD3A\nWriting your Encoding Script in PHP\nThis section will show you how to write a PHP script which uses the Bitmovin API to configure and run a DRM encrypted encoding for your content.\nYou will find the full script here: TBD\nLike every encoding script, we begin with the main workflow, setting up the API, encoding, input and output.\nPHP\n$exampleName = 'BitmovinTutorial';\n\n$configProvider = new ConfigProvider();\n\n$bitmovinApi = new BitmovinApi(Configuration::create()\n    ->apiKey($configProvider->getBitmovinApiKey()));\n\n// -- Encoding\n$encoding = createEncoding($exampleName, \"Encoding with different DRM systems\");\n\n// -- Input\n$input = createHttpsInput($configProvider->getHttpInputHost());\n$inputFilePath = $configProvider->getHttpInputFilePath();\n\n// -- Output\n$output = createS3Output(\n    $configProvider->getS3OutputBucketName(),\n    $configProvider->getS3OutputAccessKey(),\n    $configProvider->getS3OutputSecretKey()\n);\nPlease note that in this example we use a HTTP input to load your source file from and an S3 output to write the encoded/encrypted data to. You are not bound to these formats, please see\nthis page for all supported input and output storages\n.\nEncoding\nIn the upcoming sections, you will learn how to encode your content.\nYou will also learn how to package the encoded content for HLS and DASH, and optionally for Smooth Streaming.\nAnd you will learn how to encrypt the packaged content with Widevine, PlayReady and Fairplay DRM.\nSetting up Codec Configurations and Streams\nLike with every encoding, we first must define what video and audio codecs our content should be encoded into.\nFor this, we first write two functions to generate a simple H.264 video config and a simple AAC audio config.\nPHP\nfunction createH264Config()\n{\n    global $bitmovinApi;\n  \n    $h264Config = new H264VideoConfiguration();\n    $h264Config->name(\"H.264 1080p 1.5 Mbit/s\");\n    $h264Config->presetConfiguration(PresetConfiguration::VOD_STANDARD());\n    $h264Config->height(1080);\n    $h264Config->bitrate(1500000);\n  \n    return $bitmovinApi->encoding->configurations->video->h264->create($h264Config);\n}\nPHP\nfunction createAacConfig(AacChannelLayout $channelLayout)\n{\n    global $bitmovinApi;\n  \n    $aacConfig = new AacAudioConfiguration();\n    $aacConfig->name(\"AAC 128 kbit/s\");\n    $aacConfig->bitrate(128000);\n    $aacConfig->channelLayout($channelLayout);\n  \n    return $bitmovinApi->encoding->configurations->audio->aac->create($aacConfig);\n}\nThen, we write a function to create a\nStream\n. A Stream in this context is an entity that connects an input track (audio or video from the source file) to a codec. By applying a codec to an input track, you tell the Encoder to encode the track with that codec.\nPHP\nfunction createStream(Encoding $encoding, Input $input, string $inputPath, CodecConfiguration $codecConfiguration)\n{\n    global $bitmovinApi;\n  \n    $streamInput = new StreamInput();\n    $streamInput->inputId($input->id);\n    $streamInput->inputPath($inputPath);\n  \n    $stream = new Stream();\n    $stream->inputStreams([$streamInput]);\n    $stream->codecConfigId($codecConfiguration->id);\n  \n    return $bitmovinApi->encoding->encodings->streams->create($encoding->id, $stream);\n}\nAfter we have defined these functions, we go back to the main workflow and call them:\nPHP\n// -- Streams --\n// Add an H.264 video stream to the encoding\n$h264Config = createH264Config();\n$h264Stream = createStream($encoding, $input, $inputFilePath, $h264Config);\nThe first line creates the video codec configuration we want to use. The second line connects the video codec configuration with the input file.\nWe specify only the input file because in the scope of this tutorial we assume the input file has one video track and one audio track. As the codec config\n$h264Config\nis a video config, the system knows that it should use the video track of the input file.\nFor audio, the code looks similar.\nPHP\n// Add an AAC Stereo audio stream to the encoding\n$aacConfig = createAacConfig(AacChannelLayout::CL_STEREO());\n$aacStream = createStream($encoding, $input, $inputFilePath, $aacConfig);\nNote:\nOf course the Bitmovin encoder can handle input files with multiple video and audio tracks, and also specify which ones to encode, but this is out of scope of this tutorial where we focus on DRM.\nWe now have two\nStream\nvariables,\n$h264Stream\nand\n$aacStream\n, which stand for the encoded - but not yet packaged and encrypted - video and audio tracks.\nMuxing (Packaging) the Streams for HLS and DASH\nIn order to write down the encoded\nstreams\n, we must tell the encoder how to package them.\nThe packaging of encoded streams is called\nmuxing\n, and this is also the term for the result of the packaging process. A\nmuxing\nis therefore a packaged set of streams. A muxing can, for example, be an\n.mp4\nor\n.ts\nfile with audio and video in it. A muxing can also be a set of segments to support streaming formats like HLS and DASH. Examples for this are a set of .ts segments to support HLS, or a set of Fragmented MP4 files (.m4s) to support DASH and newer versions of HLS. The set of Fragmented MP4 (.m4s) segments is called an\nfMP4 muxing\n.\nIn this tutorial, we will package our Streams into fMP4 muxings to support both DASH and HLS.\nFirst, we write a function that creates an fMP4 Muxing from a Stream.\nPHP\nfunction createFmp4Muxing(Encoding $encoding, Output $output, string $outputPath, Stream $stream)\n{\n    global $bitmovinApi;\n  \n    $muxingStream = new MuxingStream();\n    $muxingStream->streamId($stream->id);\n  \n    $muxing = new Fmp4Muxing();\n    $muxing->outputs([buildEncodingOutput($output, $outputPath)]);\n    $muxing->segmentLength(4.0);\n    $muxing->segmentNaming(\"segment_\".$outputPath.\"_%number%.m4s\");\n    $muxing->streams[] = $muxingStream;\n  \n    return $bitmovinApi->encoding->encodings->muxings->fmp4->create($encoding->id, $muxing);\n}\nThe function connects the\nStream\n(via an intermediate object called MuxingStream) to the actual\nMuxing\n. The muxing defines the format (fMP4), the segment length in seconds, the segments' file naming template and the output storage and path where the segments will be written.\nBack to the main workflow, we call this function twice: once for video and once for audio.\nPHP\n$fmp4H264Muxing = createfMp4Muxing($encoding, $output, 'fmp4-h264', $h264Stream);\n$fmp4AacMuxing = createfMp4Muxing($encoding, $output, 'fmp4-aac', $aacStream);\nWe now have the variables\n$fmp4H264Muxing\nand\n$fmp4AacMuxing\nwho each represent a set of segments, one with encoded video and one with encoded audio. These segments are not yet encrypted.\nNote:\nWe are creating two muxings (one video, one audio) from one audio/video source file. The reason for this is that the fMP4 muxing format can only handle one stream. This is not a disadvantage, because for Adaptive Bitrate Streaming cases it is more efficient to have video and audio muxings separated anyway.\nEncrypting the Muxings for DRM (Widevine, Playready, Fairplay)\nNow comes the time to actually encrypt the encoded output streams which are packaged in muxings.\nFor this, we create a DRM configuration. The DRM configuration will be inserted between the muxing and the output. You can also think of the DRM configuration as an encrypted version of the muxing.\nUnencrypted muxings have outputs assigned to the\noutputs\nfield. Muxings that are to be encrypted don't need to populate the\noutputs\nfield. Instead, the outputs are assigned to the DRM configuration's\noutputs\nfield.\nThe function below takes the unencrypted muxing we created in the previous section, and wraps it into a DRM configuration.\nFor this, it connects the unencrypted\n$muxing\nwith a locally created\n$cencDrm\nobject which encrypts the muxing. The\n$cencDrm\nobject uses the information given in the\nPrerequisites\nsection of this tutorial.\nPHP\nfunction createDrmCbcConfig(Encoding $encoding, Fmp4Muxing $muxing, Output $output, string $outputPath)\n{\n    global $bitmovinApi, $configProvider;\n  \n    $cencWidevine = new CencWidevine();\n    $cencWidevine->pssh($configProvider->getDrmWidevinePssh());\n  \n    $cencPlayReady = new CencPlayReady();\n    $cencPlayReady->pssh($configProvider->getDrmPlayreadyPssh());\n  \n    $cencFairPlay = new CencFairPlay();\n    $cencFairPlay->iv($configProvider->getDrmFairplayIv());\n    $cencFairPlay->uri($configProvider->getDrmFairplayUri());\n  \n    $cencDrm = new CencDrm();\n    $cencDrm->outputs([buildEncodingOutput($output, $outputPath)]);\n    $cencDrm->key($configProvider->getDrmKey());\n    $cencDrm->kid($configProvider->getDrmWidevineKid());\n    $cencDrm->widevine($cencWidevine);\n    $cencDrm->playReady($cencPlayReady);\n    $cencDrm->fairPlay($cencFairPlay);\n    $cencDrm->encryptionMode(EncryptionMode::CBC());\n  \n    return $bitmovinApi->encoding->encodings->muxings->fmp4->drm->cenc->create($encoding->id, $muxing->id, $cencDrm);\n}\nAgain, we go back to the main workflow and call this function, once for our video muxing and once for our audio muxing:\nPHP\n$cencDrmVideo = createDrmCbcConfig($encoding, $fmp4H264Muxing, $output, 'drm-fmp4-h264');\n$cencDrmAudio = createDrmCbcConfig($encoding, $fmp4AacMuxing, $output, 'drm-fmp4-aac');\n$cencDrmVideo\nstands now for an encrypted video muxing.\n$cencDrmAudio\nis the associated audio muxing. These muxings can be played with Widevine, Playready and Fairplay.\nSmooth Streaming: Muxing and Encrypting with Playready\nNote:\nYou can skip this section if you don't plan to use the Smooth Streaming format for your content delivery.\nSo far, this tutorial has dealt with DASH and HLS delivery methods. In some cases however, Smooth Streaming is still a required delivery option, so we now describe how to do the same for Smooth.\nWhile DASH and HLS can share the same muxing, allowing for cost savings on both storage and CDN, Smooth Streaming requires an extra muxing (packaging) format albeit the same encoded streams are contained.\nFor this, we create a function that writes the encoded Streams into an\nMP4 Muxing\nsuitable for Smooth Steaming:\nPHP\nfunction createMp4MuxingForSmooth(Encoding $encoding, Output $output, string $outputPath, array $streams, string $filename)\n{\n    global $bitmovinApi;\n  \n    $muxing = new Mp4Muxing();\n    $muxing->filename($filename);\n    $muxing->fragmentDuration(4000);\n    $muxing->fragmentedMP4MuxingManifestType(FragmentedMp4MuxingManifestType::SMOOTH());\n    $muxing->outputs([buildEncodingOutput($output, $outputPath)]);\n  \n    foreach ($streams as $stream) {\n      $muxingStream = new MuxingStream();\n      $muxingStream->streamId($stream->id);\n      $muxing->streams[] = $muxingStream;\n    }\n  \n    return $bitmovinApi->encoding->encodings->muxings->mp4->create($encoding->id, $muxing);\n}\nAnd then, we use Playready to encrypt the Smooth Streaming muxings:\nPHP\nfunction createSmoothPlayreadyConfig(Encoding $encoding, Mp4Muxing $muxing, Output $output, string $outputPath)\n{\n    global $bitmovinApi, $configProvider;\n  \n    $cencPlayReady = new CencPlayReady();\n    $cencPlayReady->pssh($configProvider->getDrmPlayreadyPssh());\n  \n    $cencDrm = new CencDrm();\n    $cencDrm->outputs([buildEncodingOutput($output, $outputPath)]);\n    $cencDrm->key($configProvider->getDrmKey());\n    $cencDrm->kid($configProvider->getDrmWidevineKid());\n    $cencDrm->playReady($cencPlayReady);\n    $cencDrm->encryptionMode(EncryptionMode::CTR());\n    $cencDrm->ivSize(IvSize::IV_8_BYTES());\n  \n    return $bitmovinApi->encoding->encodings->muxings->mp4->drm->cenc->create($encoding->id, $muxing->id, $cencDrm);\n}\nNote:\nAs Smooth Streaming is a Microsoft technology, it uses Playready which is also by Microsoft. No other DRM systems are supported for Smooth.\nBack to the main workflow, you now call these functions to get the encrypted muxings for Smooth Streaming.\nPHP\n// Smooth Streaming mp4 muxings\n // Video H.264\n $mp4H264MuxingSmooth = createMp4MuxingForSmooth($encoding, $output, 'smooth', [$h264Stream], 'video.ismv');\n $smoothDrmVideo = createSmoothPlayreadyConfig($encoding, $mp4H264MuxingSmooth, $output, 'smooth-drm');\n // Audio AAC\n $mp4AacMuxingSmooth = createMp4MuxingForSmooth($encoding, $output, 'smooth', [$aacStream], 'audio.isma');\n $smoothDrmAudio = createSmoothPlayreadyConfig($encoding, $mp4AacMuxingSmooth, $output, 'smooth-drm');\nThe variables\n$smoothDrmVideo\nand\n$smoothDrmAudio\nnow stand for the encrypted Smooth Streaming muxings we created.\nRun the Encoding\nNow that we have everything configured\nthe source file's audio and video tracks connected to the H.264 and AAC configuration as\nStreams\nthe streams connected to fMP4 (or MP4) configurations as\nMuxings\nthe muxings connected to DRM configurations and output locations as\nencrypted Muxings\nwe can now actually start the encoding, by calling in the main workflow:\nPHP\n// -- Run the encoding --\nexecuteEncoding($encoding);\nFor details of the\nexectueEncoding()\nfunction (which is standard for all kinds of encodings) please see the actual example script.\nCreating Manifest for DASH\nAfter the encoding is finished, the audio and video segments have been created, but for HLS and DASH playback we also need to create manifests. The manifests tell the player where to get the segments and also where to get the DRM key to decrypt them.\nIn this section we show how to create the DASH manifest. Because this is much code, we pack it into a function:\nPHP\nfunction generateDashDrmManifest(Encoding $encoding, Output $output, string $outputPath, string $streamName,\n                                 Muxing $videoMuxing, Muxing $audioMuxing, CencDrm $cencDrmVideo, CencDrm $cencDrmAudio)\n{\n    global $bitmovinApi;\n  \n    // the manifest itself\n    $dashManifest = new DashManifest();\n    $dashManifest->name($streamName.'.mpd');\n    $dashManifest->description('A DASH manifest');\n    $dashManifest->outputs([buildEncodingOutput($output, $outputPath)]);\n    $dashManifest->profile(DashProfile::LIVE());\n    $dashManifest->manifestName($streamName.'.mpd');\n    $dashManifest = $bitmovinApi->encoding->manifests->dash->create($dashManifest);\n    \n    // the period\n    $period = new Period();\n    $period->start(0);\n    $period = $bitmovinApi->encoding->manifests->dash->periods->create($dashManifest->id, $period);\n    \n    // the video adaptation set\n    $videoAdaptationSet = new VideoAdaptationSet();\n    $videoAdaptationSet->roles([AdaptationSetRole::MAIN()]);\n    $videoAdaptationSet = $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->video->create(\n        $dashManifest->id, $period->id, $videoAdaptationSet);\n    $contentProtectionAdded = false;\n    \n    // representations - one for each quality\n    if (!$contentProtectionAdded) {\n        $contentProtection = new ContentProtection();\n        $contentProtection->drmId($cencDrmVideo->id);\n        $contentProtection->encodingId($encoding->id);\n        $contentProtection->muxingId($videoMuxing->id);\n        $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->contentprotection->create($dashManifest->id, $period->id, $videoAdaptationSet->id, $contentProtection);\n        $contentProtectionAdded = true;\n    }\n    $dashFMp4RepresentationVideo = new DashFmp4Representation();\n    $dashFMp4RepresentationVideo->encodingId($encoding->id);\n    $dashFMp4RepresentationVideo->muxingId($videoMuxing->id);\n    $dashFMp4RepresentationVideo->type(DashRepresentationType::TEMPLATE());\n    $dashFMp4RepresentationVideo->mode(DashRepresentationTypeMode::TEMPLATE_REPRESENTATION());\n    $dashFMp4RepresentationVideo->segmentPath(basename($cencDrmVideo->outputs[0]->outputPath));\n    $dashFMp4RepresentationVideo =\n        $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->representations->fmp4->create(\n            $dashManifest->id,\n            $period->id,\n            $videoAdaptationSet->id,\n            $dashFMp4RepresentationVideo);\n    \n    // audio adaptation set\n    $audioAdaptationSet = new AudioAdaptationSet();\n    $audioAdaptationSet->roles([AdaptationSetRole::MAIN()]);\n    $audioAdaptationSet->lang(\"en\");\n    $audioAdaptationSet = $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->audio->create($dashManifest->id,\n        $period->id, $audioAdaptationSet);\n    $contentProtection = new ContentProtection();\n    $contentProtection->drmId($cencDrmAudio->id);\n    $contentProtection->encodingId($encoding->id);\n    $contentProtection->muxingId($audioMuxing->id);\n    $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->contentprotection->create($dashManifest->id,\n        $period->id, $audioAdaptationSet->id, $contentProtection);\n    \n    // representation\n    $dashFMp4RepresentationAudio = new DashFmp4Representation();\n    $dashFMp4RepresentationAudio->encodingId($encoding->id);\n    $dashFMp4RepresentationAudio->muxingId($audioMuxing->id);\n    $dashFMp4RepresentationAudio->type(DashRepresentationType::TEMPLATE());\n    $dashFMp4RepresentationAudio->mode(DashRepresentationTypeMode::TEMPLATE_REPRESENTATION());\n    $dashFMp4RepresentationAudio->segmentPath(basename($cencDrmAudio->outputs[0]->outputPath));\n    $dashFMp4RepresentationAudio =\n        $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->representations->fmp4->create(\n            $dashManifest->id,\n            $period->id,\n            $audioAdaptationSet->id,\n            $dashFMp4RepresentationAudio);\n    \n    executeDashManifestCreation($dashManifest);\n}\nWe can see that this function takes a lot of the elements we created so far in the script:\nthe audio and video muxings\nthe encrypted muxings for Widevine and Playready\nthe output\nand a name for the manifest, called\nstreamName\nbecause this will represent the playback stream to players outside.\nNote:\nThe DASH manifest re-unites the audio and video muxings in one manifest. The outside player only needs the URL to this manifest to get to the audio and video segments and play back the whole presentation.\nNote:\nThe DASH manifest only contains encrypted muxings for Widevine and PlayReady, not for Fairplay. The reason is that Apple devices using Fairplay also use HLS and not DASH.\nIn the main workflow, we call this function to create the DASH manifest:\nPHP\ngenerateDashDrmManifest($encoding, $output, \"\", \"streamDrm\",\n        $fmp4H264Muxing, $fmp4AacMuxing, $cencDrmVideo, $cencDrmAudio);\nCreating manifests for HLS\nThe manifest will be created on the output with the given\nstreamName\n.\nLike with DASH, the creation of manifests for HLS is packed in a function:\nPHP\nfunction generateHlsFairplayManifest(Encoding $encoding, Output $output, string $outputPath, string $streamName,\n                                     Muxing $videoMuxing, Muxing $audioMuxing,\n                                     CencDrm $cencDrmVideo, CencDrm $cencDrmAudio)\n{\n    global $bitmovinApi;\n    \n    // the manifest itself\n    $hlsManifest = new HlsManifest();\n    $hlsManifest->name($streamName . \".m3u8\");\n    $hlsManifest->description('A HLS Fairplay manifest');\n    $hlsManifest->outputs([buildEncodingOutput($output, $outputPath)]);\n    $hlsManifest->manifestName($streamName . \".m3u8\");\n    $hlsManifest = $bitmovinApi->encoding->manifests->hls->create($hlsManifest);\n    \n    // audio group\n    $audioGroupId = 'audio';\n    \n    // add audio media\n    $audioMediaInfo = new AudioMediaInfo();\n    $audioMediaInfo->name('DRM en');\n    $audioMediaInfo->encodingId($encoding->id);\n    $audioMediaInfo->groupId($audioGroupId);\n    $audioMediaInfo->streamId($audioMuxing->streams[0]->streamId);\n    $audioMediaInfo->muxingId($audioMuxing->id);\n    $audioMediaInfo->language('en');\n    $audioMediaInfo->drmId($cencDrmAudio->id);\n    $audioMediaInfo->segmentPath(basename($cencDrmAudio->outputs[0]->outputPath));\n    $audioMediaInfo->uri('audio_' . $streamName . '.m3u8');\n    $audioMediaInfo = $bitmovinApi->encoding->manifests->hls->media->audio->create($hlsManifest->id, $audioMediaInfo);\n    \n    // add video variant\n    $streamInfo = new StreamInfo();\n    $streamInfo->encodingId($encoding->id);\n    $streamInfo->streamId($videoMuxing->streams[0]->streamId);\n    $streamInfo->muxingId($videoMuxing->id);\n    $streamInfo->audio($audioGroupId);\n    $streamInfo->segmentPath(basename($cencDrmVideo->outputs[0]->outputPath));\n    $streamInfo->uri('video_' . $streamName . '.m3u8');\n    $streamInfo->drmId($cencDrmVideo->id);\n    $streamInfo = $bitmovinApi->encoding->manifests->hls->streams->create($hlsManifest->id, $streamInfo);\n    \n    executeHlsManifestCreation($hlsManifest);\n}\nWe can see that this function takes a lot of the elements we created so far in the script:\nthe audio and video muxings\nthe encrypted muxings\nthe output\nand a name for the manifest, called\nstreamName\nbecause this will represent the playback stream to players outside.\nNote\nThe HLS manifest only uses the Fairplay DRM system, because Apple devices use both in conjunction (HLS and Fairplay are Apple standards). Non-Apple devices will use DASH and non-Fairplay DRM systems.\nIn the main workflow, we call this function to create the HLS manifests (aka master playlist, audio and video playlists):\nPHP\ngenerateHlsFairplayManifest($encoding, $output, \"\", \"streamFmp4Drm\",\n $fmp4H264Muxing, $fmp4AacMuxing, $cencDrmVideo, $cencDrmAudio);\nCreating Manifests for Smooth\nIf we use Smooth Streaming, we need to create manifests for it as well. First we write the function to create the manifests:\nPHP\nfunction generateSmoothStreamingDrmManifest(Encoding $encoding, Output $output, string $outputPath,\n                                            Mp4muxing $mp4H264Muxing, Mp4Muxing $mp4AacMuxing,\n                                            CencDrm $smoothDrmVideo, CencDrm $smoothDrmAudio)\n{\n    global $bitmovinApi;\n    \n    $smoothStreamingManifest = new SmoothStreamingManifest();\n    $smoothStreamingManifest->name('Smooth Streaming Manifest with DRM');\n    $smoothStreamingManifest->serverManifestName('stream.ism');\n    $smoothStreamingManifest->clientManifestName('stream.ismc');\n    $smoothStreamingManifest->outputs([buildEncodingOutput($output, $outputPath)]);\n    $smoothStreamingManifest\n        = $bitmovinApi->encoding->manifests->smooth->create($smoothStreamingManifest);\n    \n    // add video representation\n    $cencDrm = $smoothDrmVideo;\n    $contentProtectionVideo = new SmoothManifestContentProtection();\n    $contentProtectionVideo->drmId($cencDrm->id);\n    $contentProtectionVideo->muxingId($mp4H264Muxing->id);\n    $contentProtectionVideo->encodingId($encoding->id);\n    $contentProtectionVideo = $bitmovinApi->encoding->manifests->smooth->contentprotection->create(\n        $smoothStreamingManifest->id,\n        $contentProtectionVideo);\n    \n    $mp4RepresentationVideo = new SmoothStreamingRepresentation();\n    $mp4RepresentationVideo->encodingId($encoding->id);\n    $mp4RepresentationVideo->muxingId($mp4H264Muxing->id);\n    $mp4RepresentationVideo->mediaFile('video.ismv');\n    $mp4RepresentationVideo->trackName('video');\n    $mp4RepresentationVideo->priority(30);\n    $mp4RepresentationVideo\n        = $bitmovinApi->encoding->manifests->smooth->representations->mp4->create($smoothStreamingManifest->id, $mp4RepresentationVideo);\n    \n    // add audio representation\n    $cencDrm = $smoothDrmAudio;\n    $contentProtection = new SmoothManifestContentProtection();\n    $contentProtection->drmId($cencDrm->id);\n    $contentProtection->muxingId($mp4AacMuxing->id);\n    $contentProtection->encodingId($encoding->id);\n    $contentProtection = $bitmovinApi->encoding->manifests->smooth->contentprotection->create(\n        $smoothStreamingManifest->id,\n        $contentProtection);\n    \n    $mp4RepresentationAudio = new SmoothStreamingRepresentation();\n    $mp4RepresentationAudio->encodingId($encoding->id);\n    $mp4RepresentationAudio->muxingId($mp4AacMuxing->id);\n    $mp4RepresentationAudio->mediaFile('audio.isma');\n    $mp4RepresentationAudio->language('en');\n    $mp4RepresentationAudio->trackName('audio');\n    $mp4RepresentationAudio->priority(20);\n    $mp4RepresentationAudio\n        = $bitmovinApi->encoding->manifests->smooth->representations->mp4->create($smoothStreamingManifest->id, $mp4RepresentationAudio);\n    \n    executeSmoothManifestCreation($smoothStreamingManifest);\n}\nand then, back to the main workflow, we call the function:\ngenerateSmoothStreamingDrmManifest($encoding, $output, \"smooth-drm\",\n        $mp4H264MuxingSmooth, $mp4AacMuxingSmooth, $smoothDrmVideo, $smoothDrmAudio);\nThe Output Files\nAfter running this script, all generated files will be on the S3 bucket named in the\nexamples.properties\nfile in the parameter\nS3_OUTPUT_BUCKET_NAME\n.\nThe files are in the main output directory, which is\nthe path written in the\nS3_OUTPUT_BASE_PATH\nparameter in\nexamples.properties\n, plus\nthe\n$exampleName\nvariable as subdirectory.\nThe files are:\ndrm-fmp4-h264/\n- contains encoded and encrypted video segments for DASH and HLS\ndrm-fmp4-aac/\n- contains encoded and encrypted audio segments for DASH and HLS\naudio_streamFmp4Fairplay.m3u8\n- HLS audio playlist\nvideo_streamFmp4Fairplay.m3u8\n- HLS video playlist\nstreamFmp4Fairplay.m3u8\n- HLS master playlist (manifest), works with Fairplay\nstreamDRM.mpd\n- DASH manifest, works with Widevine and Playready\nIf you encoded your content also for Smooth Streaming, you will additionally see:\nsmooth-drm/\n- contains content and manifest files for Smooth Streaming.\nPreparing the Output Files for Playback\nIn order to playback the files created by the Bitmovin encoder, you need to place them on a web server. In case of\nDASH\nand\nHLS\n, this can be any web server. The simplest way is to stream them directly from the S3 bucket. For this, just enable public HTTPS access to the files on the S3 bucket.\nThe URLs to the manifests will look like:\nhttps://server.com/path/to/manifest/streamDrm.mpd\nhttps://server.com/path/to/manifest/streamFmp4Fairplay.m3u8\nThe\nSmooth Streaming\nfiles cannot be served from an arbitrary web server, they have to be placed on a Microsoft IIS server (Please consult your Microsoft documentation for using a Microsoft IIS server for Smooth Streaming). When the Bitmovin encoded files are stored there, the URL will look like:\nhttp://microsoft_iis_server.com/path/to/manifest/smooth.ism/Manifest\nPlayback with Bitmovin Player\nAfter you have encoded and encrypted the content, you want your users to play it back according to the licenses they bought.\nVualto has prepared a page describing how to play back protected content with the Bitmovin player:\nhttps://docs.vualto.com/projects/vudrm/en/latest/DeveloperDocumentation/integrations/players/bitmovin.html",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/default-vs-custom-manifest",
    "title": "Default vs custom manifests",
    "text": "Overview\nThe Bitmovin API offers two approaches for configuring manifests, allowing for different degrees of customization regarding the exact structure of the manifest file and its contents (video and audio renditions, thumbnails, sprites, subtitles, DRM information,...).\nA \"default manifest\" is the worry-free option that will be configured fully automatically, depending on what output your encoding creates. This is a good starting point for most low to medium complexity workflows.\nIf you need more control, go for a custom manifest.\nDefault manifests\nA default manifest will automatically include every muxing and DRM output of the specified encoding. This is a typical requirement that covers a vast majority of use-cases. There is no need (and no possibility) to take any more control of the manifest content.\nThe only properties required for creating a default manifest resource are\nencodingId\nand\noutputs\n(specifying the storage location where the manifest is to be written to).\nAPI endpoints\nCreating a Default Manifest resource:\nDASH:\nCreate Default DASH Manifest\nHLS:\nCreate Default HLS Manifest\nSmooth Streaming:\nCreate Default Smooth Streaming Manifest\nTo start manifest generation, choose one of the two options described in\nCreating manifests with the Bitmovin API\n.\nExample for DASH and HLS\nJava\n...\nexecuteEncoding(encoding);\n\ngenerateDashManifest(encoding, output, \"/\");\ngenerateHlsManifest(encoding, output, \"/\");\n\n...\n\nprivate static void generateDashManifest(Encoding encoding, Output output, String outputPath) throws Exception {\n    DashManifestDefault dashManifestDefault = new DashManifestDefault();\n    dashManifestDefault.setEncodingId(encoding.getId());\n    dashManifestDefault.setManifestName(\"stream.mpd\");\n    dashManifestDefault.addOutputsItem(buildEncodingOutput(output, outputPath));\n\n    dashManifestDefault = bitmovinApi.encoding.manifests.dash.defaultapi.create(dashManifestDefault);\n    executeDashManifestCreation(dashManifestDefault); //Start manifest creation, and wait until its state is either FINISHED, or ERROR\n}\n\nprivate static void generateHlsManifest(Encoding encoding, Output output, String outputPath) throws Exception {\n    HlsManifestDefault hlsManifestDefault = new HlsManifestDefault();\n    hlsManifestDefault.setEncodingId(encoding.getId());\n    hlsManifestDefault.addOutputsItem(buildEncodingOutput(output, outputPath));\n    hlsManifestDefault.setName(\"master.m3u8\");\n    hlsManifestDefault.setVersion(HlsManifestDefaultVersion.V1);\n    \n    hlsManifestDefault = bitmovinApi.encoding.manifests.hls.defaultapi.create(hlsManifestDefault);\n    executeHlsManifestCreation(hlsManifestDefault); //Start manifest creation, and poll status until its state is either FINISHED, or ERROR\n}\nThis is a snippet from the\nGenerating Default Manifests\ncode example available in multiple languages on Github.\nCustom manifests\nWhile being the most flexible approach, its also the most advanced one as you are configuring a manifest from scratch. However, it does enable you to deal with use-cases that are often encountered in post-production workflows, or to meet special requirements like\nspecialized manifests for different platforms,\nmanifests with a limited set of renditions of SD-subscribers of your service,\nre-creating manifests for contents where an additional audio/video/subtitle track became available (these tracks have to be encoded in the same way as the already exisiting ones, otherwise they won't comply with manifest requirements)\nand more.\nSo, creating a custom manifest enables you to utilize the flexibility of streaming formats, and cross-reference the outputs of different encodings, and create them exactly how you want them to be. You do need to know how MPEG-DASH or HLS manifests are structured and work, in order to do that as you have to create their structure as needed.\nAPI endpoints\nDASH:\nCreate Custom DASH Manifest\nHLS:\nCreate Custom HLS Manifest\nSmooth Streaming:\nCreate Smooth Streaming Manifest\nNote that for custom configurations, the\nManifest\nresource is only the starting point. Additional endpoints are available that allow adding sub-resources to it, depending on the manifest type (e.g. Periods for DASH, Media elements for HLS).\nWhen you've created all desired resources, your manifest is ready for generation. To start manifest generation, choose one of the two options described in\nCreating manifests with the Bitmovin API\n.\nThe following example outlines this process for a DASH manifest.\nExample for DASH\nCreating a very basic DASH Manifest (with some video and audio renditions) requires the following steps:\nCreate a\nDashManifest\nresource:\nCreate Custom DASH Manifest\nAdd a\nPeriod\nto it:\nAdd Period\nAdd a\nVideoAdaptationSet\nto the\nPeriod\n:\nAdd Video AdaptationSet\nAdd an\nFmp4Representation\nfor each video representation to the\nVideoAdaptationSet\nyou want to be available to a player for bitrate adaptation:\nAdd fMP4 Representation\nAdd a\nAudioAdaptationSet\nto the\nPeriod\n:\nAdd Audio AdaptationSet\nAdd an\nFmp4Representation\nfor each audio representation to the\nAudioAdaptationSet\nyou want to be available to a player for bitrate adaptation:\nAdd Audio AdaptationSet\nStart the manifest generation:\nStart DASH Manifest Creation\nComparison\nDefault manifests\nCustom manifests\nOffer one simple solution that fits most use-cases\nBuild manifests to meet the needs for complex video delivery workflows\nConfiguring requires nothing more than an encoding ID\nAll content needs to be added to the manifest explicitly. This means the IDs of the relevant encoding resources (streams, muxings, DRMs, thumbnails,...) need to be known. Either your encoding workflow needs to store this metadata or you retrieve it at a later time via the Bitmovin API.\nDoes not allow combining output of multiple encodings\nAllows combining output of multiple encodings in one manifest (when using\npost-encoding\ngeneration), e.g. to augment an existing asset with an additional audio track.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/supported-formats-storage",
    "title": "Supported Formats & Storage",
    "text": "These subpages list the array of codecs, containers and storage platforms that are supported by the Bitmovin VOD and Live Encoding products.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/troubleshooting-output-transfer-failures",
    "title": "Troubleshooting Output Transfer Failures",
    "text": "There are multiple reasons behind why the encoding output might not transfer correctly. Let's go through them one by one:\nWrong credentials\n: please make sure you are providing the right credentials in the code.\nPermission issues\n: is the use you provided got the right write access?\nFTP from Akamai got some\nlimits\n(e.g. 25 parallel connections at max per Akamai user account). The Akamai NetStorage HTTP API can handle higher loads and connections and is more resilient, therefore we recommend to use this one by default, instead of Akamai FTP if possible, you can find more about the setup from here\nhttps://bitmovin.com/docs/encoding/tutorials/how-to-setup-an-akamai-netstorage-output\nIs the cloud region of your storage’s region experiencing an\noutage\n? Check the relevant status page for any possible issues, for example for Amazon\nhttps://status.aws.amazon.com/",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/digital-rights-management-drm-overview",
    "title": "Digital Rights Management (DRM) Overview",
    "text": "What is a DRM?\nDigital Rights Management (DRM) systems provide you the ability to control how people can consume your content. Usually content owners and producers like all major Hollywood Studios, TV Stations, etc. enforce others that buy or rent content to use specific DRM systems to protect the content according to the constraints associated with the specific content. Hollywood grade DRM protection is not always needed and sometimes it’s enough to provide basic protection through token based secure authentication or simple AES encryption of the video without sophisticated license exchange and policy management.\nHow does it work?\nA DRM setup needs specific encoding, packaging, player and a license server. Bitmovin provides the encoding, packaging and the player. For license servers we refer to our partners from Irdeto, EZDRM, ExpressPlay and Axinom, which provide a multi DRM setup. It’s also possible to build your own license servers and negotiate terms directly with Google (Widevine), Microsoft (PlayReady), Adobe (PrimeTime) or Apple (Fairplay), but it usually takes longer.\nEncoding & Packaging\nFrom an encoding and packaging perspective it makes not much difference if the video is ‘just’ AES encrypted or Hollywood grade DRM encrypted because for the encryption in both cases AES is used. The major difference is that for Hollywood grade DRM’s further metadata information needs to be added in the packaging step. Hollywood grade DRM’s like\nPlayReady\n,\nWidevine\n,\nPrimeTime\nand\nFairplay\ndon’t differ on the encryption side they differ on the features that are provided. Features such as offline playback, fine grained policies (e.g., allow only SD playback, rights visibility for users, APIs, different payment modes such as subscription, purchase, rental and gifting, etc.) and\nplatforms that are supported\n(e.g., Chrome, Firefox, IE, Safari, Android, iOS, etc.) with the DRM system.\nMulti DRM and MPEG-CENC\nTypically,\neach device supports just one DRM\n. If you want to achieve maximum device reach it’s impossible to use just one DRM you need to use multiple DRM’s in parallel. The MPEG Common Encryption (MPEG-CENC) standard enables this in the most efficient way as it allows key association from different DRM’s with the same video. This means that your video can be encoded and encrypted once with the same key. Metadata for the different DRM’s will be added in the packaging step. The details of the license acquisition, license mappings, etc. will be left out to the individual DRM system. The player decides based on the platform support which specific DRM will be used.\nTraditional multi DRM setups needed to encrypt and package the content for each DRM differently. This increases the storage footprint of the content as each video needs to be encrypted and packaged with every DRM system and stored separately. Furthermore, each video also needs to be encoded into multiple resolutions and bitrates to serve different devices and then each encoding needs to be encrypted and packaged with all the different DRMs. This would not only increase the storage footprint tremendously it also increases the management efforts, because another service needs to keep track of this different versions. Beside that it reduces the efficiency of your CDN if so many different versions for each DRM are distributed.\nPlayback\nOn the player side we utilize the\nHTML5 Encrypted Media Extensions\n(EME) to enable DRM playback without plugins. If the DRM is not supported through the EME we can always fallback to Flash and Adobe Access. If the content is MPEG-CENC multi DRM encrypted the player will automatically choose the DRM that is natively supported on the given platform to playback the content in HTML5 without plugins. The player handles the authentication and the license acquisition through the EME with the metadata that is provided with the content.\nLicensing Server\nThe licensing server is the management backend of your DRM setup. It allows you to create, modify and revoke licenses for your content and users. Licensing servers and DRM’s differ in their features such as offline playback, fine grained policies, rights visibility for users, APIs, different payment (subscription, purchase, rental and gifting), etc. We do not provide licensing servers but we partner with several companies that provide such servers and services such as\nIrdeto\n,\nEZDRM\n,\nExpressPlay\n,\nAxinom\n,\nDRMtoday\n,\nConax\nand\nBuyDRM\n. It’s also possible to create your own licensing backend if you have a contract with Google (Widevine), Microsoft (PlayReady), Adobe (PrimeTime) or Apple (Fairplay) directly and you implement the specification. You can then also integrate your licensing backend with our services as long as it follows the official specifications.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/79b568d-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/gcp-buckets-for-live-outputs",
    "title": "GCS Buckets for Live Outputs",
    "text": "Overview\nBitmovin can not only run in Google Cloud, deploying the Live Encoder to a range of regions, but it can also output to a GCS bucket as an origin server for distribution directly or via a CDN.\nConfiguring GCS buckets\nWhen using GCS buckets as an output for the Live Encoder you will need to configure them to grant the appropriate roles and access control policies.\nOutput Buckets\nGCS role\n- The Storage Object Admin role must be added to the relevant IAM user.\nGCS Access\n- As long as the proper GCS role was added, Output Buckets can be configured either as publicly accessible or private.\nGCS Access Control Policy and Bitmovin ACL\n- The bucket access control policy and the Bitmovin ACL settings must be set according to the table below.\nGCS role (for IAM user)\nGCS Access\nGCS Access Control Policy\nBitmovin ACL\nStorage Object Admin\nPublic / Private\nUniform\n1\nprivate\nStorage Object Admin\nPublic / Private\nFine-Grained\n2\nprivate/public_read\n1\nWhen using Uniform access control policy type, GCS applies access control policies to all objects in the Bucket based on the IAM permissions - and all access granted by object ACLs are revoked. To get Bitmovin outputs working properly with this approach, all\nACLs\nin\nEncodingOutput.acl[*].permission\nmust be set to private\n2\nWhen using Fine-Grained access control policy type, GCS applies access control policies based on\nAccess Control Lists (ACLs)\n. In this way, the ACLs set in\nEncodingOutput.acl[*].permission\nare applied to each object to be written in the bucket.\nTake into account that Bitmovin ACLs in\nEncodingOutput.acl[*].permission\nare set to\npublic_read\nby default - for example when creating encoding jobs from the Bitmovin Dashboard, so if you do not set an ACL explicitly, you should set a\nFine-Grained\ncontrol policy type on the bucket in order to get it working\nConfiguring Live Encoding Outputs\nIn the Dashboard navigate to Live Encoding in the left side panel, and then select Outputs.\n📘\nYou will need to enter the Access key and Secret Key provided via the GCS console or your GCS system administrator.\nPress\n+ Create\nand select GCS to see the correct form.\nWhen you are finished press\nCreate\nand the output will be saved.\nUsing the Live Output\nThe bucket will appear in the Outputs list, and in the Wizard under Google Storage.\nYou can confirm the bucket is created in the API by using\nList GCS Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/beec332-Screenshot_2024-04-05_at_18.41.26.png",
      "https://files.readme.io/faaa72a-Screenshot_2024-04-05_at_18.49.56.png",
      "https://files.readme.io/0395007-Screenshot_2024-04-06_at_11.41.30.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/managing-api-keys",
    "title": "Managing API Keys",
    "text": "You can manage API Keys in the Bitmovin Dashboard via your\nAccount Settings\n:\n⚠️\nNote\nEvery API Key has root level access to your Bitmovin Account and can manage all resources. We would recommend to create groups via your\nOrganization Settings\nand grant permissions as needed.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/2093ffb1391421ba0bd9ed6009e8c9c183951551ea415d18fe11fbeff2805fdb-Screenshot_2024-10-17_at_4.33.28_PM.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-dolby-digital-plus-encodings",
    "title": "Creating Dolby Digital (Plus) Encodings",
    "text": "Overview\nBitmovin and its Encoding Service is officially and fully certified by Dolby to create Dolby Digital (Plus) audio content! This is great news as it enables you to easily deliver certified High Quality Audio content to your end users and their dolby digital compatible end devices! In this tutorial we will walk through the specific features of this new\nAudio Codec Configuration\nobject for Dolby Digital (Plus) and when/how to use them.\nIf you want to check out all the details in the API reference right away, please see the API description for\nDolby Digital\nand\nDolby Digital Plus\n.\nRequirements\nEncoder version 2.83.0 or higher\nAPI SDK version 1.78.0 or higher\nKnown Limitations\nSupport Workflows:\nVoD only\nAudio Sample rate:\n48kHz only\nSupported Muxings:\nfMP4, MP4, MOV(DD only), TS (incl. BroadcastTS, ProgressiveTS)\nSupported Streaming Formats:\nMPEG-DASH, HLS\nSupported Bitrates:\nDepends on Audio Coding Mode for DD/DD+ (Please see\nAudio Coding Modes\n)\nSupported Channel Layouts:\nup to 5.1 (Loudness control is required to be enabled at all times, which supports up to 6 channels).\nSupported Channel Layouts for Downmixing:\n5.1 or lower\nDolby Audio Codec Configuration Details\nAs for every Audio Codec Configuration, a\nDolby Digital\nor\nDolby Digital Plus\nAudio Codec Configuration requires at least:\nname\n,\nbitrate\nThe latter and its value depends on the Channel layout you're using for your encoded content. Therefore, please make sure to select the appropriate, supported value within the given range from the table below.\nAudio Coding Modes\nDolby Digital - Bitrates\nBitmovin Channel Layout\nAudio Coding Mode\nMin kbps\nMax kbps\nDefault kbps\nMONO\n1/0\n56\n640\n96\nSTEREO\n2/0\n96\n640\n192\nBACK_SURROUND, BACK_SURROUND_LFE\n2/1\n160\n640\n256\nSURROUND, 3.1\n3/0\n160\n640\n256\n4.0, 4.1\n3/1\n192\n640\n320\nQUAD, QUAD_LFE\n2/2\n192\n640\n320\n5.0, 5.1\n3/2\n224\n640\n384\nIndividual supported bitrates: 56, 64, 80, 96, 112, 128, 160, 192, 224, 256, 320, 384, 448, 512, 576, 640\nDolby Digital Plus - Bitrates\nBitmovin Channel Layout\nAudio Coding Mode\nMin kbps\nMax kbps\nDefault kbps\nMONO\n1/0\n32\n1024\n64\nSTEREO\n2/0\n96\n1024\n128\nBACK_SURROUND, BACK_SURROUND_LFE\n2/1\n128\n1024\n160\nSURROUND, 3.1\n3/0\n128\n1024\n160\n4.0, 4.1\n3/1\n160\n1024\n192\nQUAD, QUAD_LFE\n2/2\n160\n1024\n192\n5.0, 5.1\n3/2\n192\n1024\n192\n~Not available yet~\n~3/3~\n~256~\n~1024~\n~384~\n~Not available yet~\n~3/4~\n~384~\n~1024~\n~384~\nIndividual supported bitrate values: 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 144, 160, 175, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1008, 1024\nDolby Digital Audio Codec Features\nDolby Digital\nand\nDolby Digital Plus\ndo support certain features like\nloudness control\n,\npre-processing\nand\ndownmixing\n.\nLoudness Control\nIt allows you to either manually adjust the eventual loudness of the encoded audio content on your own, or to apply defined loudness correction methods like\nEBU R128\n,\nATSC A85 FIXED\n, or\nATSC A85 AGILE\nto align the loudness levels of your contents and therefore to enhance the experience for your viewers.\nThe Loudness Control feature is always enabled\nand therefore a required property. If not explicitely configured, the\nmode\nis set to\nCORRECTION\nand\nregulationType\nto\nATSC_A85_FIXED\nby default. You can choose between the modes\nPASSTHROUGH\nor\nCORRECTION\nto configure how it is applied to your content. Please see the following table which\nmode\nand loudness control features are compatible\nMode\nregulationType\npeakLimit\ndialogueIntelligence\ndialnorm\nPASSTHROUGH\nnot supported\n✅\n✅\n✅\nCORRECTION\nMANUAL\n✅\n✅\n✅\nCORRECTION\nEBU_R128, ATSC_A85_FIXED, ATSC_A85_AGILE\nnot supported\nnot supported\nnot supported\nPre-Processing\nPre-Processing allows to prepare for characteristics of your input file. Please mind the table below as not every feature supports every channel layout/audio coding mode.\nLFE Low Pass Filter\nis disabled by default. If enabled, it filters frequencies above 120Hz on the LFE channel, when enabled.\n90° Phase Shift\nis enabled by default. Its applied to the surround channels during encoding to decorrelate front and back channels. This is done to ensure dolby surround compatible output if the encoded multi-channel content gets downmixed, so these front and back channels don't cancel each other out.\n3dB Attenuation\nis disabled by default. Enable it, if your input file is master file and comes from a film room, or wasn't yet converted to a home theater format. Cinema surround channels are mixed 3 dB higher to account for amplifier gains in cinemas, which can be corrected with this option.\nProperty\nSupported Bitmovin Channel Layout\nSupported Audio Coding Modes\nlfeLowPassFilter\nBACK_SURROUND, BACK_SURROUND_LFE, QUAD, QUAD_LFE,4.0, 4.1, 5.0, 5.1\n2/1, 2/2, 3/2, 3/1\nninetyDegreePhaseShift\nBACK_SURROUND, BACK_SURROUND_LFE, QUAD, QUAD_LFE,4.0, 4.1, 5.0, 5.1\n2/1, 2/2, 3/1, 3/2\nthreeDbAttenuation\nBACK_SURROUND, BACK_SURROUND_LFE, QUAD, QUAD_LFE,4.0, 4.1, 5.0, 5.1\n2/1, 2/2, 3/1, 3/2\nDownmixing\nThis feature allows to either remove certain channels to achieve a different Channel layout, or to adjust the loudness level of certain channels by a fixed amount (depends on the channel you want to adjust).\nPlease see table below to check which option is available for which channel layout.\nProperty\nBitmovin Channel Layout\nSupported Audio Coding Modes\nloRoCenterMixLevel\nCL_SURROUND, 3.1, 4.0, 4.1, 5.0, 5.1\n3/0, 3/1, 3/2\nltRtCenterMixLevel\nCL_SURROUND, 3.1, 4.0, 4.1, 5.0, 5.1\n3/0, 3/1, 3/2\nloRoSurroundMixLevel\nBACK_SURROUND, BACK_SURROUND_LFE, QUAD, QUAD_LFE,4.0, 4.1, 5.0, 5.1\n2/1, 2/2, 3/1, 3/2\nltRtSurroundMixLevel\nBACK_SURROUND, BACK_SURROUND_LFE, QUAD, QUAD_LFE,4.0, 4.1, 5.0, 5.1\n2/1, 2/2, 3/1, 3/2\npreferredMode\nBACK_SURROUND, BACK_SURROUND_LFE, QUAD, QUAD_LFE, CL_SURROUND, 3.1, 4.0, 4.1, 5.0, 5.1\n3/0, 2/1, 2/2, 3/1, 3/2\nCreate an Encoding\nThis tutorial is based on the already existing\nMulti-Codec Encoding\nexample, which was using AC3 codec configurations before, but due to its replacement by Dolby Digital Audio Codec Configuration, is using Dolby Digital Audio Codec Configurations by now.\nAs with every encoding, we will need an\nEncoding\nObject, that holds all related\nstream\nobjects and their\nconfiguration\n, as well as an\nInput\nand\nOutput\nObject.\nCreate or Reuse an Input\nHTTP is just used for simplicity in this example, as there is no limitation in where the input file can be downloaded from for that use-case. Of course, you can also reuse existing Input objects available in your Bitmovin Account.\nJava\nHttpInput input = createHttpInput(configProvider.getHttpInputHost());\nCreate or Reuse an Output\nAWS S3 is just used for simplicity in this example, as there is no limitation in where the content can be written to. Of course, you can also reuse existing Output objects available in your Bitmovin Account.\nJava\nOutput output =\n        createS3Output(\n            configProvider.getS3OutputBucketName(),\n            configProvider.getS3OutputAccessKey(),\n            configProvider.getS3OutputSecretKey());\nCreate an Encoding Object\nJava\nEncoding encoding = createEncoding(\"H.265 Encoding\", \"H.265 -> fMP4 muxing, Dolby Digital -> fMP4 muxing\");\nCreate an Video Stream using an H265 Video Codec Configuration\nA\nStream\nObject effectively maps a stream of the input file to a given\nCodecConfiguration\nwhich will be used to encode it.\nJava\nH265VideoConfiguration videoConfiguration =\n          createH265VideoConfig(rendition.height, rendition.bitrate);\n      Stream videoStream = createStream(encoding, input, inputFilePath, videoConfiguration);\nCreate an fMP4 Muxing using the Video Stream\nJava\nFmp4Muxing fmp4Muxing =\n          createFmp4Muxing(\n              encoding,\n              output,\n              String.format(\n                  H265AndDolbyDigitalEncodingTracking.H265_FMP4_SEGMENTS_PATH_FORMAT,\n                  rendition.height,\n                  rendition.bitrate),\n              videoStream);\nCreate an Audio Stream using an Dolby Digital Audio Codec Configuration\nA\nStream\nObject effectively maps a stream of the input file to a given\nCodecConfiguration\nwhich will be used to encode it.\nPlease\nJava\nDolbyDigitalAudioConfiguration dolbyDigitalConfig = createDolbyDigitalAudioConfig();\nencodingTracking.dolbyDigitalAudioStream = createStream(encoding, input, inputFilePath, dolbyDigitalConfig);\ncreateDolbyDigitalAudioConfig()\ncreate a new Audio Codec Config in your account as usual, and utilizes its default configurations by default.\nJava\nprivate static DolbyDigitalAudioConfiguration createDolbyDigitalAudioConfig() {\n    DolbyDigitalAudioConfiguration config = new DolbyDigitalAudioConfiguration();\n    config.setName(\"DolbyDigital Channel Layout 5.1\");\n    config.setBitrate(256_000L);\n    config.setChannelLayout(DolbyDigitalChannelLayout.CL_5_1);\n    return bitmovinApi.encoding.configurations.audio.dolbyDigital.create(config);\n}\nCreate an fMP4 Muxing using the Audio Stream\nThe creation of a muxing is no different than before (If supported, please see\nknown limitations\nfor details about supported muxing types). You only have to provide it with the stream that is configured to use the Dolby Digital (Plus) Audio Codec Configuration.\nJava\ndolbyDigitalFmp4Muxing =\n        createFmp4Muxing(\n            encoding,\n            output,\n            H265AndDolbyDigitalEncodingTracking.DOLBY_DIGITAL_FMP4_SEGMENTS_PATH,\n            encodingTracking.dolbyDigitalAudioStream);\nStart the Encoding\nThe Multi-Codec Example, as per best practise, launches three separate encodings, one per Video Codec. So the example we use here launches three threads and monitors each encoding for its completion.\nJava\nList<Callable<Encoding>> encodingTasks =\n        Arrays.asList(\n            () -> executeEncoding(h264AndAacEncodingTracking.encoding),\n            () -> executeEncoding(h265AndDolbyDigitalEncodingTracking.encoding),\n            () -> executeEncoding(vp9AndVorbisEncodingTracking.encoding));\nexecutor.invokeAll(encodingTasks);\nexecutor.shutdown();\nOther Examples\nThe example used in this tutorial is available for all available Bitmovin API SDK's in our Github Repository. Check out the\nMultiCodecEncoding\nexample for the SDK of your choice!\nJava\n|\nJavascript/Typescript\n|\nPython\n|\n.NET\n|\nPHP\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-combined-multi-drm-protected-content",
    "title": "Creating Combined Multi-DRM Protected Content",
    "text": "Overview\nThe Bitmovin API and player allows you to use multiple DRMs in parallel. This means that you encode, encrypt and package your content once and you can playback with several different DRMs, such as Widevine, PlayReady, PrimeTime, etc. This is especially important if you want to increase your device reach.\nDue to\nfragmentation in the market\nit is not possible to reach all major devices with just one DRM. Therefore, you need to use multiple in parallel, which is possible as all DRM systems use AES for encryption. If you use the same key in the different DRM systems for the same video you just need to add additional metadata for each DRM to this video and then it can be played back by with DRM systems. In detail it’s a little bit more complex as this needs additional logic on the encoding as well as on the player side, but Bitmovin provides you with solutions for both.\nWhat you also need for such a setup is a 3rd party multi DRM provider such as\nIrdeto\n,\nEZDRM\n,\nExpressPlay\n,\nAxinom\n, etc. It’s also possible to create your own licensing backend if you have a contract with Google (Widevine), Microsoft (PlayReady), Adobe (PrimeTime) or Apple (Fairplay) directly and you implement the specification.\nCombined DRM Requirements\n\"combined DRM\" means that you only have to encode your content once, but you can use with different DRM solutions. MPEG-CENC is an encryption standard that is independent from DRM providers and allows the decryption of the same media file (encoded and encrypted once) using multiple DRM systems (Widevine, PlayReady, etc.) to support various client devices.\nFairPlay DRM does not support it at the moment, as it encrypts its content differently, so you have to encode your content again with the encryption method that is supported by the DRM solution.\nAbout this example\nThe code snippets shown here are based on the\nfull example\ncalled\nCencDrmContentProtection.java\n, using our\nBitmovin SDK for Java\n.\nHint:\nIf you haven't created any encodings with our Service yet, its recommended to start with our quick start guide called \"\nGet Started with the Bitmovin API\n\" first, before you continue :)\nEncoding with DRM Configuration\nThe key part to create an encoding that encodes and encrypt content with DRM solutions that support MPEG-CENC, is to add a\nCencDRM\nConfiguration to a\nMuxing\n.\nTo encrypt your content so it can be used with Widevine DRM, an\nencryption key\n(referred to as\nkey\nlater on) is required. All other values are optional, however sometimes required by specific DRM vendors, therefore have to be set (\nkid\n,\npssh\n, ...).\nGeneral configuration values:\nkey\n: (required) You need to provide a key that will be used to encrypt the content (16 byte encryption key, represented as 32 hexadecimal characters)\nkid\n: (optional) also known as Key ID, or ContentID. Its a unique identifer for your content (16 byte initialization vector, represented as 32 hexadecimal characters)\nHINT:\nSome DRM providers provide you with a dedicated service to create and safely store Encryption Keys, so you don't have to create and manage them by yourself. These values, along with the Apple certificate, are required to generate a proper playback license using DRM solution providers like Irdeto, EZDRM, ExpressPlay, Axinom, etc. to control playback permissions on the client side.\nLearn more\n.\nWidevine DRM\nspecific configuration options:\npssh\n: Base64 encoded String, PSSH payload Example:\nQWRvYmVhc2Rmc2FkZmFzZg==\nPlayReady DRM\nspecific configuration options: You can either provide an\npssh\nstring or an\nlaUrl\nin this configuration.\npssh\n: (optional)Base64 encoded String, PSSH payload Example:\nQWRvYmVhc2Rmc2FkZmFzZg==\nlaUrl\n: (optional) The License Aquistion URL that shall be used by the player.\nFairPlay DRM\nspecific configuration options:\niv\n: The initialization vector is optional. If it is not provided we will generate one for you. (16 byte initialization vector, represented as 32 hexadecimal characters)\nuri\n: If provided, this URI will be used for license acquisition, (e.g. skd://userspecifc?custom=information)\nJava SDK Example - createDrmConfig() Method\n(\nLine in Example\n)\nJava\nprivate static CencDrm createDrmConfig(\n    Encoding encoding, Muxing muxing, Output output, String outputPath) throws BitmovinException {\n  CencDrm cencDrm = new CencDrm();\n  cencDrm.addOutputsItem(buildEncodingOutput(output, outputPath));\n  cencDrm.setKey(\"cab5b529ae28d5cc5e3e7bc3fd4a544d\");\n  cencDrm.setKid(\"08eecef4b026deec395234d94218273d\");\n  \n  CencWidevine widevineDrm = new CencWidevine();\n  widevineDrm.setPssh(\"QWRvYmVhc2Rmc2FkZmFzZg==\");\n  cencDrm.setWidevine(widevineDrm);\n  \n  CencPlayReady playReadyDrm = new CencPlayReady();\n  playReadyDrm.setLaUrl(\"http://playready.directtaps.net/pr/svc/rightsmanager.asmx\");\n  //playReadyDrm.setPssh(\"QWRvYmVhc2Rmc2FkZmFzZg==\");\n  cencDrm.setPlayReady(playReadyDrm);\n  \n  CencFairPlay cencFairPlay = new CencFairPlay();\n  cencFairPlay.setIv(\"e7fada52ac654bbe80367505dceeb318\");\n  cencFairPlay.setUri(\"skd://userspecifc?custom=information\");\n  cencDrm.setFairPlay(cencFairPlay);\n  \n  return bitmovinApi.encoding.encodings.muxings.fmp4.drm.cenc.create(\n      encoding.getId(), muxing.getId(), cencDrm);\n}\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rtmp-inputs",
    "title": "RTMP Inputs",
    "text": "The Live Encoder supports both RTMP and RTMPS protocols.\n📘\nWe offer support for Main and Backup RTMP/RTMPS inputs to each Live Encoder as standard.\nIn this guide we'll explain how to configure inputs using both the API and UI.\nIt's important to note that Inputs can be configured in two different modes of operation.\nIP Based RTMP ingest\nStatic RTMP/RTMPS ingest\nThe main differentiator between these two modes is based on the user requirements to set the application and stream key. A table illustrating these differences is shown below.\nFeature\nIP based RTMP ingest\nStatic RTMP Ingest\nMain and Backup Inputs\n✅\n✅\nIP based URL\n✅\nUser configurable back-up application\n✅\nUser configurable stream key\n✅\n✅\nAutomatically generated UUID stream key\n✅\nReserved stream keys\n✅\nStatic DNS based URL\n✅",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/pitch-shifting-in-encoded-audio-when-changing-tracks",
    "title": "Pitch Shifting in Encoded Audio When Changing Tracks",
    "text": "This issue can occur with MSE implementations that do not correctly handle different sample rates for audio. If you manually set the sample rate in your audio codec configuration, ensure, that for every audio track you use the same sample rate. If you do not choose an explicit sample rate in your audio codec configuration, the Bitmovin API will encode the audio in the same sample rate as the input audio stream.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-akamai-object-storage-for-encoding",
    "title": "Using Akamai Object Storage for Encoding",
    "text": "In order to use Akamai Object Storage (previously called Linode) in your encodings, you'll need to leverage our S3-generic API endpoint, as shown below:\ncURL\ncurl --location --request POST 'https://api.bitmovin.com/v1/encoding/outputs/generic-s3' \\\n--header 'X-Api-Key: {your api key}' \\\n--header 'X-Tenant-Org-Id: {your orgID}' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n\"accessKey\": \"{your bucket's access key}\",\n\"secretKey\": \"{your bucket's secret key}\",\n\"bucketName\": \"my-linode-bucket\",\n\"host\": \"it-mil-1.linodeobjects.com\",\n\"port\": 443,\n\"ssl\": true,\n\"signatureVersion\": \"S3_V4\"\n}'\nIt's important to choose signature version 4 and, when firing the encoding, select the input/output ID created above, choosing encoder version at least 2.148.0.\nYou can conveniently add Akamai Object Storage as Input (for VOD) and Output for (VOD and LIVE) using Bitmovin dashboard:",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/c1645c776d040540d3c4b5376da42a379cc1144ede857e4e7b769c3fbbd7f0ce-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-dtshd-dtsx-encodings",
    "title": "Creating DTS:HD / DTS:X Encodings",
    "text": "Overview\nBitmovin and its Encoding Service is officially and fully certified by Xperi to create certified DTS:HD and DTS:X audio content! This is great news as it enables you to easily deliver certified High Quality Audio content to your end users and their DTS compatible end devices! In this tutorial we will walk through the specific features of this new Audio Codec Configuration object for DTS:HD/DTS:X and when/how to use them.\nRequirements\nEncoder Version:\nv2.88.0 or higher\nAPI SDK Version:\nv1.83.0 or higher\nKnown Limitations\nSupport Workflows:\nVoD only\nDRM:\nnot supported yet\nAudio Sample rate:\n48kHz only\nAudio Sample Format:\n16, 24 Bit\nSupported Muxings:\nMP4, segmented RAW\nSupported Streaming Formats:\nMPEG-DASH (SegmentBase only)\nHLS (EXT-X-BYTE-RANGE only)\nDTS:HD Specific Limitations\nSupported Channel Layouts:\n5.1 only\nSupported Bitrates:\n255 kbps, 384 kbps, 768kbps\nSupported Modes:\nExpress, Digital Surround\nDTS:X Specific Limitations\nSupported Channel Layouts:\n5.1, 5.1.4\nSupported Bitrates:\n5.1: 160, 192, 224, 256, 288, 320 kbps\n5.1.4: 288, 384, 448 kbps\nDTS:HD Codec Configuration Details\nDTS:HD comes with two different modes -\nEXPRESS\n, and\nDIGITAL SURROUND\n. While both support 5.1 Channel layouts,\nDIGITAL SURROUND\nis used mostly for audio on DVD's. DTS Express also supports 5.1, and further allows for a higher compression rate as it was developed for audio streaming services.\nAs for every Audio Codec Configuration, a DTS:HD Codec Configuration requires at least:\nname\nand\nbitrate\n.\nSupported Bitrates by Mode:\nDTS Express: 255, 384 kbps\nDTS Digital Surround: 768 kbps\nUsing\nEXPRESS\nmode:\nJava\nprivate static DtsAudioConfiguration createDtsHdExpressAudioConfig() {\n    DtsAudioConfiguration config = new DtsAudioConfiguration();\n    config.setMode(DtsMode.EXPRESS_AUDIO);\n    config.setBitrate(255000L);\n    //config.setBitrate(384000L);\n    return bitmovinApi.encoding.configurations.audio.dts.create(config);\n}\nUsing\nDIGITAL_SURROUND\nmode:\nJava\nprivate static DtsAudioConfiguration createDtsHdDigitalSurroundAudioConfig() {\n    DtsAudioConfiguration config = new DtsAudioConfiguration();\n    config.setMode(DtsMode.DIGITAL_SURROUND);\n    config.setBitrate(768000L);\n    return bitmovinApi.encoding.configurations.audio.dts.create(config);\n}\nDTS:X Codec Configuration Details\nCompared to DTS:HD, DTS:X supports besides 5.1, also 5.1.4 as channel format. The third digit refers to 4 additional channels which are firing from above the same points as the left, right, back left and back right channel.\nSupported Loudness Control Modes:\nDTSX_OTT_LOUDNESS_INPUT\n: The\nlkfs\nvalue has to be provided by the user as part of the audio codec configuration, and will be stored in the encoded content.\nDTSX_OTT_LOUDNESS_TARGET\n: the\nlkfs\nvalue has to be provided by the user as part of the audio codec configuration, and the loudness of the source audio will be adjusted accordingly.\nDTSX_OTT_LOUDNESS_DETECT\n: It determines the loudness of the source file and stored in the encoded content. An\nlkfs\nvalue, if provided, will be ignored.\nAudio Codec Configuration Example:\nJava\nprivate static DtsXAudioConfiguration createDtsXAudioConfig() {\n    DtsXAudioConfiguration dtsXAudioConfiguration = new DtsXAudioConfiguration();\n    dtsXAudioConfiguration.setBitrate(320000L);\n    dtsXAudioConfiguration.setChannelLayout(DtsXChannelLayout.CL_5_1);\n    dtsXAudioConfiguration.setOttLoudnessMode(OttLoudnessMode.DTSX_OTT_LOUDNESS_INPUT);\n    dtsXAudioConfiguration.setLkfsValue(-23.0d); //reduces loudness by -23dB\n    dtsXAudioConfiguration.setSyncInterval(5L); //place a audio synchronization frame every 5 seconds\n    return bitmovinApi.encoding.configurations.audio.dtsx.create(config);\n}",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-playready-drm-protected-content",
    "title": "Creating PlayReady DRM Protected Content",
    "text": "Overview\nMicrosoft released PlayReady in 2008 and it’s one of major DRM systems out in the market with broad\ndevice support\n, sophisticated features and has been used at scale already by many events such as the Olympics in Sochi, Russia. The Bitmovin encoding service supports PlayReady encryption and packaging with MPEG-CENC. The Bitmovin player plays PlayReady encrypted videos on platforms that support the PlayReady DRM natively in HTML5 without plugins.\nPlayReady DRM Requirements\nA multi DRM licensing server provider such as Irdeto, EZDRM, ExpressPlay, Axinom or others.\nAbout this example\nThe code snippets shown here are based on the\nfull example\ncalled\nCencDrmContentProtection.java\n, using our\nBitmovin SDK for Java\n.\nHint:\nIf you haven't created any encodings with our Service yet, its recommended to start with our quick start guide called \"\nGet Started with the Bitmovin API\n\" first, before you continue :)\nEncoding with DRM Configuration\nThe key part to create an encoding that encodes and encrypt content with DRM solutions that support MPEG-CENC, is to add a\nCencDRM\nConfiguration to a\nMuxing\n.\nTo encrypt your content so it can be used with PlayReady DRM, an\nencryption key\n(referred to as\nkey\nlater on) is required. All other values are optional, however sometimes required by specific DRM vendors, therefore have to be set (\nkid\n,\npssh\n, ...).\nGeneral configuration values:\nkey\n: (required) You need to provide a key that will be used to encrypt the content (16 byte encryption key, represented as 32 hexadecimal characters)\nkid\n: (optional) also known as Key ID, or ContentID. Its a unique identifer for your content (16 byte initialization vector, represented as 32 hexadecimal characters)\nHINT:\nSome DRM providers provide you with a dedicated service to create and safely store Encryption Keys, so you don't have to create and manage them by yourself. These values are required to generate a proper playback license using DRM solution providers like Irdeto, EZDRM, ExpressPlay, Axinom, etc. to control playback permissions on the client side.\nLearn more\n.\nPlayReady DRM\nspecific configuration options: You can either provide an\npssh\nstring or an\nlaUrl\nin this configuration.\npssh\n: (optional)Base64 encoded String, PSSH payload Example:\nQWRvYmVhc2Rmc2FkZmFzZg==\nlaUrl\n: (optional) The License Aquistion URL that shall be used by the player.\nJava SDK Example - createDrmConfig() Method\n(\nLine in Example\n)\nJava\nprivate static CencDrm createDrmConfig(\n    Encoding encoding, Muxing muxing, Output output, String outputPath) throws BitmovinException {\n  CencDrm cencDrm = new CencDrm();\n  cencDrm.addOutputsItem(buildEncodingOutput(output, outputPath));\n  cencDrm.setKey(\"cab5b529ae28d5cc5e3e7bc3fd4a544d\");\n  cencDrm.setKid(\"08eecef4b026deec395234d94218273d\");\n  \n  ...\n  \n  CencPlayReady playReadyDrm = new CencPlayReady();\n  playReadyDrm.setLaUrl(\"http://playready.directtaps.net/pr/svc/rightsmanager.asmx\");\n  //playReadyDrm.setPssh(\"QWRvYmVhc2Rmc2FkZmFzZg==\");\n  cencDrm.setPlayReady(playReadyDrm);\n  \n  ...\n  \n  return bitmovinApi.encoding.encodings.muxings.fmp4.drm.cenc.create(\n      encoding.getId(), muxing.getId(), cencDrm);\n}\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/supported-input-and-output-formats",
    "title": "Supported Input and Output Formats",
    "text": "Support Matrixes\nBitmovin encodes MPEG-DASH, Apple HLS, Smooth Streaming, progressive TS, and progressive MP4 streams. With these formats you can reach all major devices. The streams can be played back with all major multimedia players and with the Bitmovin Player that provides you maximum device reach.\nAs seen below, Bitmovin supports a multitude of formats. To navigate the complexity of supported formats, and their combinations, 3 different support matrixes are provided and updated regularly:\nProduct Support Matrix - ENCODING - VOD Muxings\nProduct Support Matrix - ENCODING - Live Muxings\nProduct Support Matrix - ENCODING - Subtitles & Captions\nDisclaimer: Due to the complexity of combinations one can use with the Bitmovin Encoding product, those matrixes are for\nguidance only\nand can't be seen as promise or fact. Nor can they serve as basis for any claims. In doubt, please contact Bitmovin support via the support form in your dashboard.\nInput Formats\nVideo Codecs\nFeature\nVOD\nLive\nMPEG-1 Video\n✅\nMPEG-2 Video\n✅\n✅\nMPEG-4 Video\n✅\n✅\nH.261\n✅\nH.262\n✅\nH.263\n✅\nH.264/AVC\n✅\n✅\nH.265/HEVC\n✅\n✅\nVP6\n✅\nVP8\n✅\nVP9\n✅\n✅\nDNxHD\n✅\nDNxHR\n✅\nTheora\n✅\nXDCAM\n✅\nXDCAM HD422\n✅\nXDCAM IMX\n✅\nDV\n✅\nDVCPRO\n✅\nDVCPRO HD\n✅\nProRes\n✅\nAVCHD\n✅\nAVCIntra\n✅\nCineform HD\n✅\nIntel Indeo\n✅\nJPEG2000 (J2K)\n✅\nWMV\n✅\nDolby Vision\n✅\nAudio Codecs\nFeature\nVOD\nLive\nAAC\n✅\n✅\nMP3\n✅\nAC3 / Dolby Digital\n✅\nE-AC3 / Dolby Digital Plus\n✅\nFLAC\n✅\nVorbis\n✅\n✅\nPCM\n✅\nDolby Atmos\n✅\nIPCM\n✅\nDolby E*\n✅\nALAC\n✅\n* =\ncertain restrictions apply\nFile Formats\nFeature\nVOD\nLive\nMP4\n✅\n✅\nMKV\n✅\nMOV\n✅\nAVI\n✅\nFLV\n✅\nMPEG2 TS\n✅\nMPEG2 PS\n✅\nMXF\n✅\nLXF\n✅\nGXF\n✅\n3GP\n✅\nWebM\n✅\nMPG\n✅\nQuickTime\n✅\nWAV\n✅\nStreaming Formats\nFeature\nVOD\nLive\nApple HLS\n✅\nRTMP Push\n✅\nSRT (listener and caller modes)\n✅\nZixi\n✅\n🚧\nLive Encoding requires FIXED frame rates on input streams.\nThe Live Encoder can not support input signals with variable frame rate. Make sure that the configuration on contribution devices or software is set to output a fixed frame rate.\nOutput Formats\nDRM\nFeature\nVOD\nLive\nGoogle Widevine\n✅\n✅\nMicrosoft PlayReady\n✅\n✅\nApple Fairplay\n✅\n✅\nMarlin\n✅\n✅\nOutput Streaming Formats\nFeature\nVOD\nLive\nMPEG-DASH\n✅\n✅\nApple HLS\n✅\n✅\nSmooth Streaming\n✅\nDASH-IF Live Media Ingest Protocol\n✅\nContainer Formats\nFeature\nVOD\nLive\nProgressive MP4\n✅\n✅\nFragmented MP4\n✅\n✅\nMOV\n✅\n✅\nMPEG2-TS\n✅\n✅\nWebM\n✅\n✅\nCMAF\n✅\n✅\nWAV\n✅\nVideo Codecs\nFeature\nVOD\nLive\nH.262/MPEG-2 Video (XDCAM HD 422)\n✅\nH.264/AVC\n✅\n✅\nH.265/HEVC (SDR and HDR)\n✅\n✅\nVP8\n✅\nVP9\n✅\n✅\nAV1\n✅\nMJPEG\n✅\nDolby Vision\n✅\nAudio Codecs\nFeature\nVOD\nLive\nAAC\n✅\n✅\nHE-AAC v1\n✅\n✅\nHE-AAC v2\n✅\n✅\nDolby Digital\n✅\nDolby Digital Plus\n✅\nDolby Atmos\n✅\nMP2\n✅\nMP3\n✅\nVorbis\n✅\n✅\nOpus\n✅\n✅\nDTS:HD and DTS:X\n✅\nPCM*\n✅\n* =\ncertain restrictions apply",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/aws-live-outputs",
    "title": "AWS Live Outputs",
    "text": "Bitmovin supports three options for creating AWS S3 outputs, one for ease and other modes that support different organisations InfoSec policies.\nIf using Role Based or Access Keys, we recommend first reading this\ninformation on the S3 permissions\nthat need to be set.\nAWS Simple S3 Live Outputs\nAWS S3 Access Key Live Outputs\nAWS S3 Role Based Live Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/understanding-the-different-encoding-states",
    "title": "Understanding the Different Encoding States",
    "text": "An encoding can have multiple states:\nCREATED\n,\nQUEUED\n,\nRUNNING\n,\nFINISHED\n, or\nERROR\n.\nCREATED\nThe first step of every encoding configuration is to create an encoding resource with the Bitmovin API, where you can define the Cloud and a region, if you want, where this encodign shall be executed in. If this initial API call succeeds, a new encoding resource gets created and has the state\nCREATED\n.\nWhile an encoding is in this state you add or remove other resources like\nStream\ns and\nMuxing\ns accordingly, until you achieve the desired configuration for your encoding. Once you are done with that, you can send the API request to start this encoding (POST\nhttps://api.bitmovin.com/v1/encodings/encoding/<encodingId>/start\n). If the configuration passes the initial validation, the encoding will be scheduled for processing, and its state changes to ...\nQUEUED\nIn this state our system is checking the particular cloud provider and region the encoding is supposed to run in, for available encoder resources.\nIf no encoders resources are available at the moment, new ones have to be requested first. This time can vary, between Cloud Providers like AWS, Google or Azure, as well as their region that is used for an encoding. Also the current utilization of a particular region can impact the time an encoding remains in this state. However, we continuosely are optimizing our scheduling algorithms, so new resources are usually available within 1-2 minutes.\nIf there are encoder resources available already, those get assigned to the queued encoding and will be configured accordingly. This only takes a few seconds and eventually the state of the encoding will switch from QUEUED to RUNNING.\nRUNNING\nIn this state several steps are happening.\nInput Download\n- Inititally the encoding starts with downloading the input file for the encoding\nInput Analysis\n- Once completed, the input file get analyzed to determine its resolution, available audio/video tracks and other metadata information.\nEncoding\n- Now the encoding process starts generating the configured output formats, thumbnails, sprites and everything else.\nTransmuxing\n- If progressive outputs are part of the encoding configuration, they will be finished during this step, by merging the audio/video tracks that were encoded before.\nTransfer\n- The transfer of the encoded content to the provided output location is an implicit part of the whole encoding. Once completed successfully, the encoding changes its state to\nFINISHED\nif the encoding and tranfer step were successful, or to\nERROR\nin case one of those failed unfortunately.\nFINISHED\nAll steps of the encooding process were finished successfully, and your content is available on your storage ready to be streamed to your users :)\nERROR\nIn case of an issue during the whole encoding process, the task log panel in the details view in the dashboard, provides more details about the reason for the error and should already be helpful to understand what happened. If you still need assistance please don't hesitate and reach out to us through our\nsupport request forms\nhere.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/stitching-and-trimming-getting-started",
    "title": "Stitching and Trimming - Getting started",
    "text": "Overview\nSometimes, you need to be able to manipulate multiple files or parts of files as input for an encoding job. For simple use cases, you might need to add a bumper highlighting your brand at the start of your assets. In more complex post-production scenarios, you may want to compose a final cut of your asset on the basis of a \"recipe\" of instructions (for example an EDL (edit decision list) or a CPL (composition playlist).\nSince version 2.6.0, the Bitmovin encoding solution supports many such use cases, through a set of new APIs. There are a few principles to understand, which we will go through first, before going through a practical example.\nPrinciples\nIf you have used the Bitmovin encoding solution before, you should be familiar with the simple mechanism that let the encoder know where to find the source file: all you had to do was to add configuration when creating a\nStream\nobject, pointing to an\nInput\nobject's identifier representing the input storage (eg. an S3 bucket) as well as a path and input stream selection criteria. You can see this in many of the examples bundled with our SDKs.\nWith now multiple input files to consider and more flexibility required, there is a need to introduce a new concept and a new dedicated object: the\nInput Stream\n.\nThere are currently 3 types of Input Streams:\nThe\nIngest Input Stream\n: it is used to refer to an Input object and a path, in a way comparable to before.\nThe\nTrimming Input Stream\n: this object defines how an input stream is to be trimmed. There are currently 3 ways to do so: based\non time\n, on a\ntimecode track\n, or based on the\nH264 picture timing\nThe\nConcatenation Input Stream\n: used (unsurprisingly) to concatenate multiple input streams.\nSo, how do we put it all together in our encoding configuration? The rest of this tutorial will explain it in detail, but let's first create ourselves a realistic use case.\nAn example scenario\nLet's assume that we are a content producer, and that for every asset that we distribute to content partners, we want to add a short bumper at the start of the file, which highlights our brand. In addition, somewhere in the middle of the asset and at the end of it, we want to also add some cross-promotional content. Often, our main content also needs to be topped and tailed to remove unnecessary editorial markers.\nLet's represent this graphically:\nIngesting streams\nWith this goal clear in our mind, all we need to do now if to configure our encoding. A few lines create the essential objects we need.\nJava\nEncoding encoding = new Encoding();\nencoding.setName(\"Concatenated and trimmed encoding with bumper and promo\");\nencoding.setCloudRegion(CloudRegion.AUTO);\nencoding.setEncoderVersion(\"STABLE\");\nencoding = encodingApi.encodings.create(encoding);\nIn this example, let's just assume that all files are available through HTTPS URLs from the same server. We therefore first create an\nHttpsInput\n:\nJava\nHttpsInput input = new HttpsInput();\ninput.setHost(HTTPS_INPUT_HOST);\ninput = encodingApi.inputs.https.create(input);\nSince there are three files that we need to handle, we first create ingest input streams for them. It doesn't matter that the promo and main asset appear twice in the making of the final asset, we only need to register them once. In each case, we just let the encoder automatically select the audio and video streams from the input file.\nJava\nIngestInputStream main = new IngestInputStream();\nmain.setInputId(input.getId());\nmain.setInputPath(HTTPS_MAIN_INPUT_PATH);\nmain.setSelectionMode(StreamSelectionMode.AUTO);\nmain = encodingApi.encodings.inputStreams.ingest.create(encoding.getId(), main);\n\nIngestInputStream bumper = new IngestInputStream();\nbumper.setInputId(input.getId());\nbumper.setInputPath(HTTPS_BUMPER_INPUT_PATH);\nbumper.setSelectionMode(StreamSelectionMode.AUTO);\nbumper = encodingApi.encodings.inputStreams.ingest.create(encoding.getId(), bumper);\n\nIngestInputStream promo = new IngestInputStream();\npromo.setInputId(input.getId());\npromo.setInputPath(HTTPS_PROMO_INPUT_PATH);\npromo.setSelectionMode(StreamSelectionMode.AUTO);\npromo = encodingApi.encodings.inputStreams.ingest.create(encoding.getId(), promo);\nTrimming the main stream\nNext, we need to let the encoder know how the main asset is to be trimmed. This time, we need two separate objects to represent the 2 parts of the asset that will make it to the final cut. To do this, we create time-based trimming input streams, and link them to the previously created ingest input stream.\nOur main asset has 10 seconds at the start that we don't need. Part 1 is 90 seconds, and part 2 is 60 seconds, starting directly after part 1. Translated into the appropriate\noffset\nand\nduration\nparameters, this means:\nJava\nTimeBasedTrimmingInputStream mainPart1 = new TimeBasedTrimmingInputStream();\nmainPart1.setInputStreamId(main.getId());\nmainPart1.setOffset(10D);\nmainPart1.setDuration(90D);\nmainPart1 = encodingApi.encodings.inputStreams.trimming.timeBased.create(encoding.getId(), mainPart1);\n\nTimeBasedTrimmingInputStream mainPart2 = new TimeBasedTrimmingInputStream();\nmainPart2.setInputStreamId(main.getId());\nmainPart2.setOffset(100D);\nmainPart2.setDuration(60D);\nmainPart2 = encodingApi.encodings.inputStreams.trimming.timeBased.create(encoding.getId(), mainPart2);\nConcatenating the streams\nWe are now ready to put it all together into one concatenation input stream object:\nJava\nConcatenationInputStream allTogether = new ConcatenationInputStream();\n\nConcatenationInputConfiguration bumperConfig = new ConcatenationInputConfiguration();\nbumperConfig.setInputStreamId(bumper.getId());\nbumperConfig.setPosition(0);\nbumperConfig.setIsMain(false);\nallTogether.addConcatenationItem(bumperConfig);\n\nConcatenationInputConfiguration part1Config = new ConcatenationInputConfiguration();\npart1Config.setInputStreamId(mainPart1.getId());\npart1Config.setPosition(1);\npart1Config.setIsMain(true);\nallTogether.addConcatenationItem(part1Config);\n\nConcatenationInputConfiguration promo1Config = new ConcatenationInputConfiguration();\npromo1Config.setInputStreamId(promo.getId());\npromo1Config.setPosition(2);\npromo1Config.setIsMain(false);\nallTogether.addConcatenationItem(promo1Config);\n\nConcatenationInputConfiguration part2Config = new ConcatenationInputConfiguration();\npart2Config.setInputStreamId(mainPart2.getId());\npart2Config.setPosition(3);\npart2Config.setIsMain(false);\nallTogether.addConcatenationItem(part2Config);\n\nConcatenationInputConfiguration promo2Config = new ConcatenationInputConfiguration();\npromo2Config.setInputStreamId(promo.getId());\npromo2Config.setPosition(4);\npromo2Config.setIsMain(false);\nallTogether.addConcatenationItem(promo2Config);\n        \nallTogether = encodingApi.encodings.inputStreams.concatenation.create(encoding.getId(), allTogether);\nThe\nposition\nproperty for each of the items defines the order in which they will appear in the final output. Note how the item for Part 1 is defined as being the\nmain\nitem. Only one of the input streams can be set in this way, and it is used as reference for scaling, aspect ratio, frame rate, sample rate, etc.\nA summary at this stage would be useful, for which a simple diagram should hopefully be sufficient\nFinalising the encoding\nIt's now all in place to finalise our encoding.\nFrom this point on, the rest of the encoding configuration works in the usual way: define your codec configurations, streams, muxings and start your encoding.\nThere is just one last thing of interest to highlight in those steps. When you define your input streams, instead of providing an input ID and file path as you'd have done before, you now just need to provide the identifier of the input stream:\nJava\nStreamInput streamInput = new StreamInput();\nstreamInput.setInputStreamId(allTogether.getId());\n\nStream audioStream = new Stream();\naudioStream.addInputStreamsItem(streamInput);\naudioStream.setCodecConfigId(aacConfiguration.getId());\naudioStream = encodingApi.encodings.streams.create(encoding.getId(), audioStream);\n\nStream videoStream = new Stream();\nvideoStream.addInputStreamsItem(inputStream);\nvideoStream.setCodecConfigId(videoConfiguration1080p.getId());\nvideoStream = encodingApi.encodings.streams.create(encoding.getId(), videoStream);\nAdded flexibility\nFor best results, it is recommended to try and ensure that all the files being stitched together have similar specs, primarily when it comes to frame rate, resolution and aspect ratio for video streams, and sample rate and audio layout for audio streams.\nHowever, since v2.36, the Bitmovin encoder will also allow you to concatenate files with different specs. In that case, the input stream flagged as being the main one in the\nConcatenationInputConfiguration\nwill be considered as the model to which all other input streams will be conformed, before any encoding configuration is applied.\nIn case aspect ratios are different, you can use the\naspectMode\nparameter of the\nConcatenationInputConfiguration\nto dictate how the input streams should be transformed.\nAdditionally, with the\npaddingBefore\nand\npaddingAfter\nparameters, you can also introduce black padding between the stitched sections.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/939d532-image.png",
      "https://files.readme.io/29c169c-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/how-to-implement-a-live-to-vod-workflow-with-the-bitmovin-api",
    "title": "How to implement a Live-to-VoD workflow with the Bitmovin API",
    "text": "VoD manifest generation\nWhen talking about Live-to-VoD generation, what Bitmovin does specifically is the generation of a VoD manifest or playlist which contains references to the segments already generated - or to be generated, for the live stream. In this way, when we trigger the Live-to-VoD workflow, we are basically creating a new VoD manifest while keeping the original live encoding workflow untouched. Since Bitmovin is not re-encoding neither re-packaging the live content but just creating a new manifest pointing to the proper live segments, the Live-to-VoD process is really fast, making it possible to have a new VoD manifest available in matter of seconds. Additionally, Bitmovin Live-to-VoD workflow makes it possible to define what portion\n(2)\nof the main live event needs to be included in the VoD manifest.\nAs any other manifest creation process supported by Bitmovin, we support a fully manifest customization - so new manifests can even differ from the original one, for instance, some renditions may be omitted, new periods could be added or new manifest type (HLS or DASH) could be created, etc.\n(2) The Live-to-Vod creation process provides a segment base granularity. In this way, the VOD manifest start and end point match the start or end of the live segments.\nCommon use cases\nRecording full live events - A full live event could be recorded as a VoD content. Additionally, customers are able to trim the content in order to remove unnecessary segments for the very beginning or the end of the transmission.\nHighlighting content - Several highlights can be created from a live stream while the live event is still active, for instance to share clips or generate highlights from Sports events, News, Concerts, etc. Each highlight or clipping would be consider as a new VoD content, so in practice, the customers can create as many highlights as they need.\nCreating VoD chapters - Some live events may be splitted into smaller pieces such as chapters to better organize how the content will be stored and consumed once the event finishes. Each chapter could be considered a new VoD content.\nAPI and workflow details\nIn the following section we will walk trough the required steps to set a Live-to-Vod workflow. This should be setup in addition to the\nstandard live streaming configuration\n.\nFrom the Bitmovin APIs point of view, Live-to-VoD is not more than just a manifest creation process but carefully referencing to encodings, muxes and streams that should have been created during the standard live encoding workflow - see the diagram below as a reference.\nRequired fields from the live encoding object - The live encoding objects\n<LIVE-ENCODING-ID>\n,\n<LIVE-MUXING-ID>\n,\n<LIVE-STREAM-ID>\nare required when creating a new VoD manifest.\nOptional fields -\nstartSegment, endSegment - Start and end segments that define the segment range included in the Vod manifest. If not provided, all the live segments are included.\n<LIVE-OUTPUT-ID>\nand\n<LIVE-OUTPUT-PATH>\nobjects may be required. This ensures that the segments into the VoD manifest reference to exactly the same live segments storage location.\nSample code - DASH Manifest\nThe following sample code shows how to generate a VoD Dash manifest from a specific range of live encoding segments using our PHP SDK. The variables called\n$live_* refers\nto values obtained from the live workflow\nPHP\n// Create a new VoD Dash Manifest\n$dashManifest = new DashManifest();\n$dashManifest->name(\"stream_vod.mpd\");\n$dashManifest->manifestName(\"stream_vod.mpd\");\n$dashManifest->outputs([buildEncodingOutput($live_Output, $live_OutputPath)]);\n$dashManifest = $bitmovinApi->encoding->manifests->dash->create($dashManifest);\n\n// Add a new Period into the Dash Manifest\n$period = new Period();\n$period = $bitmovinApi->encoding->manifests->dash->periods->create($dashManifest->id, $period);\n\n// Add a new video adaptation set into the Period\n$videoAdaptationSet = new VideoAdaptationSet();\n$videoAdaptationSet = $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->video->create($dashManifest->id, $period->id, $videoAdaptationSet);\n\n// Add a new video representation into the Adaptation Set\n$videoRepresentation = new DashFmp4Representation();\n$videoRepresentation->type(DashRepresentationType::TEMPLATE());\n$videoRepresentation->encodingId($live_EncodingId);\n$videoRepresentation->muxingId($live_1080p_Fmp4MuxingId);\n$videoRepresentation->segmentPath($live_1080p_SegmentPath);\n$videoRepresentation->startSegmentNumber($startSegment);\n$videoRepresentation->endSegmentNumber($endSegment);\n$bitmovinApi->encoding->manifests->dash->periods->adaptationsets->representations->fmp4->create($dashManifest->id, $period->id, $videoAdaptationSet->id, $videoRepresentation);\nFirst, a new VoD $dashManifest is created. For this specific sample code, the manifest will be written at the same live output location, i.,e,\n$live_OutputPath\n.\nA new\n$period\nand a new\n$videoAdaptationSet\nis created into the VoD\n$dashManifest\n. This is the standard process for defining the manifest with Bitmovin APIs.\nAlso, when defining the\n$videoRepresentation\nto be added, the VoD\n$videoRepresentation\nmust point to the existing live encoding objects, i.e.,\n$live_EncodingId\n,\n$live_1080p_Fmp4MuxingId\n. Additionally, for this sample code, we considered that either live and VoD segments are located in the same location, i.e,\n$live_1080p_SegmentPath\n.\nThe VoD manifest contains the segments between\n$startSegment\nand\n$endSegment\n.\nThe Same approach applies to creating and trim the audio representations into the same Dash Manifest, for instance:\nPHP\n// Add a new audio adaptation set into the Period\n$audioAdaptationSet = new AudioAdaptationSet();\n$audioAdaptationSet = $bitmovinApi->encoding->manifests->dash->periods->adaptationsets->audio->create($dashManifest->id, $period->id, $audioAdaptationSet);\n\n// Add a new audio adaptation set into the Period\n$audioRepresentation = new DashFmp4Representation();\n$audioRepresentation->type(DashRepresentationType::TEMPLATE());\n$audioRepresentation->encodingId($live_EncodingId);\n$audioRepresentation->muxingId($live_audio_Fmp4MuxingId);\n$audioRepresentation->segmentPath($live_audio_SegmentPath);\n$audioRepresentation->startSegmentNumber($startSegment);\n$audioRepresentation->endSegmentNumber($endSegment);\n$bitmovinApi->encoding->manifests->dash->periods->adaptationsets->representations->fmp4->create($dashManifest->id, $period->id, $audioAdaptationSet->id, $audioRepresentation);\nFinally, the dash manifest is created\nPHP\n$bitmovinApi->encoding->manifests->dash->start($dashManifest->id);\nSample code - HLS Manifest\nThe following sample code shows how to create an HLS manifest from a live encoding.\nPHP\n// Create a new HLS Manifest\n$hlsManifest = new HlsManifest();\n$hlsManifest->name('Live to Vod Manifest');\n$hlsManifest->manifestName(\"master_vod.m3u8\");\n$hlsManifest->outputs([buildEncodingOutput($live_output, $live_outputPath)]);\n$hlsManifest = $bitmovinApi->encoding->manifests->hls->create($hlsManifest);\n\n// Create a new streamInfo into the Manifest\n$streamInfo = new StreamInfo();\n$streamInfo->uri('video_vod.m3u8');\n$streamInfo->encodingId($live_EncodingId);\n$streamInfo->streamId($live_1080p_StreamId);\n$streamInfo->muxingId($live_1080p_Fmp4MuxingId);\n$streamInfo->startSegmentNumber($startSegment);\n$streamInfo->endSegmentNumber($endSegment);\n$streamInfo->audio('audio');\n$streamInfo->segmentPath($live_1080p_SegmentPath);\n$bitmovinApi->encoding->manifests->hls->streams->create($hlsManifest->id, $streamInfo);\n\n// Create a new audioMediaInfo into the Manifest\n$audioMediaInfo = new AudioMediaInfo();\n$audioMediaInfo->groupId('audio');\n$audioMediaInfo->name('en');\n$audioMediaInfo->uri('audio_vod.m3u8');\n$audioMediaInfo->encodingId($live_EncodingId);\n$audioMediaInfo->streamId($live_audio_StreamId);\n$audioMediaInfo->muxingId($live_audio_Fmp4MuxingId);\n$audioMediaInfo->segmentPath($live_audio_SegmentPath);\n$audioMediaInfo->startSegmentNumber($startSegment);\n$audioMediaInfo->endSegmentNumber($endSegment);\n$audioMediaInfo->language('en');\n$bitmovinApi->encoding->manifests->hls->media->audio->create($hlsManifest->id, $audioMediaInfo);\n$bitmovinApi->encoding->manifests->hls->start($hlsManifest->id);\nThe process is pretty similar to the DASH VoD manifest creation, for instace:\nFirst, a new VoD\n$hlsManifest\nis created. The manifest will be written at the same live output location, i.,e,\n$live_Output $live_OutputPath\n.\nA new\n$streamInfo\nis created into the VoD\n$hlsManifest\n.\nWhen defining the\n$streamInfo\n, it point to the existing live encoding objects, i.e.,\n$live_EncodingId\n,\n$live_1080p_StreamId\n,\n$live_1080p_Fmp4MuxingId\n.\nVoD segments are located in the same live location, i.e,\n$live_1080p_SegmentPath\n.\nThe VoD manifest is trimmed to\n$startSegment\nand\n$endSegment\nnumbers.\nSame workflow applies for\n$audioMediaInfo\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/1cf824b-image.png",
      "https://files.readme.io/7cb8f26-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/how-live-standby-pools-work",
    "title": "How Live Standby Pools work",
    "text": "How Live Standby Pools work\nReady State\nLive Standby Pools can be thought of as a number of resource groups that maintain a set number of live encodings that are all pre-configured based on an\nEncoding Template\nand set to a running state. In the image below, two Live Standby Pools have been configured, each using a different Template and therefore configuration. In this example both are set to maintain three running live encoders at any time and both are in a full and ready state.\nThe organization will have\nsix running\nlive encoders, these will be billable and use live encoding slots in the organization.\nAcquisition\nWhen a Live Encoding is required it is\nacquired\nfrom the Live Standby Pool and can then be used to ingest and provide an output to the live origin. The Live Standby Pool will detect it is not at the correct size and start a new Live Encoding using the Encoding Template. This is shown in the image below where we focus on Live Standby Pool 1, the Live Encoder 1.1 that is acquired will instantly be able to receive an input, meanwhile Live Standby Pool 1 will detect it no longer has 3 running encodings and re-fill. The preparation time for Live Encoder 1.4 will depend on the queuing time in the cloud provider and region at that particular moment in time.\nIncluding Live Standby Pool 2, the organization will now have\nsix running\nlive encoders and\none queued\n,\nWhen Live Encoder 1.4 has finished preparing in the pool it will enter the running state.\nThe organisation will now have\nseven running\nlive encoders, and is running these will be billable and use live encoding slots in the organization. It's for this reason that customers should consider the account limits as part of their requirements.\nAccount Limits\nIn your organization settings you can find out if the Live Standby Pool has been enabled by checking Organization Settings -> Limits\nYou can find this by clicking on the Organization name in the top right corner of the Dashboard\nOnce open, check if\nmaxStandbyPools\nis present in the list and that the number represents the total number of Live Standby Pools (and therefore different configurations) you will require.\nCheck the\nmaxLiveEncodingsPerStandbyPool\nallows you enough capacity for peak demand on each Live Standby Pool.\nCheck the\nMaximum LIVE encodings started\nnumber is able to include enough capacity for the total amount of running Live Encoders you require to be Acquired and In Use and in the Live Standby Pools.\nIf these Limits are present then you will be able to configure Live Standby Pools, and the option will be visible in Live Encoding -> Configurations -> Live Pools",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/08622e2aab856cc4bd826ea52b064eb76308d7f0688867245a92b405acb43b42-Two_Live_Standby_Pools_Ready.png",
      "https://files.readme.io/e33337fcd3a924280d8df2d6d68345c2acc7f0943b8eb0ac3d09f60c3aa7e892-Live_Standby_Pools_Acquiring.png",
      "https://files.readme.io/0629b3cb641a2509c56cac619a18ab89d23311b6dd17f93573efd21e6e513009-Screenshot_2024-11-29_at_15.19.15.png",
      "https://files.readme.io/9d456254c35a02a2a94767d6ed020f5e6ca4b7a69d27943369ca58b70428f45d-Live_Standby_Pools_All_running.png",
      "https://files.readme.io/4ea07655e2fc2847645aceb1700c45e58836b1bc4a746d05589187260f00d6cd-Screenshot_2024-11-22_at_18.09.20.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-an-s3-encoding-input-or-output-with-the-bitmovin-api",
    "title": "Creating an S3 Encoding Input or Output with the Bitmovin API",
    "text": "Overview\nYour encoding workflow needs to have access to Input and Output storage to retrieve source files and write encoded files. When using AWS (Amazon Web Services), you will need to use S3 buckets as storage.\nIn the simplest configuration, you will provide the Bitmovin Encoding Service with an Access and Secret key pair, attached to an AWS IAM (Identity and Access Management) User with appropriate permissions to access those buckets.\nThis tutorial will walk you through the steps to create an S3 bucket, configure it to your needs, and then create Input and Output resources in the Bitmovin Encoding service.\nNote: There is an alternative mechanism to grant access to your S3 buckets, with role-based access. If this is what you need, consult the\nS3 Role-Based Input and Output tutorial\ninstead.\nCreate an AWS S3 Bucket\nIn the AWS Management Console, open the\nS3 section\n.\nClick on the\nCreate Bucket\nbutton which starts the bucket creation wizard\nIn the \"Name and Region\" panel, choose a bucket name (for example\nmy-bitmovin-bucket\n) and a Region (for example\n(EU) Ireland)\n), then press\nNext\nConfigure the\nObject Ownership\ndepending on your needs a.\nACLs enabled\n, this is\nrequired\nif PUBLIC_ACCESS is configured for Encoding outputs. Encodings that are started in our\nBitmovin Dashboard\nset the PUBLIC_ACCESS to enable preview playback.\nb. ACLs disabled. If this policy is used Encoding outputs need to be set to have the PRIVATE ACL.\nConfigure\nBlock Public Access settings for this bucket\na. The default settings will\nBlock\nall\npublic access\nb. To\nenable playback\nfor manifests and files from the bucket,\nuncheck\n🔳\nBlock\nall\npublic access.\nFinish going through the wizard and click\nCreate Bucket\nTo allow players to request content for streaming from your S3 bucket, you will also need to allow origin access with a CORS configuration. See\nHow can I configure an AWS S3 Bucket to test playback of my content?\non how to configure this for your bucket.\nYour bucket is now ready to be used.\nCreate an AWS IAM User\nNext we need to create a user that will be accessing the bucket. We continue working with the AWS Console\nOpen the\nIdentity and Access Management (IAM) console\nOn the left pane, click on \"Access Management\" -> \"Users\".\nClick on\nAdd User\n. The\nAdd User\npage appears.\nStep 1: add your desired User name.\nStep 2: Permissions\na. Make sure the user has the right permissions to for Amazon S3\nb. The easiest way to achieve this is AmazonS3FullAccess\nc. In “\nPermission options”\nselect “\nAttach policies directly”\nd. In\n“Permission policies”\nfilter and select AmazonS3FullAccess\n(Note: The pre-defined AmazonS3FullAccess policy is known to be suitable but since it provides unrestricted access to your bucket, you might need to create a custom policy with fine-tuned access rights. Please review details of the permissions required for\nbuckets for Encoding Input and Output\nbuckets and create a specific IAM Policy and associate it with this user as needed\n)\nGo through the next screens - those settings are optional and do not affect the configuration.\nOn the last screen, press\nCreate User\nSelect the newly created user in the\nUsers\noverview\nSelect the\n“Security credentials”\ntab\nScroll down to\n“Access keys”\nand hit the\n“Create access key”\nbutton\nFollow the\n“Create access key”\nsteps\na. For\n“Access key best practices & alternatives”\nselect\n“Application running outside AWS”\nb. You need to securely store\nAccess Key\nand\nSecret Access Key.\nYou need to store them somewhere securely as you will need them later. Note that once you've left this screen, you will not be able to retrieve the Secret Access Key anymore and will need to generate a new one.\nIf you want to learn more about Users in AWS, please see their\ndocumentation\n.\nCreate an S3 Input/Output\nS3 input and output resources can be created via the Bitmovin API or in the Bitmovin Dashboard. The minimal required information to create an S3 input or output are the following :\nbucketName\n: the name of your S3 bucket\naccessKey\nand\nsecretKey\n: the Access Key ID and Secret Access Key obtained earlier.\nThis example uses our latest\nOpen API client for Java\n, which is available on Github.\n(Java) S3 Output Example\nCreate a new S3 Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\n\nAclEntry aclEntry = new AclEntry();\naclEntry.setPermission(AclPermission.PRIVATE);\n\nList<AclEntry> acl = new ArrayList<>();\nacl.add(aclEntry);\n\nS3Output s3Output = new S3Output();\ns3Output.setBucketName(\"<BUCKET_NAME>\");\ns3Output.setAccessKey(\"<AWS_ACCESS_KEY>\");\ns3Output.setSecretKey(\"<AWS_SECRET_KEY>\");\ns3Output.setAcl(acl);\n\ns3RoleBasedOutput = bitmovinApi.encoding.outputs.s3.create(s3Output);\nHint: In case you chose to enable\nBlock public access\non your S3 bucket (recommended), you would have to make sure that the ACL is set to\nPRIVATE\non the output (as shown above) as well as on your Muxing configurations.\nTo create an Input is fairly similar, but you just use the\nS3Input\nresource and the\nbitmovinApi.encoding.inputs.s3\nendpoint\nUse an existing S3 Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\n\nS3Output s3Output = bitmovinApi.encoding.outputs.s3.get(\"YOUR_S3_OUTPUT_ID\");\n(CURL) S3 Output Example\nCreate a new S3 Output\nAPI reference:\ncreate an S3 Output\n:\nShell\ncurl -X POST \\\n  https://api.bitmovin.com/v1/encoding/outputs/s3 \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY' \\\n  -d '{\n    \"bucketName\": \"<BUCKET_NAME>\",\n    \"accessKey\": \"<AWS_ACCESS_KEY>\",\n    \"secretKey\": \"<AWS_SECRET_KEY>\",\n    \"acl\": [\n        {\n            \"permission\": \"PRIVATE\"\n        }\n    ]\n}'\nGet an existing S3 Output\nAPI reference:\nget an S3 Output\nShell\ncurl  https://api.bitmovin.com/v1/encoding/outputs/s3/YOUR_OUTPUT_ID \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY'\nWhat's Next?\nNow that you have created S3 Role-Based Inputs and/or Outputs, you can use them in your encoding in much the same way as you would any other Input or Output.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-bitmovin-cloud-connect-with-aws",
    "title": "Using Bitmovin Cloud Connect with AWS",
    "text": "This document explains how to set up Bitmovin Encoding on AWS infrastructure so that the Bitmovin platform can run encoders using the AWS EC2 API.\nThe instructions in this document for the\nREST API Services\napply to live encoding and file-based encoding. For a complete list of formats and input types, see the\nBitmovin website\n.\nPrerequisites\n🚧\nActivation required\nThis feature requires a commercial agreement and needs to be specifically activated for a Bitmovin account, it is not available by default. You will not be able to complete the configuration below without this activation.\nA Bitmovin account enabled for usage of Cloud Connect\nIf you want to use Cloud Connect with a sub organization, this sub organization must be enabled for usage of Cloud Connect\nAn\nAWS account\nConfigure your AWS account\nIn this section, you will create a separate user that will be used by the Bitmovin platform when interacting with your AWS infrastructure. Also, we will create the appropriate infrastructure setup to enable Encoding jobs.\n📘\nOptional: Configure with Terraform\nIf you prefer infrastructure-as-code, Bitmovin AWS Cloud Connect is also configurable using Terraform scripts:\nBitmovin Terraform Cloud Connect\nCreate an Amazon IAM user\n🚧\nDo not use multi-factor authentication (MFA)\nPlease make sure not to use any IAM policies on this account that require the user to have a virtual multi-factor authentication (MFA) device set up.\nThis user is used by the Bitmovin platform to get access to your AWS account and create compute resources to run parts of the the encoding service within it, including starting and stopping EC2 instances.\nOpen the\nIdentity and Access Management (IAM)\nconsole:\nhttps://console.aws.amazon.com/iam/\nGo to \"Access management >\nUsers\n\", click the \"\nCreate user\n\" button\nIn \"Specify user details\" (Step 1), provide a \"\nUser name\n\" of your choice\nNote:\nSince this user is only used for programmatic access, you do not need to grant AWS Management Console access here.\nIn \"Set permissions\" (Step 2), ...\nselect \"Permissions options >\nAttach policies directly\n\"\nsearch for and select the managed \"Permissions policies >\nAmazonEC2FullAccess\n\" policy (recommended for testing/evaluation purposes only)\nIn \"Review and create\" (Step 3) check if everything is configured as shown before, then click the \"\nCreate user\n\" button\n📘\nTighten permissions for production\nSince the managed\nAmazonEC2FullAccess\npolicy might not be suitable for security-aware production configurations, we recommend to use a tailored user policy. See \"\nAppendix: Limited-access IAM user policy\n\".\n📘\nConsiderations about the used AWS Account\nBitmovin Cloud Connect requires the IAM User to be created in the same AWS Account that is also used for Encoding Jobs. A role-based login across accounts is at the moment not possible. Its not a requirement but a suggestion to properly separate workloads and easily identify and allocate Bitmovin Cloud Connect related cloud costs to create a dedicated AWS Account for Bitmovin Cloud Connect only.\nGet the Amazon IAM user credentials\nThis step will collect the access credentials to allow the Bitmovin platform to access your infrastructure.\nUnder \"IAM > Access management >\nUsers\n\", click on the\nname of your newly created user\nto open the details view\nAt the top, click \"Summary >\nCreate access key\n\"\nIn \"Access key best practices & alternatives\" (Step 1), select \"\nThird-party service\n\" and check \"I understand the above recommendation and want to proceed to create an access key.\", then click the \"\nNext\n\" button\nIn \"Set description tag\" (Step 2), provide a \"\nDescription tag value\n\" of your choice, then click the \"\nCreate access key\n\" button\nIn \"Retrieve access keys\" (Step 3), copy the \"\nAccess key\n\" and \"\nSecret access key\n\", or click the \"\nDownload .csv file\n\" button\nThen click the \"\nDone\n\" button\nConfigure your Amazon VPC resources (optional)\nWe recommend that you use the\ndefault Virtual Private Cloud (VPC)\nfor the respective region. The default VPC is the only VPC in which you can avoid specifying a subnet, which gives you the advantage that if one availability zone has reached its capacity, your instance will automatically be launched in another (free) zone. If you select a specific subnet on instance launch (which you are required to do if a non-default VPC is used), the instance can only be launched in the Availability Zone of the selected subnet, and if there are no more resources available in that zone, then no instance can be launched.\nRecreate a default VPC\nIf you have deleted the default VPC in your AWS account, you can follow the AWS \"\nCreate a default VPC\n\" docs.\nCreate a custom VPC configuration\nFollow the AWS \"\nCreate a VPC\n\" docs for your custom configuration.\nThe subnet must have\nAuto-assign public IPv4 address\nenabled\nCustom VPCs are supported from Encoder v2.51.0 and above\nCreate a Security Group\nA Security Group defines firewall rules that enable network access into your VPC and to the EC2 instances in particular.\nOpen the\nEC2 Dashboard\nconsole:\nhttps://console.aws.amazon.com/ec2/home\nMake sure to have your\ndesired region\nselected at the top right, e.g. \"Europe (Frankfurt) eu-central-1\"\nGo to \"Network & Security >\nSecurity Groups\n\", click the \"\nCreate security group\n\" button\nUnder \"Basic details\", provide a \"\nSecurity group name\n\" and a \"\nDescription\n\" of your choice\nClick the \"\nCreate security group\n\" button (the security group needs to be created to be available for selection in the next step)\nOn the security group detail view under the \"\nOutbound rules\n\" tab, make sure that the default rule still exists (otherwise, create it according to the table)\nKey\nValue\nType\nAll traffic\nProtocol\nAll\nPort range\nAll\nDestination\n0.0.0.0/0\nDescription\n-\nOn the security group detail view under the \"\nInbound rules\n\" tab, click the \"\nEdit inbound rules\n\" button\nUnder the \"\nInbound rules\n\" tab, create new rules for each of the following tables (3 tables will result in 5 rules) by clicking the \"\nAdd rule\n\" button:\nKey\nValue\nType\nAll traffic\nProtocol\nAll\nPort range\nAll\nSource\n(select the previously created security group)\nDescription\nFor communication between the session manager VM instance and its instance manager VM instances\nKey\nValue\nType\nCustom TCP\nProtocol\nTCP\nPort range\n9999\nSource\n104.199.97.13/32, 35.205.157.162/32\n(comma-separated Source will create two rules)\nDescription\nFor communication with the service that manages the encoding\nKey\nValue\nType\nCustom TCP\nProtocol\nTCP\nPort range\n9443\nSource\n104.199.97.13/32, 35.205.157.162/32\n(comma-separated Source will create two rules)\nDescription\nFor HTTPS communication with the service that manages the encoding\nKey\nValue\nType\nSSH\nProtocol\nTCP\nPort range\n22\nSource\n104.199.97.13/32, 35.205.157.162/32\n(comma-separated Source will create two rules)\nDescription\nFor incoming commands from the Bitmovin API to control the encoding\nNote: Firewall rules are checked to be limited to the correct source ip addresses, and therefore allowing all ip addresses in source would fail the configuration when adding region settings.\n📘\nLive encodings\nAdditional inbound rules are required if you are encoding live streams transported over RTMP, SRT or Zixi.\nAdditional inbound rules for RTMP live streams\nKey\nValue\nType\nCustom TCP\nProtocol\nTCP\nPort range\n1935\nSource\nAnywhere-IPv4/6\n(or the specific set of addresses where streams will originate from)\nDescription\nFor RTMP live streams\nKey\nValue\nType\nCustom TCP\nProtocol\nTCP\nPort range\n443\nSource\nAnywhere-IPv4/6\n(or the specific set of addresses where streams will originate from)\nDescription\nFor RTMPS live streams\nAdditional inbound rules for SRT live streams\nKey\nValue\nType\nCustom TCP and Custom UDP\n(multiple rules)\nProtocol & Port range\ntcp:2088\nudp:2088\nudp:2089\nudp:2090\nudp:2091\nPort range\n1935\nSource\nAnywhere-IPv4/6\n(or the specific set of addresses where streams will originate from)\nDescription\nFor SRT live streams\nAdditional inbound rules for Zixi live streams\nKey\nValue\nType\nCustom TCP\nProtocol\nTCP\nPort range\n4444\nSource\nAnywhere-IPv4/6\n(or the specific set of addresses where streams will originate from)\nDescription\nFor Zixi live streams\nConfigure your Bitmovin account\nBefore you continue, make sure you have collected the following information from AWS:\nFrom your AWS account\nAccount ID\nFrom your IAM user\nAccess key\nSecret access key\nFrom your security group\nSecurity group ID\n(if applicable) From your custom VPC\nSubnet ID\nLink your AWS account\nTo enable your Bitmovin account to run encodings in your AWS account, you need to link it with Infrastructure and Region Settings objects.\nOpen the\nBitmovin Dashboard\n:\nhttps://dashboard.bitmovin.com/\nGo to \"VOD/LIVE Encoding >\nCloud Connect\n\"\nClick the \"\nAdd infrastructure account\n\" button, select the \"\nAmazon Web Services\n\" option, click the \"\nNext\n\" button\nProvide a \"\nName\n\" of your choice, fill in the\nAccount number\n,\nAccess key\nand\nSecret (access) key\n, click the \"\nNext\n\" button\nSelect the appropriate \"\nCloud Region\n\" (where the security group was created), fill in the\nSecurity group ID\n(and if applicable, the Subnet ID), click the \"\nNext\n\" button\nGetting access to AMIs\nThe Bitmovin platform uses Amazon Machine Images (AMIs) from which to create encoder instances in AWS.\nAfter you have created an infrastructure object in your Bitmovin account, we will trigger automatic access allowlisting for our AMIs. This process is required to be able to run encodings and can take up to 20 minutes.\nRun encoding jobs in AWS\nAfter configuration has been completed, you will be able to run encoding jobs in your own AWS account. To do so, use the Bitmovin API client SDKs to submit encoding jobs, in the same way as you would do for encodings running in the Bitmovin Managed Cloud service. The only difference is that you need to specify the new infrastructure instead of public cloud regions.\nHere is a Python snippet demonstrating how to link your encoding to your infrastructure.\nPython\n# ID of the Infrastructure object\n    infra_id = ‘<infrastructure_id>’ \n    \n    # AWS region of the AWS-connect setup\n    infra_region = CloudRegion.AWS_EU_WEST_1\n    infrastructure = InfrastructureSettings(infrastructure_id=infra_id, \n                            cloud_region=infra_region)\n    \n    encoding = Encoding(name='aws connect encoding',\n        cloud_region=CloudRegion.EXTERNAL,\n        infrastructure=infrastructure,\n        encoder_version='STABLE')\nSub Organizations\nIf you have set up your infrastructure in a sub organization, you must tell the Bitmovin API that you want to run the encoding in that sub organization. Thus, in addition to the code snippet above, make sure to set the\ntenant_org_id\nalongside the\napi_key\nin the\nbitmovin_api\nobject:\nPython\n# ID of the sub organisation you added the infrastructure to\n    organisation_id = '<sub_organisation_id>'\n\n    bitmovin_api = BitmovinApi(api_key=config_provider.get_bitmovin_api_key(), \n                       tenant_org_id=organisation_id,\n                       logger=BitmovinApiLogger())\nResource Quotas\nIf you want to run several encodings in parallel, the default limits may not be sufficient. In that case, you will have to request limit increases for the following resource in your Region(s), through the\nService Quotas page\n:\nFor the limits to request we will be using these variables:\nVariable name\nExplanation\n(maximum number of encodings)\nThe maximum number of parallel encodings the infrastructure must be able to run.\nTypically this is the number of encoding slots assigned to the Bitmovin account or sub-org associated with the infrastructure.\n(maximum number of instances per encoding)\nThe number of instances used by one encoding.\nThis number varies depending on the input file size and the number and data rate of the encoder representations. However, we recommend to use 60 as the maximum number of instances per encoding when getting started and to increase this limit if it proves insufficient.\nFor Dolby Vision encodings and conversions, starting from Encoder\nv2.208.0\n, the maximum number of instances have been increased to\n- 100 instances if the\nencodingMode\nconfigured is\nSTANDARD\n(which currently maps to\nTWO_PASS\n),\nSINGLE_PASS\nor\nTWO_PASS\n- 200 instances if the\nencodingMode\nconfigured is\nTHREE_PASS\nUsing the variables above, please request the following limits:\nQuota name\nLimit to request\nRunning On-Demand All Standard (A, C, D, H, I, M, R, T, Z) instances\n(maximum number of encodings) * 8 vCPUs\nProvisioned IOPS SSD (io1 & io2) volume storage (TiB)\n(maximum number of encodings) * 0.5\nProvisioned IOPS SSD (io1 & io2) I/O operations (IOPS)\n(maximum number of encodings) * 2000\nAll Standard (A, C, D, H, I, M, R, T, Z) Spot Instance Requests\n(maximum number of encodings) * (maximum number of instances per encoding) * 8 vCPUs * 3\nNetwork interfaces\n(maximum number of encodings) * (maximum number of instances per encoding)\nGeneral Purpose SSD (gp2 & gp3) volume storage (TiB)\n(maximum number of encodings) * (maximum number of instances per encoding) * 0.05\nThe values above assume 8-core instances. If you believe that your use case requires instances with a different number of cores, this number may need to be increased after discussion with your Bitmovin team.\nWe recommend requesting a limit increase for spot instance requests of at least 20,000-30,000 to ensure error-free and fast encodings.\nGenerally, it is a good idea to multiply the expected limit calculated for your current situation by 2, to have some margin in case you need to ramp up.\n📘\nNew Resource Quota requests for encoder versions >= 2.179.0\nStarting with the Bitmovin Encoder Version 2.179.0, we use the new more performant IO2 SSD storage type for our encodings starting with certain cloud regions. We are going to rollout this change to further regions.\nWhen you're using an encoder version greater or equal to 2.179.0 or\nBETA\nas your configured encoder version in your encoding workflow in combination with Bitmovin Cloud Connect AWS, you need to ensure to request new quota requests for the IO2 storage type in order to ensure a smooth encoding process:\nIOPS for Provisioned IOPS SSD (io2) volumes\nStorage for Provisioned IOPS SSD (io2) volumes, in TiB\nIn addition to this change, we've also doubled the amount of IOPS used per disk to increase encoding performance. In order to keep your workflow running smoothly with the abovementioned newer encoder versions, you need to request quota increases for double the amount of IO1 (and IO2) than you had configured beforehand. We've already adjusted the values in the table above accordingly.\nNote: there is no form for Spot Instance Request Limit increases, so you have to request this in the Use Case description of the Instance Limit increase form.\nAppendix\nTroubleshooting Guide\nWhen configuring your Cloud Connect account, several things might go wrong, especially when you are doing it for the first time. Below is a concise list of common issues along with corresponding error messages or behaviors.\nWrong account number, access key, secret key\nYou will encounter a failure when attempting to create a new region setting for that infrastructure.\nError message: “Got AmazonEC2Exception on retrieval of security group 'sg-12345': AWS was not able to validate the provided access credentials.”\nWrong security group ID\nYou will encounter a failure when attempting to create a new region setting.\nError message: “Got AmazonEC2Exception on retrieval of security group 'sg-12345': Invalid id: \"sg-12345\".”\nWrong region for the security group ID\nYou will encounter a failure when attempting to create a new region setting.\nError message: “Got AmazonEC2Exception on retrieval of security group 'sg-12345': The security group 'sg-12345' does not exist.”\nWrong inbound rule \"All traffic\" - security group internal\nYou will encounter a failure when attempting to create a new region setting.\nError message: “No 'All' IpPermission found for Group sg-12345 .”\nWrong inbound rules: \"SSH port / Port 22\"\nThe encoding process is stuck in the 'Queue' state, but resolving the inbound rule issue allows the encoding to complete.\nWrong inbound rules: \"Port 999\"\nThe encoding process is stuck in the 'Queue' state, but resolving the inbound rule issue allows the encoding to complete.\nWrong outbound rule\nThe encoding process is stuck in the 'Queue' state, but resolving the inbound rule issue allows the encoding to complete.\nLimited-access IAM user policy\nThe following JSON snippet contains the set of permissions required to perform encodings on EC2 instances. You can assign it to the IAM User, if you prefer not to use the\nAmazonEC2FullAccess\npolicy.\nUnder \"IAM > Access management >\nUsers\n\", click on the\nname of your (newly created) user\nto open the details view\nUnder \"Permissions > Permissions policies\", click on the \"\nAdd permissions\n>\nCreate inline policy\n\" button\nSelect \"Policy editor >\nJSON\n\"\nPaste the following JSON policy into the \"\nPolicy editor\n\" window:\nJSON\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:TerminateInstances\",\n        \"ec2:StartInstances\",\n        \"ec2:CreateTags\",\n        \"ec2:RunInstances\",\n        \"ec2:StopInstances\"\n      ],\n      \"Resource\": [\n        \"arn:aws:ec2:*:*:subnet/*\",\n        \"arn:aws:ec2:*:*:instance/*\",\n        \"arn:aws:ec2:*:*:volume/*\",\n        \"arn:aws:ec2:*:*:security-group/*\",\n        \"arn:aws:ec2:*:*:network-interface/*\",\n        \"arn:aws:ec2:*::image/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ec2:DescribeInstances\",\n        \"ec2:RequestSpotInstances\",\n        \"ec2:DescribeTags\",\n        \"ec2:DescribeVpnConnections\",\n        \"ec2:DescribeVolumesModifications\",\n        \"ec2:DescribeSpotInstanceRequests\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:GetConsoleOutput\",\n        \"ec2:DescribeSpotPriceHistory\",\n        \"ec2:CancelSpotInstanceRequests\",\n        \"ec2:GetPasswordData\",\n        \"ec2:GetLaunchTemplateData\",\n        \"ec2:DescribeScheduledInstances\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeScheduledInstanceAvailability\",\n        \"ec2:DescribeElasticGpus\",\n        \"ec2:DescribeInstanceStatus\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\nClick the \"\nNext\n\" button\nIn \"Review and create\" (Step 2), provide a \"\nPolicy name\n\" of your choice, then click the \"\nCreate policy\n\" button",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/812af24-image.png",
      "https://files.readme.io/d2da493-image.png",
      "https://files.readme.io/a4ed685-image.png",
      "https://files.readme.io/20b7689-image.png",
      "https://files.readme.io/597262b-image.png",
      "https://files.readme.io/109059d-image.png",
      "https://files.readme.io/eac935c-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-encoding-with-broadpeak-io",
    "title": "Live Encoding with broadpeak.io",
    "text": "broadpeak.io\nis a cloud-based SaaS video API service from\nBroadpeak\nthat offers a range of services including Server Side Ad Insertion (SSAI). To work with Bitmovin Live Encoding the following HlsManifestAdMarkerType should be enabled:\nEXT_X_DATERANGE\n.\nPrerequisites\nYou will require your own\nbroadpeak.io\naccount. You can sign up for a trial account at\nhttps://app.broadpeak.io/\nEnsure you're familiar with Live Encoding settings match the requirements in\nLive Encoding with HLS, SCTE-35 and SSAI\nHow to Setup\nStart a Bitmovin Live Encoder\ne.g.: use this Java API code example as the basis\nLive Encoding with HLS, SCTE-35 and SSAI\nmodifying it with the following changes:\nChange the HlsManifestAdMarkerType to EXT_X_DATERANGE\nOptionally, adjust the video ladders (codec specs and muxing type) to match the default transcoding profile in\nbroadpeak.io\nHLS compatibility\nHLS version 4 or lower. Higher versions may still work in some limited cases.\nContains a single audio group\nContains at most one subtitle group\nTS Media containers\n(recently\nbroadpeak.io\nadded Cmaf/MP4 support)\nEXT-X-STREAM-INF tags have the \"BANDWIDTH\" and \"CODECS\" attributes\nEXT-X-PROGRAM-DATE-TIME tag must be present in media manifests\nAn Initialization Vector (IV) is present if your content uses AES-128 Encryption\nEXT-X-DATERANGE and/or EXT-OATCLS-SCTE35 tags enabled\nStart your Live ingest stream\ne.g.: Use the example method from the “Sample ingest with a demo file” section in\nLive Encoding with HLS, SCTE-35 and SSAI\nSetup a\nbroadpeak.io\nconfiguration\n📘\nNote\nThe following changes will only be possible when the live stream is already running, otherwise the Source resource cannot be created.\nAdditionally, unless specified, pre-configured sample resources will be used, eg. for the ad server\nCreate a Live stream input source:\nGo to\nSources\nClick\nCreate New\nType: Live, Name: Any, URL: URL to your live stream’s master manifest\nCreate a new ad insertion service:\nGo to\nServices\nClick\nCreate New\nType: Ad insertion, Name: Any,  Content: the previously created live source → next\nType: Ad replacement → add server: dummy ad server → next → run\nAdd ad transcoding if required with the default profile, and a gap filler slate\n⚠️ The transcoding profile may or may not work for your content. Get in touch with\nbroadpeak.io\nsupport to check on that\n⚠️ For every different Muxing, number of A/V channels, renditions etc. you need a dedicated profile. Only the\nbroadpeak.io\nteam are able to create it\nInitiate SCTE markers in the upstream contribution platform from the Bitmovin Live Encoder\n⚠️\nbroadpeak.io\ndoes not support multiple triggers on the same segment, if the break durations of those triggers do not align.\n⚠️ When you first try playback of the\nbroadpeak.io\nservice URL, you may not see your ad breaks replaced with new ads; it may take a little while for the ads to be transcoded by\nbroadpeak.io\nbefore they can be inserted in your stream. This is usually done within a few minutes - depending on the number and size of the ads\nIf everything is configured correctly the output should look like:\nResources\nbroadpeak.io documentation",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/afa4f17-SCTE_partners_SSAI_workflow_with_Broadpeak.io.png",
      "https://files.readme.io/59fc13e-Updated_Image_1.png",
      "https://files.readme.io/954169e-Updated_Image_2.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoder-22000-22490",
    "title": "Encoder 2.200.0 - 2.230.0",
    "text": "2.230.0 (BETA)\nReleased 2025-04-01\nAdded\nAdded support for configuring ESAM (Event Signaling and Management) settings in Live encodings. Currently, only the NOOP action is supported —CREATE, REPLACE, and DELETE actions are not yet available. The Bitmovin encoder can submit an SCTE 35 splice insert message to a Placement Opportunity Information System (POIS) to manage ad markers during live streams. ESAM settings are applied to Live encodings and can be configured via the\nStart Live Encoding\nrequest.\nFixed\nBackup packet generation fixed during prolonged reconnection periods with rejected reconnects. Frame rate recalculation during reconnects now filters extreme outliers in packet timestamps.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.229.0\nReleased 2025-03-18\nAdded\nIf specified in the encoding, cache-control settings are now applied to HLS and DASH manifests upon upload.\nSCTE-35 triggers are now injected into\nTsMuxings\nfor streams with\nH264VideoConfiguration\nor\nDolbyDigitalAudioConfiguration\nin VOD encodings.\nThe BCF no longer rejects a video stream after a reconnect if the only change compared to the reference stream (prior to the reconnect) is that any of the color properties (color_primaries, color_range, color_space, color_trc) transition from unspecified to specific values.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.228.0\nReleased 2025-03-11\nChanged\nInternal stability improvements\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.227.0\nReleased 2025-03-06\nFixed\nstartNumber, endNumber, mediaPresentationDuration are now calculated and set for template as well as timeline dash manifests once a live encoding is finished. The type is also changed to static. That means that once a Live Encoding is stopped, the already existing/available content that remains is represented within the manifest and a player can handle that accordingly to play the remaining segments until the actual end of content is reached.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.226.0\nReleased 2025-02-25\nFixed\nWhen using\nSCTE-35 triggers\nin 3-pass encodings, the\nCUE tags\nwere missing in the manifests.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.225.0 (STABLE)\nReleased 2025-02-18\nFixed\nFor video inputs where the first packet does not have the lowest PTS (Presentation Time Stamp), SCTE-35 triggers may have been missing or incorrectly timestamped.\nFixed an issue that caused Audio Output to stop in long-running live streams.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.224.0\nReleased 2025-02-11\nFixed\nImproved playback synchronization by ensuring AAC audio codec delay (priming) and audio roll is properly signaled in progressive MP4 muxings (in the\nelst\n,\nsgpd\nand\nsbgp\nboxes).\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.223.0\nReleased 2025-02-04\nAdded\nAdded support for periodic\nProgramDateTime\ntag placement in HLS manifests.\nFixed\nImproved resiliency while reconnecting the ingest for a Live Encoding after a disconnect due to network issues.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.222.1\nReleased 2025-01-22\nAdded\ncodecMaxBitrateFactor\nand\ncodecBufsizeFactor\nare added to\nStreamPertitleSettings\n, to enable different\nhrd\nparameters per stream when using Per-Title.\nThe values calculated with these factors will override any other\nmaxBitrate\nrespectively\nbufsize\nvalue.\nFixed\nIf LiveDashManifest's\nminimumUpdatePeriod\nis explicitly set for Live Encodings and\nDashRepresentationType.TEMPLATE\nit is now correctly set in the output Live DASH manifest.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.221.0\nReleased 2025-01-14\nAdded\n[Beta-Feature] Added the possibility to configure\nSCTE 35 triggers\nfor VOD Encoding. When a binary encoded representation of an SCTE 35 trigger is attached to an encoding, the Bitmovin encoder will interpret it and insert a segment boundary to the output streams if needed. When a stream of such an encoding is used to generate an HLS manifest the triggers will be added as cue tags to the respective segments. A SCTE 35 trigger is added to all streams of an encoding. The usage of such triggers is limited to encodings defining at least one video stream and without subtitle streams. Concatenation, Dolby Vision, or trimming workflows are not yet supported. This feature is currently in BETA and some changes are expected in the coming releases.\nLive Encoding Enhancements:\n- HE-AAC & HE-AAC v2 (2048 frame size)\n- HE-AAC: Supports mono, stereo, and 5.1 audio at 44.1 kHz or 48 kHz.\n- HE-AAC v2: Supports mono and stereo audio at 44.1 kHz or 48 kHz.\n- HE-AAC & HE-AAC v2 (1024 frame size)\n- General AAC streams with a 1024 frame size remain fully supported.\nChanged\nUpgraded SRT version to most recent version 1.5.4 for Live Encodings, resulting in improved/reduced latency, enhanced connection stability, and better compatibility with newer workflows.\nFixed\nFixed an issue with triggering reconnects in the case of Live Input's verification failure.\nFix to set\nBANDWIDTH\nin HLS manifest based on\nmaxBitrate\nif\ncrf\nis used in codec configuration.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.220.0\nReleased 2024-12-17\nAdded\nColor Parameter\natom has been added to progressive MP4 muxings for HDR10 and Dolby Vision.\nFixed\nCombination of Rotate filter and Enhanced Watermark filter sometimes resulted in encodings to stall. This has been fixed.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.219.0\nReleased 2024-12-10\nAdded\nMastering Display Color Volume\nand\nContent Light Level Information\natoms have been added to progressive MP4 muxings for HDR10 and Dolby Vision 8.1.\nAn additional live input verification step was added, improving resiliency for all live input types.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.218.0\nReleased 2024-12-03\nFixed\nIn case of 3-Pass it was possible that parts of the outputs were marked as \"Constant Bitrate\" via the\ncbr_flag\n. This was fixed. Average bitrate encodes (ABR) are not supposed to be partially constant.\nFixed an issue where encoding SRT to CEA 608/708 with more than 5 renditions could fail.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.217.0\nReleased 2024-11-19\nChanged\nInternal efficiency improvements to reduce the overall latency of Live Encodings\nStrict 608 subtitle messages, that need to be shifted due to length-restrictions of the standard's definition,  are now shown closer to the originally signaled presentation time.\nFixed\nWrong Dolby Vision Profile 5 codec tag for progressive DASH manifests with manifest generator V2: E.g. instead of\ndvh1.2.4.H120.90\nthe correct tag\ndvh1.05.03\nis set now\nAdded\nThe brand\ndby1\nwas added to Dolby Vision Profile5 Mp4/Fmp4 compatible-brands\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.216.0\nReleased 2024-11-12\nAdded\nEncodings from Dolby Vision input to HDR10 and SDR output now support cropping.\nHLS and DASH manifests for Dolby Vision 8.1 can now be created using manifest generator V2.\nFixed\nFixed potentially failing Dolby Vision encodings with applied crop filter. Dolby Vision outputs don't allow a change in the aspect ratio (i.e., an image distortion). However, with applied crop filters, the aspect ratio is allowed to change as long as this does not result in a distorted output. Previously the crop filter was not considered when performing the aspect ratio validation for Dolby Vision outputs.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.215.0\nReleased 2024-10-29\nAdded\nSupport for the following regions on Akamai Connected Cloud:\n- Madrid, Spain (\nes-mad\n)\n- Jakarta, Indonesia (\nid-cgk\n)\n- Chennai, India (\nin-maa\n)\n- Milan, Italy (\nit-mil\n)\n- Amsterdam, Netherlands (\nnl-ams\n)\n- Stockholm, Sweden (\nse-sto\n)\n- Los Angeles, USA (\nus-lax\n)\n- Miami, USA (\nus-mia\n)\n- Chicago, USA (\nus-ord\n)\nEnabled additional instance types in Akamai Connected Cloud, increasing the available peak capacity for the Bitmovin LIVE as well as VOD Encoder in each Akamai region.\nLive Encoding has a new option added to the shutdown configuration\nwaitingForFirstConnectTimeoutMinutes\n. This live encoding will wait for the first connection for the specified duration. If the input is not connected within this time, the encoding will stop automatically.\nChanged\nWhen reconnecting an input stream after disconnection for Live Encodings, all streams' codec parameters must be equivalent to the previous input. This release excludes the stream's\nextra data\nfrom the comparison.\nFixed\nImproved manifest writing logic for Live Encodings to prevent incomplete manifests in cases of delayed first-segment processing, ensuring more reliable and robust performance.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.214.0\nReleased 2024-10-22\nAdded\nImplemented rollover behaviour for\nTimecodeTrackTrimming\nfor which the end time code of the input exceeds the 24h format. E.g., a configured trimming start time code of 00:00:00.000 would be interpreted as 24:00:00.000 for an input with a start time code of 23:59:45.000 and an end time code of 24:10:00.000.\nFixed\nEnhanced Live encoder frame rate support for common frame rates, addressing corner cases that could cause A/V sync issues (e.g. with 23.976 fps)\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.213.1\nReleased 2024-10-15\nAdded\nIntroduced functionality for converting subtitles from\nSRT format to CEA-608/708\nclosed captions.\nThe H265VideoConfiguration now supports the creation of Dolby Vision Profile 8.1 outputs, allowing for high-quality HDR video encoding. A new value, DOLBY_VISION_PROFILE_8_1, has been added to the H265DynamicRangeFormat enum to simplify the configuration of H265 outputs using this profile. Additionally, support for the CropFilter has been introduced for use with the Dolby Vision 8.1 profile, enabling further customisation of video output. This feature is available only for progressive outputs and currently does not support manifest formats such as DASH or HLS. This would be added soon.\nFixed\nResolved an issue where SCTE-35 auto-return markers were not being set correctly when used in conjunction with the 'splice immediately' command. This fix ensures proper marker behaviour and seamless content transitions during ad insertion or other splice events.\nImproved Live encoder frame rate handling for common frame rates to support contribution encoder and fix some edge cases where A/V sync issues might occur.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings\n2.212.0\nReleased 2024-10-02\nAdded\nSegmented workflows using\nAkamai NetStorage\nas output now verify the correct upload of segments and report an error of the encoding in case some are missing.\nChanged\nFurther improvement to the Per-Title tuning of the previous release. We introduced a limit to restrict the bitrate increases to a maximum of 50%.\nImproved resilience to interact with Akamai outputs during internal processes which should reduce the few sporadic network problems occurring for such encodings.\nFixed\nBug fixed for audio output when mapping a 6-channel input audio track to a (5.1) and (5.1 back) configuration using AudioMix. In the output, only the Front Left (FL) and Front Right (FR) channels had audio, while the other channels remain silent. Now all 6 channels have same audio as in input.\nKnown Issues\nSegment validation is not supported for S3 role based, generic S3 (includes Linode and OCI), and FTP and not for LIVE encodings.\n2.211.0\nReleased 2024-09-24\nAdded\nAdded\noffsetInSeconds\nto\nreset Live manifest time-shift\ncall. With that it is possible to reset the live manifest at a given duration of the live event.\nChanged\nPer-Title improvement for assets with very high fluctuations in complexity. In this case per-title will choose a higher bitrates to resolution ratio to improve quality.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.210.0\nReleased 2024-09-17\nFixed\nIn some cases, the number of segments between Live manifest and Live2Vod manifest were not the same. This has been corrected now.\nFor Live Encodings, an issue with handling variable frameRate in combination with Cea608708SubtitleConfigurations has been fixed.\nChanged\nThe time zone indicator in the HLS manifest date format for the tag\n#EXT-X-PROGRAM-DATE-TIME\nnow uses the character\nZ\ninstead of\n+000\n. For instance instead of\n2024-09-16T18:49:48.720+0000\n, it would be\n2024-09-16T18:49:48.720Z\n.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.209.1\nReleased 2024-09-10\nAdded\nHLS manifest creation with WebVtt muxed in FMP4 is now supported.\nWe have updated the Live Encoder to support contribution devices that output variable frame rates, the live encoder can now offer a constant bitrate based on the average frame rate conforming to a common standard of 24, 25, 29.97, 30 etc... on the output. This means we can support streams from software and devices such as iOS, Android and Zoom.\nFixed\nFixed wrong AVERAGE-BANDWIDTH and BANDWIDTH attributes for HLS I-frame playlists ( EXT-X-I-FRAME-STREAM-INF ) generated with manifest generator V2.\nFixed an issue where using multiple chunked text muxings would lead to missing subtitle cues in the output segments.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.208.1\nReleased 2024-08-28\nFixed\nIn cases of complex inputs like JPEG2000 (J2K) and ProRes, when the output resolution is configured to be smaller than the input content resolution, crop filter was not working as expected. This has been fixed now.\n2.208.0\nReleased 2024-08-27\nChanged\nDolby Vision encodings would benefit from increased machines per encode (100 instances for EncodingMode\nSINGLE_PASS\n,\nTWO_PASS\nand 200 instances for\nTHREE_PASS\n), resulting in improved execution time.\nFixed\nImproved rate-control of VP9 encoder.\nIn cases of complex inputs like JPEG2000 (J2K) and ProRes, the\ncrop\nfilter was incorrectly applied twice due to internal processing of complex inputs. This has been fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.207.0\nReleased 2024-08-13\nAdded\nFor RTMP input including the stream-key directly in the URL instead of in a separate field is now supported. So both\nrtmp://[server-domain-or-ip]:[port]/[application]/[stream-key]\nor\nrtmp://[server-domain-or-ip]:[port]/[application]/+ [stream-key]\nas a separate field are now accepted.\nChanged\nVarious improvements for HEVC under the hood which positively influence quality and turnaround speed.\nFixed\nFixed potential stalling encodings with HEVC slices greater 1.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.206.0\nReleased 2024-08-06\nFixed\nFixed an issue where encodings would sometimes fail in case more than 10 output streams where configured with at least one of them being subtitles.\nFixed failing per-title encodings having duplicate configured output paths in different outputs.\nFixed an issue where ignored progressiveWavMuxings lead to an error.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.205.0\nReleased 2024-07-23\nAdded\nSupport for muxing Webvtt into FMP4 container has been added.\nFixed\nFixed an issue that caused delays for triggering the LIVE_INPUT_STATUS_CHANGED webhook for the status CONNECTED.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.204.0\nReleased 2024-07-16\nFixed\nFixed a bug, where inserting a custom keyframe exactly on an audio segment boundary and using the associated keyframe ID for defining a Dash period resulted in a wrong start segment for the audio dash representations.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.203.0\nReleased 2024-07-09\nAdded\nSupport added for Google Cloud region:\nme-central2\nProgressive MP4 muxings are now supported for live encodings.\nPCM Codec Configuration\ncan now be used for creating custom PCM streams from any input audio stream (previously, it's usage was restricted to xdcamhd workflow).\nSupport for\nS32LE\nsampleFormat\nand\nSTEREO\nchannelLayout\nhas been added to\nPCM Codec Configuration\n.\nFixed\nFixed potential out-of-memory issues for encodings having multiple high-resolution Dolby Vision renditions.\nAn issue where custom keyframes were not correctly propagated into mp4Muxing with fragmentDuration was fixed. Custom keyframes are now correctly applied.\nFixed a bug that caused Live HLS manifest to stall. This bug was triggered when single segments would take longer to process than expected.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.202.0\nReleased 2024-06-25\nAdded\nIntroduced a new Muxing - ProgressiveWavMuxing. It is now supported to use PassthroughAudioConfiguration for PCM input streams with ProgressiveWavMuxing.\nAdded support for Live Heartbeat Webhooks. It is now possible to configure global Webhooks that periodically send updates about the Live Encoding status.\nNote:\nThis feature is available with\nencoderVersion\n=>\n2.201.0\nand above. Encodings started with older encoder versions than\n2.201.0\nwill not trigger heartbeat notifications. Only the live encodings that are created after the webhook was configured will send heartbeats. Encodings that are already running will not send heartbeats. When a webhook is deleted, Live encodings created afterward will not send heartbeats. The existing running live encodings will keep sending the heartbeats.\nChanged\nSpeed improvement of internal preprocessing to prevent gateway timeout in case many streams with stream conditions are used.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.201.0\nReleased 2024-06-18\nAdded\nAudio Passthrough of AAC audio streams into\nProgressiveMovMuxing\nis now supported.\nChanged\nWhen using the endKeyframeId of fmp4 Representation or Chunked Text Representation the segment containing the end keyframe is not included in the segment list of the representation anymore by default. This means, that the last frame included in the period is the frame before the keyframe having the endKeyframeId.\nFixed\nWhen using Per-Title with many fixed resolutions, it was possible that the highest bitrate was not assigned to the highest fixed resolution. This is fixed.\nFixed bug in the Dolby E extraction where the extraction of the stream would not happen correctly in case the Dolby E ingest stream is not selected as the first stream\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.200.0\nReleased 2024-06-11\nAdded\nIt is now supported to configure audio passthrough with\nAudioPassthroughConfiguration\nfor AAC input streams into\nMP4Muxing\nand PCM input streams into\nProgressiveMovMuxing\n.\nAdded new filter\nAzureSpeechToCaptionsFilter\n(for LIVE Encoder). This can be used to generate subtitles by transcribing an audio Stream by using Azure's Speech To Text Cognitive services.\nAdded support for Dolby E in a single stereo stream as input.\nFixed\nProgressive text muxings, with audio only workflows, would not generate any subtitles. This has been fixed.\nTimescale associated to subtitle representations in\ndash manifests\n(either just-in-time or post-encoding) would not always be consistent when multiple periods were configured. The timescale is now always set to 1000 and is consistent with the presentation time offset.\nA bug where rate change could drop all frames in the last segment of the video processing was leading to stall and later to error. This has been fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-per-title-encodings-with-drm-solutions",
    "title": "Using Per-Title Encodings with DRM Solutions",
    "text": "The DRM use-case works in exact the same way as with a normal encoding.\nIn addition to our\ngeneral tutorial on Per-Title encoding\n, you just need to add the DRM configuration to the Muxing of the Per-Title template Streams in the same way as you would do it for a normal encoding configuration. One thing to watch out is if you are using different encryption keys for SD, HD and UHD content. There are two things to consider:\nIf you use the autoRepresentations feature, make sure to have one Per-Title template Stream with configured resolution close to each category where you want to use a different encryption key. E.g., for SD use a width of 640.\nMake sure to add the DRM configuration with the encryption key you want to use for that category. If you are not using autoRepresentations, just configure the DRM configuration on every Per-Title template stream with the encryption key you want to use for that representation.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-standby-pools-management",
    "title": "Live Standby Pool Management",
    "text": "Managing multiple Live Standby Pools\nOnce created, an overview of all the currently configured and running Live Encoding Standby Pools can be seen in Live -> Configurations -> Live standby pools.\nVia the API a list of configured pools can be discovered with\nList Standby pools\nHere you can see the health of the pool via the coloured dot next to the Pool name. The Pool ID can be copied easily, the Template can be previewed and replaced. The current size can be seen and the Target size adjusted.\nUsing the quick menu on the right side of the pool, you can:\nView the Pool log\n: to see all actions taken and at what time\nStop the Pool\n: to stop all running live encoders\nDelete the Pool\n: to remove it\nView the log\nThe log will record all events taken at a management level on the pool such as adjustments or live encoders being acquired or stopped, or record any errors the pool has run into.\nThe API endpoint for this data is\nList event logs for a standby pool\nAdjusting the Template\nThe Encoding Template can be edited at any time in the Dashboard, again you will need to previewed and approve the template before updating it.\nVia the API this can be done by using the PATCH command\nPartially update standby pool by id\n📘\nRefreshing the Pool\nOnce a new Template is selected, any Live Encoders that are currently running will remain using the same configuration. To refresh the pool, set the Target size to 0, or Stop the Pool. Once stopped, set the correct size again, all Live Encoders will now restart with the updated Encoding Template configuration.\nTarget Size\nThe Target size can be adjusted at any time, providing you have the correct Account Limits set to accommodate the number required.\nTarget size can be adjusted via the PATCH command\nPartially update standby pool by id\nViewing the Pool and Acquiring\nYou can view the state of a pool by clicking on the pool, you'll see all Live encoders in each state.\nState\nDescription\nPreparing\nCloud infrastructure has been requested and waiting to be made available, or currently being configured with the correct image and settings.\nReady\nCloud infrastructure has been provisioned and the Live Encoder can be acquired.\nError\nTypically an error with the infrastructure or encoding template configuration. Logs will offer greater insights, and customers should contact support if problems persist.\nThis is available via the API\nList encodings from a standby pool\nit is possible to sort the results by\ncreatedAt\n+\neventType\n+\nstandByPoolId\nAcquiring from the pool\nAcquiring from the pool before using it and connecting an input is vital in order to make sure the pool backfills the encoding.\nThis can be easily done in the Dashboard within the Pool by pressing Acquire encoding, and you'll be redirected to the pool page.\nWith the Encoding ID known this can be done via the API\nAcquire an encoding from a standby pool",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/c9df510e0e85e294820be880f9702c097f249fcb41204148d557cc052b1d9c32-Screenshot_2024-12-02_at_11.02.34.png",
      "https://files.readme.io/a1333ec09a5a4b954b9c898811472323822bf781297d1799e90b575721a308e2-Screenshot_2024-11-29_at_17.05.19.png",
      "https://files.readme.io/d2c436f4406962d0e1d230a3b917929004950491278584a596629c57b4665e53-Screenshot_2024-11-29_at_17.10.43.png",
      "https://files.readme.io/51e33e73ec56cb85f3e5ec8e0cee88689f89d124c0bf09d7a1273f611be17878-Screenshot_2024-11-29_at_17.15.41.png",
      "https://files.readme.io/c50a15504cfb1f59412ccbc5b8d2dc9e1d94a00796a772cce30946d2626328de-Screenshot_2024-11-29_at_17.16.34.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/setting-up-an-akamai-netstorage-input",
    "title": "Setting Up an Akamai NetStorage Input",
    "text": "Overview\nThe NetStorage service of Akamai provides many ways to interact with it. The best way from our perspective is to use its HTTP API interface. In this tutorial you will learn how it works and what it needs to obtain proper credentials to create an Akamai NetStorage Input/Output in our service.\nThe biggest advantage is the recilience of HTTP requests and the amount of parallel connections you can leverage compared to FTP transfers, where it is capped to 25 connections for one akamai upload account. The\nNetStorage HTTP CMS API\nhowever doesn't has such limits and is therefore well suited to handle many parallel chunked transfers in a fast and reliable manner.\nSo lets look into it how you can configure an Akamai NetStorage to be used for your encodings in our service.\nIMPORTANT\n: This tutorial assumes that you already have access to an Akamai NetStorage and Setup and a dedicated Upload Account for it. If you are missing any of it yet,\nplease read through this tutorial first\non how to create an Akamai NetStorage and Upload Account before you proceed with this tutorial.\nCreate a Bitmovin Akamai NetStorage Input\nTo create a Bitmovin\n\"Akamai NetStorage\" Input configuration\n, that can be used by our encoding service, you need to know the\nHTTP API Key\nand\nUsername\nof your upload account, as well as its\nHost\nURL.\nUse the Dashboard UI\nSelect the\nEncoding\nmenu on the left and go to\nInputs\n.\nClick on\nCreate\nin the upper right corner of the Inputs overview\nSelect\nAkamai\nas Input type and\nEnter all required fields (please see the image below)\nClick on\nCreate\nUse an Bitmovin API SDK\nEach of our Open API SDK's implements the Bitmovin API, and make it easy to start its integration in your project or use-case. Use them to create reusable input resources to be used for your encodings:\nBitmovin API SDK for Java - Input example\nJava\nAkamaiNetStorageInput input = new AkamaiNetStorageInput();\ninput.setHost(AKAMAI_NETSTORAGE_HTTP_API_HOST); // e.g. xxxxx-nsu.akamaihd.net\ninput.setUsername(AKAMAI_NETSTORAGE_USERNAME);\ninput.setPassword(AKAMAI_NETSTORAGE_PASSWORD);\ninput = bitmovinApi.input.akamaiNetStorage.create(input);\nSee all available examples for each of our Bitmovin API SDK's in our\nGH Example Repository\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/b11c9aa-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/specifying-required-resolutions-for-per-title-encoding",
    "title": "Specifying Required Resolutions for Per-Title Encoding",
    "text": "Based on our [How to create a pertitle\ngeneral tutorial on Per-Title encoding\n, you can do the following:\nAdd Per-Title template Streams for every possible resolution that you want to support. The codec configuration needs to contain at least the height, or the width in that case. If only one of the both resolutions is specified the\naspect ratio\nwill be kept.\nAdd one or more muxings to the Per-Title template Stream, e.g., MP4 muxing if you want to have single progressive MP4 files as output, or segmented fMP4 muxing for DASH/HLS. Also define an output path with the supported placeholders to make it unique:\n{uuid}\n,\n{bitrate}\n,\n{width}\n,\n{height}\n. We always encourage to make\n{uuid}\npart of the output path to ensure it will be unique. E.g.,\n/video/{bitrate}_{uuid}\n.\nIn the start call of the encoding you can find a Per-Title configuration object and inside this object a configuration per codec, e.g., h264Configuration. You do not need to configure anything specific here if you want the default values to apply. That’s it! Start your encoding in this configuration and the Per-Title algorithm will generate an optimal bitrate ladder using the resolutions you have defined for your asset. Please note that the algorithm might decide to not use a specific resolution if it does not make sense for a specific asset.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-pre-warmed-encoder-pools",
    "title": "Using Pre-warmed Encoder Pools",
    "text": "Overview\nPre-warmed encoder pools, are a collection of idle encoding resources which can be immediately acquired by starting encodings that are configured to take resources from such a pool. If an encoding takes a resource from a pool, this pool automatically provisions additional encoding resources to maintain its target size at any time.\nWhy would I use that?\n- It allows you to optimize the total turn around time of your workflow by basically removing the queue time for an encoding. When properly configured for your use-case, your encoding queue time drops down ~0 to a few seconds consistently. This way you can achieve a continuous throughput as a dedicated pool of encoding resources is exclusively available to your Bitmovin Account.\nKey Features\nCreate a pool of pre-warmed encoding resources\n- Pools refill automatically and maintain a minimum number of always available encoding resources.\nDynamically scale pre-warmed encoder pools on demand\n- Changing demand or peaks in you Encoding flow can be addressed by dynamically adapting the number of available encoding resources.\nSchedule the availability of pre-warmed encoder pools\n- If you only want to have them pre-warmed, e.g. during your business hours or other specific time frames where you know you would need them, you can schedule the start and stop of a pool ahead of time.\nWorks with\nCloud Connect\nfor AWS, GCP, and Azure\n- Create pre-warmed encoder pools in your own connected cloud infrastructure.\nRequirements\nOpen API SDK v1.168+ (\ndynamicPool\nsince v1.177.0+)\nAs running pools generate costs even if not utilized, they have to be enabled by our team.\nKnown Limitations\nThe following limitations exist:\nonly one pre-warmed encoder pool can be assigned to the start-request for an encoding\nAt the moment, pre-warmed encoder pools can be used for VoD encoding workflows only\nPool status - At the moment, it only reflects if a pool has been STARTED or STOPPED. The readiness of a pool to be used by encodings, can't be determined automatically yet. In general, after starting a pool, depending on the cloud provider and region, it can take ~ 5 minutes before it is ready to use.\ntargetPoolSize\nis limited to the number of available encoding slots to your Bitmovin Account.\nCreate a Pool\nRequired Pool Configuration Properties:\nencoderVersion\n- An explicitly defined encoder version OR\nSTABLE\ntag (recommended) for this pool. Only encodings that are configured to use the same encoder version can take resources from this pool.\ncloudRegion\n- An explicitly defined cloud region of a cloud provider for this pool. Only encodings that are configured to use the cloud region can take resources from this pool.\ntargetPoolSize\n- The number of encoding resources the pool has to keep available at all times. If one is taken from the pool, it \"refill\"s a new one automatically.\ndiskSize\n- (500GB|1000GB|2000GB) available to the encoding resource. The appropriate size depends on the use-case you are dealing with. E.g. when processing mainly small input files with a typical encoding profile, 500GB are usually sufficient. Workflows creating big progressive Outputs or dealing with RAW or Mezzanine files should use 1000GB or more.\nOptional Pool Configuration Properties:\ndynamicPool\n- Set a minimum\ntargetPoolSize\nof encoding resources, the number of encoding resources will increase and decrease on demand. To help managing the costs, any prewarmed encoder pool has a maximum size of\n10\ninstances. This value can't be changed and is applied to all pools. It will be enforced as soon as the pool adapts its size to incoming traffic (usually after the first encoding using the pool is started or after a prolonged period of inactivity).\ngpuEnabled\n- Use hardware-acceleration (GPU) in a pool's encoding resources.\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nPrewarmedEncoderPool poolToCreate = new PrewarmedEncoderPool();\npoolToCreate.setName(\"Fast-Track Encodings\");\npoolToCreate.setDescription(\"Use for encodings that have to be done immediately\");\npoolToCreate.setEncoderVersion(\"STABLE\");\npoolToCreate.setCloudRegion(CloudRegion.AWS_EU_WEST_1);\npoolToCreate.setDiskSize(PrewarmedEncoderDiskSize.GB_500);\npoolToCreate.setTargetPoolSize(1);\n\n//optional properties\npoolToCreate.setDynamicPool(true);\npoolToCreate.setGpuEnabled(true);\n\nPrewarmedEncoderPool createdPool = bitmovinApi.encoding.infrastructure.prewarmedEncoderPools.create(poolToCreate);\nStart a Pool\nOnce created, you can start a pool by issuing the start API Call:\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nbitmovinApi.encoding.infrastructure.prewarmedEncoderPools.start(createdPool.getId());\nWait for Pool to be Up and Running\nAs mentioned in \"Known Limitations\", only STARTED and STOPPED is available to reflect the status of a pool right now. Therefore, its recommended to wait for a few minutes (~5min) before the pool is ready to use, after being started.\nCheck the status of a Pool\nAs its an asynchronous process, you can check the status of a pool accordingly like this:\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nbitmovinApi.encoding.infrastructure.prewarmedEncoderPools.get(createdPool.getId());\nIts either STOPPED, or STARTED. As mentioned at \"Known Limitations\", the readiness of a pool to be used by encodings, can't be determined automatically yet. In general, after starting a pool, depending on the cloud provider and region, it can take ~ 5 minutes before it is ready to use.\nStart an Encoding using a Pool\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nEncoding encoding = new Encoding();\nencoding.setEncoderVersion(\"STABLE\");\nencoding.setCloudRegion(CloudRegion.AWS_EU_WEST_1);\n\n...\n//your encoding configurations...\n...\n\n//Start an Encoding with StartEncodingRequest Configuration\nScheduling scheduling = new Scheduling();\nscheduling.setPrewarmedEncoderPoolIds(array.asList(createdPool.getId()));\nStartEncodingRequest startEncodingRequest = new StartEncodingRequest();   \n\nstartEncodingRequest.setScheduling(scheduling);\nbitmovinApi.encodings.start(encoding.getId(), startEncodingRequest);\nStop a Pool\nIf a pool is no longer needed at the moment, you can stop using the stop API call. Once issued, the pool immediately starts to gracefully de-provision all its idle encoding resources.\nHint:\nAlready running encodings, using resources from a stopping pool, will continue to run and its resources will be de-provisioned as soon as they have successfully finished.\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nbitmovinApi.encoding.infrastructure.prewarmedEncoderPools.stop(retrieved.getId());\nDelete a Pool\nIf a pool is no longer needed, it can be deleted. A pool has to be STOPPED first, before it can be deleted.\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nbitmovinApi.encoding.infrastructure.prewarmedEncoderPools.delete(createdPool.getId());\nSchedule the START/STOP for a pool\nRequired Properties:\naction\n- (START|STOP) Determines if the pool shall be started or stopped\ntriggerDate\n- The date and time in the future (at least a full minute) the action shall be executed at. The scheduling time is on a per-minute basis, so seconds and milliseconds are not respected by the scheduling.\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nSchedule the Start of a Pool\nJava\nDate poolStartDate = new DateTime(2020, 11, 20, 9, 0, 0).toDate();\n\nPrewarmedEncoderPoolSchedule poolSchedule = new PrewarmedEncoderPoolSchedule();\npoolSchedule.setAction(PrewarmedEncoderPoolAction.START);\npoolSchedule.setTriggerDate(poolStartDate);\n\nPrewarmedEncoderPoolSchedule createdPoolSchedule =\nbitmovinApi.encoding.infrastructure.prewarmedEncoderPools.schedules.create(\ncreatedPool.getId(), poolSchedule);\nSchedule the Stop of a Pool\nJava\nDate poolStopDate = new DateTime(2020, 11, 20, 17, 0, 0).toDate();\n\nPrewarmedEncoderPoolSchedule poolSchedule = new PrewarmedEncoderPoolSchedule();\npoolSchedule.setAction(PrewarmedEncoderPoolAction.STOP);\npoolSchedule.setTriggerDate(poolStopDate);\n\nbitmovinApi.encoding.infrastructure.prewarmedEncoderPools.schedules.create(\nretrieved.getId(), poolSchedule);\nDelete a scheduled START/STOP of a Pool\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nbitmovinApi.encoding.infrastructure.prewarmedEncoderPools.schedules.delete(createdPool.getId(),createdPoolSchedule.getId());",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/using-bitmovin-cloud-connect-with-azure",
    "title": "Using Bitmovin Cloud Connect with Azure",
    "text": "This document explains how to set up Bitmovin Encoding on Azure Cloud infrastructure so that the Bitmovin platform can run encoders using the Azure API.\nThe instructions in this document for the\nREST API Services\napply to live encoding and file-based encoding. For a complete list of formats and input types, see the\nBitmovin website\n.\nPrerequisites\n🚧\nActivation required\nThis feature requires a commercial agreement and needs to be specifically activated for a Bitmovin account, it is not available by default. You will not be able to complete the configuration below without this activation.\nA Bitmovin account enabled for usage of Cloud Connect\nIf you want to use Cloud Connect with a sub organization, this sub organization must be enabled for usage of Cloud Connect\nAn\nAzure account\nwith a \"Microsoft.Compute >\nTotal Regional vCPUs\n\" quota for your region of at least \"\n8\n\" (also see\nResource Quotas\nbelow)\nNote:\nA free Azure trial subscription is not sufficient for Cloud Connect, as it has a \"\nTotal Regional vCPUs\n\" quota of \"\n4\n\". Consider upgrading your subscription at least to an Azure Pay as you go (PAYG) plan and requesting a quota increase in your desired region.\nConfigure your Azure account\nIn this section, you will create a separate resource group that will be used by the Bitmovin platform when interacting with your Azure infrastructure. Also, we will create the appropriate infrastructure setup to enable Encoding jobs.\nAdd the\nbitmovin-azure-connect\napplication\nThe\nbitmovin-azure-connect\napplication is used to access the\nAzure Compute Gallery\n(previously Shared Image Gallery) where the images reside that are required to start virtual machines (VMs) for Bitmovin encodings.\nOpen the Azure portal:\nhttps://portal.azure.com/#blade/Microsoft_AAD_IAM/ActiveDirectoryMenuBlade/Overview\nCopy your\nTenant ID\nfrom \"Overview >\nBasic information\n\"\nReplace\n<Your Tenant ID>\nwith your\nTenant ID\nin the following link and open it in your browser:\nHTTPS\nhttps://login.microsoftonline.com/<Your Tenant ID>/oauth2/authorize?client_id=ad59b58a-9910-409a-909e-cf98258bb566&response_type=code&redirect_uri=https%3A%2F%2Fbitmovin.com%2F\nThis will will open a \"\nPermissions requested\n\" modal for\nbitmovin-azure-connect\n, then click the \"\nAccept\n\" button\nCreate a Resource group\nThis Resource group is a container to hold all Bitmovin-related resources to run Cloud Connect Encoding jobs. (See:\nhttps://learn.microsoft.com/azure/azure-resource-manager/management/manage-resource-groups-portal\n) Additionally, we will add the\nbitmovin-azure-connect\napplication to the Resource group to manage Virtual machines.\nOpen the\nSubscriptions\nservice and select your subscription:\nhttps://portal.azure.com/#view/Microsoft_Azure_Billing/SubscriptionsBladeV2\nGo to \"Settings >\nResource groups\n\" in the sidebar, then click the \"\nCreate\n\" button\nIn \"Basics\" (Step 1), provide a \"\nResource group\n\" name and \"\nRegion\n\" of your choice, then click the \"\nReview + create\n\" button\nIn \"Review + create\" (Step 3), wait until the validation passed, then click the \"\nCreate\n\" button\nAllow the\nbitmovin-azure-connect\napplication to run Virtual machines\nWithin your\nSubscription\n, open the newly created\nResource group\nGo to \"\nAccess control (IAM)\n\" in the sidebar, click the \"\nAdd\n\" button and select \"\nAdd role assignment\n\"\nIn \"Role\" (Step 1), select the \"\nPrivileged administrator roles\n\" tab and select the \"\nContributor\n\" role, then click the \"\nNext\n\" button\nIn \"Members\" (Step 2), click the \"\nSelect members\n\" button. Search for and select the \"\nbitmovin-azure-connect\n\" application, then click the \"\nSelect\n\" button. Then click the \"\nReview + assign\n\" button\nIn \"Review + create\" (Step 3), click the \"\nReview + assign\n\" button\n📘\nTighten permissions for production\nIf you want to define and use a fine-grained custom role to narrow down the permissions on that resource group please reach out to the support team to get guidance on the required permissions.\nCreate a Network security group (per region)\n\"A network security group contains security rules that allow or deny inbound network traffic to, or outbound network traffic from, several types of Azure resources.\" (Source:\nhttps://learn.microsoft.com/azure/virtual-network/network-security-groups-overview\n)\n📘\nRunning Encodings in different regions\nIf you need to run different Encoding jobs in different regions, repeat the following steps for every region.\nWithin your\nSubscription\n, open your previously created\nResource group\n, then click the \"\nCreate\n\" button\nIn the Resource group \"\nMarketplace\n\", search for \"\nNetwork security group\n\"\nSelect \"\nNetwork security group\n\" (Microsoft Azure Service), then select \"\nCreate\n\" and click \"\nNetwork security group\n\"\nIn \"Basics\" (Step 1), provide a \"\nName\n\" and \"\nRegion\n\" of your choice, then click the \"\nReview + create\n\" button\nIn \"Review + create\" (Step 3), wait until the validation passed, then click the \"\nCreate\n\" button\nThis will open the \"\nNetwork security group\n\" after creation, then click the \"\nGo to resource\n\" button\nGo to \"Settings >\nInbound security rules\n\" in the sidebar, create new rules for each of the following tables by clicking the \"\nAdd\n\" button:\nKey\nValue\nSource\nIP Addresses\nSource IP addresses/CIDR ranges\n104.199.97.13/32, 35.205.157.162/32\nSource port ranges\n-\nDestination\nAny\nService\nCustom\nDestination port ranges\n9999\nProtocol\nTCP\nAction\nAllow\nPriority\n100\nName\nAllowBitmovinApi1\nKey\nValue\nSource\nIP Addresses\nSource IP addresses/CIDR ranges\n104.199.97.13/32, 35.205.157.162/32\nSource port ranges\n-\nDestination\nAny\nService\nCustom\nDestination port ranges\n9443\nProtocol\nTCP\nAction\nAllow\nPriority\n101\nName\nAllowBitmovinApi2\nKey\nValue\nSource\nIP Addresses\nSource IP addresses/CIDR ranges\n104.199.97.13/32, 35.205.157.162/32\nSource port ranges\n-\nDestination\nAny\nService\nCustom\nDestination port ranges\n22\nProtocol\nTCP\nAction\nAllow\nPriority\n102\nName\nAllowBitmovinApi3\n📘\nLive encodings\nAdditional inbound rules are required if you are encoding live streams transported over RTMP, SRT or Zixi.\nAdditional inbound rules for RTMP live streams\nKey\nValue\nDestination port ranges\n1935\nProtocol\nTCP\nPriority\n300\nName\nrtmp-listener\nDescription\nFor RTMP live streams\nKey\nValue\nDestination port ranges\n443\nProtocol\nTCP\nPriority\n302\nName\nAllowRTMPSInbound\nDescription\nFor RTMPS live streams\nAdditional inbound rules for SRT live streams\nKey\nValue\nDestination port ranges\n2088\nProtocol\nTCP\nPriority\n400\nName\nsrt-listener-tcp\nDescription\nFor SRT live streams\nKey\nValue\nDestination port ranges\n2088\nProtocol\nUDP\nPriority\n500\nName\nsrt-listener-udp-2088\nDescription\nFor SRT live streams\nKey\nValue\nDestination port ranges\n2090\nProtocol\nUDP\nPriority\n700\nName\nsrt-listener-udp-2090\nDescription\nFor SRT live streams\nKey\nValue\nDestination port ranges\n2091\nProtocol\nUDP\nPriority\n800\nName\nsrt-listener-udp-2091\nDescription\nFor SRT live streams\nAdditional inbound rules for Zixi live streams\nKey\nValue\nDestination port ranges\n4444\nProtocol\nTCP\nPriority\n900\nName\nzixi-listener\nDescription\nFor Zixi live streams\nCreate a Virtual network (per region)\n📘\nRunning Encodings in different regions\nIf you need to run different Encoding jobs in different regions, repeat the following steps for every region.\nWithin your\nSubscription\n, open your previously created\nResource group\n, then click the \"\nCreate\n\" button\nIn the Resource group \"\nMarketplace\n\", search for \"\nVirtual network\n\"\nSelect \"\nVirtual network\n\" (Microsoft Azure Service), then select \"\nCreate\n\" and click \"\nVirtual network\n\"\nIn \"Basics\" (Step 1), provide a \"\nVirtual network name\n\" and \"\nRegion\n\" of your choice, then click the \"\nNext\n\" button\nIn \"IP addresses\" (Step 3), delete the default subnet (Name:\ndefault\n, Size:\n/24 (256 addresses)\n)\nNote:\nThe default IPv4 address space gives you a CIDR notation of\n/16\nwith a size of 65,636 IP addresses, which should be sufficient.\nClick the \"\nAdd a subnet\n\" button, provide a \"\nName\n\" of your choice and select \"\nSize\n\" with\n/16 (65,536 addresses)\n, then click the \"\nAdd\n\" button\nClick the \"\nReview + create\n\" button, then finish the creation by clicking the \"\nCreate\n\" button\nThis will open the \"\nVirtual network\n\" after creation, then click the \"\nGo to resource\n\" button\nAssign the Network security group\nGo to \"Settings >\nSubnets\n\" in the sidebar,  then select your newly created\nSubnet\nSelect in \"\nNetwork security group\n\" your previously created Network security group\nConfigure your Bitmovin account\nBefore you continue, make sure you have collected the following information from Azure:\nFrom your Azure account\nTenant ID\nFrom your Resource group\nSubscription ID\nResource group name\nFrom your Virtual network\nVirtual network name\nSubnet name\n(under \"Settings > Subnets\")\nLink your Azure account\nTo enable your Bitmovin account to run encodings in your Azure account, you need to link it with Infrastructure and Region Settings objects.\nOpen the\nBitmovin Dashboard\n:\nhttps://dashboard.bitmovin.com/\nGo to \"VOD/LIVE Encoding >\nCloud Connect\n\"\nClick the \"\nAdd infrastructure account\n\" button, select the \"\nMicrosoft Azure\n\" option, click the \"\nNext\n\" button\nProvide a \"\nName\n\" of your choice, fill in the\nSubscription ID\n,\nResource Group ID/Name\nand\nTenant ID\n, click the \"\nNext\n\" button\nSelect the appropriate \"\nCloud Region\n\" (where the Resource group was created), fill in the\nNetwork Name\nand \"\nSubnet Name\n\", click the \"\nNext\n\" button\nRun encoding jobs in Azure\nAfter configuration has been completed, you will be able to run encoding jobs in your own Azure account. To do so, use the Bitmovin API client SDKs to submit encoding jobs, in the same way as you would do for encodings running in the Bitmovin Managed Cloud service. The only difference is that you need to specify the new infrastructure instead of public cloud regions.\nHere is a Python snippet demonstrating how to link your encoding to your infrastructure.\nPython\n# ID of the Infrastructure object\n    infra_id = ‘<infrastructure_id>’ \n    \n    # Azure region of the Azure-connect setup\n    infra_region = CloudRegion.AZURE_EUROPE_WEST\n    infrastructure = InfrastructureSettings(infrastructure_id=infra_id, \n                            cloud_region=infra_region)\n    \n    encoding = Encoding(name='azure connect encoding',\n        cloud_region=CloudRegion.EXTERNAL,\n        infrastructure=infrastructure,\n        encoder_version='STABLE')\nSub Organizations\nIf you have set up your infrastructure in a sub organization, you must tell the Bitmovin API that you want to run the encoding in that sub organization. Thus, in addition to the code snippet above, make sure to set the\ntenant_org_id\nalongside the\napi_key\nin the\nbitmovin_api\nobject:\nPython\n# ID of the sub organisation you added the infrastructure to\n    organisation_id = '<sub_organisation_id>'\n\n    bitmovin_api = BitmovinApi(api_key=config_provider.get_bitmovin_api_key(), \n                       tenant_org_id=organisation_id,\n                       logger=BitmovinApiLogger())\nResource Quotas\nIf you want to run several encodings in parallel, the default limits may not be sufficient. In that case, you will have to request limit increases for the following resource in your Region(s), through the\nQuotas page\n:\nFor the limits to request we will be using these variables:\nVariable name\nExplanation\n(maximum number of encodings)\nThe maximum number of parallel encodings the infrastructure must be able to run. Typically this is the number of encoding slots assigned to the Bitmovin account or sub-org associated with the infrastructure.\n(maximum number of instances per encoding)\nThe number of instances used by one encoding.\nThis number varies depending on the input file size and the number and data rate of the encoder representations. However, we recommend to use 60 as the maximum number of instances per encoding when getting started and to increase this limit if it proves insufficient.\nFor Dolby Vision encodings and conversions, starting from Encoder\nv2.208.0\n, the maximum number of instances have been increased to\n- 100 instances if the\nencodingMode\nconfigured is\nSTANDARD\n(which currently maps to\nTWO_PASS\n),\nSINGLE_PASS\nor\nTWO_PASS\n- 200 instances if the\nencodingMode\nconfigured is\nTHREE_PASS\nUsing the variables above, please request the following limits:\nProvider\nQuota Name\nLimit to request\nMicrosoft.Network\nPublic IP Addresses - Basic\n(maximum number of encodings) * (max # of instances per encoding)\nMicrosoft.Compute\nStandard DSv3 Family vCPUs\n(maximum number of encodings) * 8\nMicrosoft.Compute\nStandard FSv2 Family vCPUs\n(maximum number of encodings) * 8\nMicrosoft.Compute\nTotal Regional vCPUs\n(maximum number of encodings) * 8\nMicrosoft.Compute\nTotal Regional Spot vCPUs\n(maximum number of encodings) * (maximum number of instances per encoding) * 8\nMicrosoft.Compute\nVirtual Machines\n(maximum number of encodings) * (maximum number of instances per encoding) - The default value of 25000 should be sufficient\nMicrosoft.Compute\nVirtual Machine Scale Sets\nThe default value of 2500 should be sufficient\nMicrosoft.Compute\nStandard Storage Managed Disks\n(maximum number of encodings) * (maximum number of instances per encoding) - The default value of 50000 should be sufficient\nMicrosoft.Compute\nPremium Storage Managed Disks\n(maximum number of encodings) * (maximum number of instances per encoding) - The default value of 50000 should be sufficient\nThis implies the standard case: 8 core instances. If your use case requires instances with a different number of cores, multiply by that.\nThe maximum number of instances needed depends on the maximum number of parallel encodings running multiplied by the maximum number of instances needed for one encoding. The number of instances used by one encoding varies depending on the input file size and the number and data rate of the encoder Representations and cannot exceed 120.\nGenerally, it cannot hurt to multiply the expected limit calculated for your current situation by 2, to have some margin in case you need to ramp up.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/7643cd9-image.png",
      "https://files.readme.io/4797e82-image.png",
      "https://files.readme.io/c272e0e-image.png",
      "https://files.readme.io/b7a2cf6-image.png",
      "https://files.readme.io/4f0472b-image.png",
      "https://files.readme.io/a6179d5-image.png",
      "https://files.readme.io/7dc373d-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/the-bitmovin-dashboard",
    "title": "The Bitmovin Dashboard",
    "text": "Leverage\nBitmovin's Dashboard\nto\ncreate video streams\n,\nstart on-demand or live video encodings\n,\nset up & manage your video player\nand\nget real-time insights via our Video Analytics\n.\nAlso, you will learn how to configure the dashboard & settings to your needs in the following pages.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/51c116f6ee566ad1308bd0e67aaeba6441cadca7dd48e151a7b61183ab1dd40f-dashboard_home.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-dvr-best-practice",
    "title": "Live DVR (timeshift) configuration guide",
    "text": "What DVR functionality provides\nDVR typically means Digital Video Recorder and in the hardware world this would be a feature in digital set top boxes that recorded the content from the tuned channel service to a local cache such as a hard disk, allowing the user to go back in time and playback content from the past.\nToday some OTT platforms also want to offer the same experience to their users, allowing them to start a live event, and go back in time to points they may missed or wish to see again.\nThis is often described as a DVR \"window\", as the number of hours or minutes that the user can go back it time in will be offset from the current moment in time. A two hour DVR window will allow users watching at 10am UTC the option to go back and watch content from 8 - 10am, but an hour later than window will have moved to 9 - 11am.\nIn Bitmovin this feature can be enabled using the timeshift parameter.\nConfiguring timeshift in the manifest\nWhen configuring the manifest for a live encoding, set the timeshift parameter (in seconds) this is seen in the API POST request, under hlsManifest and dashManifest object arrages for:\nStart Live Encoding\n(using the Custom Live Configurations option)\nStart HD Options\n(using the Live Encoding HD options)\nAn example of how this would look in a script using Java is shown below, see the setTimeshift parameters.\nJava\nDashManifest dashManifest = createDashManifest(output, \"/\", encoding);\nHlsManifest hlsManifest = createHlsManifest(output, \"/\", encoding);\n\nLiveDashManifest liveDashManifest = new LiveDashManifest();\nliveDashManifest.setManifestId(dashManifest.getId());\nliveDashManifest.setTimeshift(300D);\nliveDashManifest.setLiveEdgeOffset(90D);\n\nLiveHlsManifest liveHlsManifest = new LiveHlsManifest();\nliveHlsManifest.setManifestId(hlsManifest.getId());\nliveHlsManifest.setTimeshift(300D);\nRecommended timeshift settings\nAs the timeshift parameter defines the number of segments that the manifest will reference, the larger the DVR window or timeshift value, the greater the number of elements that the manifest must reference each time it is written. Through testing we have found that the maximum number of elements the manifest should contain is\n54,000\n.\nIf the manifest contains more than 54,000 elements it is observed that devices will begin to struggle with playback and the service can not write to the manifest file fast enough. The result is an increasing latency in playback. Our recommendation for a default settings that will work for most configurations would be:\n👍\nBitmovin recommends a 3 hours DVR window (a timeshift value of\n10800\n)\nCalculating your maximum timeshift setting\nBecause the size of the DVR window depends on: a constant of the maximum number of elements in the manifest, the segment length and number of renditions that the encoding is configured with, different configurations will have a different value for the maximum timeshift value.\nTo calculate what the maximum value for your configuration should be, use the following formula.\nExamples\nThe table below provides some guidance using some example settings.\nVariants\nSegment length\ntimeshift value (seconds)\nDVR window (hours)\n3\n1\n18000\n5\n3\n1.28\n23040\n6.4\n3\n2.56\n46080\n12.8\n3\n3.84\n69120\n19.2\n3\n7.68\n138240\n38.4\n6\n1\n9000\n2.5\n6\n1.28\n11520\n3.2\n6\n2.56\n23040\n6.4\n6\n3.84\n34560\n9.6\n6\n7.68\n69120\n19.2",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/41b94ca-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/how-live-encoding-works",
    "title": "How Live Encoding works",
    "text": "The\nBitmovin Live Encoder\nis an award winning multi-cloud SaaS distribution encoder, and simplifies live streaming by structuring the workflow around three key elements: input, encoding, and output. Each element has a specific purpose, ensuring a smooth and efficient process.\nInput: Setting the Blueprint\nThe workflow begins with the input, which defines how live video is brought into the encoder. Acting as a blueprint, the input specifies transport protocols such as RTMP, SRT, or Zixi, ensuring compatibility with a wide range of production setups. It supports a variety of resolutions, from SD (480i) to 4K (2160p), as well as portrait orientations for mobile streams. By accepting industry-standard audio video codecs such as H.264, HEVC, AAC and OPUS, the input stage provides a reliable and flexible foundation for encoding.\nEncoding: The Core Process\nAt the heart of the workflow is encoding. This is where the Bitmovin Live Encoder processes the incoming video, audio and metadata, transforming it into high-quality adaptive streams suitable for delivery. Using adaptive bitrate (ABR) streaming, the encoder generates outputs in formats like HLS and DASH, ensuring smooth playback across devices and varying network conditions. Advanced features, such as video scaling, image overlay insertion, deinterlacing, loudness correction, subtitle conversion and DRM encryption offer users extensive customization options to meet specific technical and creative requirements.\nOutput: Delivering the Stream\nThe output defines where the encoded video is stored or forwarded after processing. The Bitmovin Live Encoder supports integration with a variety of storage solutions, including Akamai NetStorage, Akamai Object Storage, Azure Blob Storage, Google Cloud Storage (GCS), and AWS S3. These storage options ensure reliable and scalable distribution, making the content available for further delivery via Content Delivery Networks (CDNs) or playback platforms. Configurations are optimized to maintain high performance and compatibility across diverse streaming environments.\nSetting Up Your Live Encoding Workflow\nSetting up your live streaming workflow with the Bitmovin Live Encoder is both flexible and user-friendly. The platform offers three distinct methods to configure inputs, encoding settings, and outputs:\nAPI:\nFor developers seeking full control and automation, Bitmovin’s powerful API allows for detailed customization of every aspect of the workflow, from input specifications to encoding profiles and output destinations.\nTemplates:\nCustomisable templates simplify the process of configuring the live encoder with the full range of API features, enabling users to quickly replicate consistent setups for recurring live events, saving time and ensuring uniformity.\nDashboard UI\n: The\nBitmovin Dashboard\nprovides an intuitive user interface that guides users through the configuration process with a step-by-step wizard, making it ideal for those new to live streaming or looking for a straightforward setup option. Additionally, the Dashboard allows users to independently create and manage inputs and outputs, offering the flexibility to configure specific parts of the workflow as needed.\nThese flexible configuration methods empower you to tailor every part of your workflow (input, encoding, and output) to suit your specific needs, ensuring reliable and high-quality live streams with minimal effort.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/understanding-hls-aes-encryption",
    "title": "Understanding HLS AES Encryption",
    "text": "Apple HTTP Live Streaming Encryption\nHLS DRM does not always need to be a Hollywood grade DRM. Sometimes it’s enough to just add another layer of security through AES encryption. The Bitmovin encoding service and player enable this workflow for HLS and MPEG-DASH.\nHLS AES Encryption\nApple HLS supports two encryption methods:\nAES-128\nIt encrypts the whole segment with the Advanced Encryption Standard (AES) using a 128 bit key, Cipher Block Chaining (CBC) and PKCS7 padding.The CBC will be restarted with each segment using the Initialization Vector (IV) provided.\nSAMPLE-AES\nIt encrypts each individual media sample (e.g., video, audio, etc.) by its own with AES. The specific encryption and packaging depends on the media format, e.g., H.264, AAC, etc. SAMPLE-AES allows fine grained encryption modes, e.g., just encrypt I frames, just encrypt 1 out of 10 samples, etc. This could decrease the complexity of the decryption process. Several advantages result out of this approach as fewer CPU cycles are needed and for example mobile devices need less power consumption, higher resolutions can be effectively decrypted, etc.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/akamai-msl4-live-outputs",
    "title": "Akamai MSL4 Live Outputs",
    "text": "Overview\nBitmovin can not only run in Akamai Connected Cloud, deploying the Live Encoder to a range of regions, but it can also output to Akamai\nMedia Services Live\n(MSL) as an ingest point for distribution over the Akamai CDN.\nConfiguring Akamai MSL4\nYou will need to have an account with Akamai and be able to access the Akamai Control Center.\nPrerequisites\nWhen creating a new stream in MSL4 you'll want to consider what type of streaming formats and containers you wish to distribute to your users via the CDN.\nAkamai MSL4 can support a range of\ninput formats\nhowever they are not all compatible with Bitmovin, and where they are may require a particular muxing type.\nStream Format\nSupported by Bitmovin\nMuxing Required\nHLS\nfor Apple HTTP Live Streaming\n✅\nTS\nHDS\nfor Adobe HTTP Dynamic Streaming\n❌\n-\nDASH\nfor Dynamic Adaptive Streaming over HTTP\n✅\nfmp4\nCMAF\nfor Common Media Application Format*\n✅\nfmp4\n*\nCMAF\nMSL4 streams can receive HLS and DASH outputs from Bitmovin and must use fmp4 muxing.\nCreate you MSL Stream in Akamai\nYou will need to use an existing MSL4 stream or create a new one. Akamai provides instructions on how to do this in their\nMSL4 documentation\n.\nCollect the Stream ID\nOnce created take note of the\nStream ID\nwhich is required when configuring the live output in Bitmovin. This can be found in the Stream Name (Stream ID) column, the stream ID of your stream is the number in parenthesis, e.g. 9021090.\nConfiguring Bitmovin Live Encoding Outputs\nUsing the Bitmovin Dashboard\nIn the Dashboard navigate to Live Encoding in the left side panel, and then select Outputs.\n📘\nRemember you will need to know the Stream ID and Stream Format for the MSL4 stream, from your Akamai Control Center or Administrator.\nPress\n+ Create\nand select Akamai MSL4 to see the correct form.\nProvide a\nName\nthis is a free form name that will be used as a label in the Bitmovin Dashboard, and a useful description to help other Bitmovin users and administrators.\nThe\nStream ID\nthat you have taken from your MSL4 stream in the Akamai Control Center\nSet the\nStream Format\nthat correlates with the MSL4 settings.\nUsers can choose an arbitrary name for your\nEvent Name\n, the requirements are that it should be\none word\nwith letters and numbers but\nwithout special characters\n. E.g. \"bitmovin123\".\nWhen you are finished press\nCreate\nand the output will be saved.\nUsing the Bitmovin API\nWe provide the following API endpoints to configure MSL4 Live Outputs\nCreate Akamai MSL Output\nto create new outputs.\nList Akamai MSL Outputs\nto get a list of configured outputs including their\noutput_id\n(UUID returned as\nid\n)\nAkamai MSL Output Details\nto get information about a specific MSL4 Live Output using the\noutput_id\nDelete Akamai MSL Output\nto delete specific MSL4 Live Outputs using the\noutput_id\nUsing the Live Output\nThe output will appear in the Outputs list, and in the Wizard under Akamai MSL.\nYou can confirm the output is created in the API by using\nList Akamai MSL Outputs\nRemember to select the correct muxing format, that is compatible with your MSL4 stream format.\nManifest and Segment Names\nBe aware of the following naming conditions:\nThe template for the segments must not contain underscores (\n). E.g, if your template is segment\n%number%.m4s , change it to segment-%number%.m4s\nThe output path for the manifest, if not empty, must contain leading and trailing slashes (/).\nThe segment name must be in the format '[prefix]-{rand\nchars[:number]}-%number%.[extension]'\n_e.g. : segment-{rand_chars:10}-%number%.m4s\n📘\nThis naming pattern will be set automatically when using the Live Dashboard but must be set correctly if using the API.\nPreviewing Outputs\nIt is not possible to automatically preview the outputs in the Live Encodings status in the Bitmovin Dashboard. However, it is possible to use the\nTest your stream\npage under the Player menu in the Dashboard to view the output and validate the stream if the CDN path is known.\nTo construct the preview URL you must retrieve your\namd-property-hostname\nwhich is the published and playable DNS name for your CDN endpoint. This can be found in the Akamai Control Center, as explain in the\nAkamai documentation\n.\nOnce this is known you can apply it to the following naming pattern:\nhttps://\n<amd-property-hostname>\n/\n<stream-format>\n/live/\n<stream-id>\n/\n<event-name>\n/\n<manifest>\nExample MSL4 Manifest URLs\nIf you are using the Dashboard wizard to create the outputs, default manifest names would be applied. If using the API the manifest name would need to be set and may differ.\nBelow there are examples with the default manifest names for different formats, where:\nstream-id = 9021090\nevent-name = bitmovin123\namd-property-hostname = streamtime.bitmovin.net\nMSL4 Stream Format\nBitmovin Output\nBitmovin Preview\nHLS\nHLS\nhttp://streamtime.bitmovin.net/hls/live/9021090/bitmovin123/manifest.m3u8\nDASH\nDASH\nhttp://streamtime.bitmovin.net/dash/live/9021090/bitmovin123/manifest.mpd\nCMAF\nHLS\nhttp://streamtime.bitmovin.net/cmaf/live/9021090/bitmovin123/manifest.m3u8\nCMAF\nDASH\nhttp://streamtime.bitmovin.net/cmaf/live/9021090/bitmovin123/manifest.mpd",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/2691623-Screenshot_2024-04-10_at_11.02.24.png",
      "https://files.readme.io/beec332-Screenshot_2024-04-05_at_18.41.26.png",
      "https://files.readme.io/650f7fd-Screenshot_2024-04-06_at_13.20.02.png",
      "https://files.readme.io/d0afa26-Screenshot_2024-04-06_at_13.29.45.png",
      "https://files.readme.io/49b55c7-Screenshot_2024-04-06_at_13.27.20.png",
      "https://files.readme.io/0810645-Screenshot_2024-04-06_at_13.57.04.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-encoding-with-yospace",
    "title": "Live Encoding with YoSpace",
    "text": "YoSpace\nprovides Ad insertion services, and supports SSAI. To work with Bitmovin Live Encoding the following HlsManifestAdMarkerType should be enabled:\nEXT_X_CUE_OUT_IN\n. It also works with\nEXT_X_SCTE35\nand\nEXT_X_DATERANGE\n.\nPrerequisites\nYou will require your own YoSpace account.\nEnsure you're familiar with Live Encoding settings match the requirements in\nLive Encoding with HLS, SCTE-35 and SSAI\nHow to Setup\nStart a Bitmovin Live Encoder\ne.g.: use the full Java API code example from\nLive Encoding with HLS, SCTE-35 and SSAI\nuse the Dashboard Wizard - in step 5, selecting HLS with TS muxing and selecting the compatible tags\nStart your Live ingest stream\ne.g.: Use the example method from the “Sample ingest with a demo file” from\nLive Encoding with HLS, SCTE-35 and SSAI\nIn your YoSpace account Setup a Promotion Group\nClick on the Ad Operations tab and select in column Promotion Groups Manage Promotion Groups\nClick on the left on Create Promotion Group\nGive any meaningful name\nState: Active\nClick Save\nSetup a Usage Profile\nAsk Yospace support to create one\nSetup a Ad content Group with an Ad content\nClick on the Content tab and select in column Ad Content Manage Ad Content Groups\nClick on the left on Create Ad Content Group\nSet any meaningful name\nClick Save\nClick on the left on Upload Ad Content\nUsage Profile: If available, select usage profile\nPut Into: Select previously created Ad Content Group\nEither take a File from your PC, or configure a remote URL\nGive it any name\nUncollapse Content Metadata\nTitle: any\nPublish Date: today\nClick save\nSetup a Dynamic Promotion\nClick on the Ad Operations tab and select in column Promotions Create Dynamic Promotion\nGive it any meaningful name\nState: Active\nPromotion Group: previously created Promotion Group\nOn-the-fly Ingestion: ✅\nApply To: Broadcast Streaming/VOD Context\nAd Tag Template: your VAST/ad decision server URL\ne.g.\nhttps://bitmovin-api-eu-west1-ci-input.s3.eu-west-1.amazonaws.com/scte35/tutorial/vast3_10s_ad_example.xml\nIngest Ads into: previously created Ad Content Group\nIngest Usage Profile: If available, select usage profile\nAd Creative Unique Key: Use Rentition URL with ([^?]+)\nUncollapse Live DAI Placement Settings\nAd Playlists: ✅ Search for multiple ads in response\nTarget Duration: ✅ Retry ad request if target ad duration not reached\nLive Underrun Policy: Expose Broadcast\nLive Truncation Policy: Ad Boundary\nUncollapse Advanced Settings\nServer-side Ad Tracking: ✅ Hit Impression URLs and ✅ Hit Quartile URLs\nVAST Wrappers: ✅ Allow multiple ads in wrapper response by default\nClick Save\nSetup a live stream\nClick on the Content tab and select in column Live Streaming Manage Streams\nClick on the left on Create broadcast stream\nOptional: Feel overwhelmed by the sheer amount of options\nfill out the following fields:\nName: Any\nExternal ID: Any, but not your name and no spaces\nPromotion: previously created promotion\nManifest Polling: HLS\nCheckmarks on Manifest Polling Enabled ✅ and Master Playlist Alignment ✅\nAd Break Prefetch: 100\nUncollapse HLS manifest sources\nset a number as an ID\nSource media playlist URL: link to your video playlist (⚠️ NOT the master manifest!)\ne.g.\nhttps://storage.googleapis.com/bitmovin-api-europe-west1-ci-output/encoding_test/liveEncoderWithScte35AutoReturn/video_0.m3u8\nOrigin Master Playlist Level URI: That is the part of the media playlist only.\nSo in the above example it is video_0.m3u8\nUncollapse Output Stream configuration\ngive it any name and external ID. Again it might help to not select the same name and id.\nPromotion: inherited\nReplacement Policy Server URL: your VAST/ad decision server URL.\ne.g.\nhttps://bitmovin-api-eu-west1-ci-input.s3.eu-west-1.amazonaws.com/scte35/tutorial/vast3_10s_ad_example.xml\nAccess Control: Unrestricted\nAd Break Prefetch: 100\nUncollapse Advanced\nAssigned to HARRO: Select any option here\nUncollapse Ingest Mapping\nConfigure Ad Avail Start tag properly for your ad tags. E.g. #EXT-X-CUE-OUT:(?\n(?:\\d+(?:.\\d+)?|unset))\nConfigure Ad Avail End tag properly for your ad tags. E.g. #EXT-X-CUE-IN\nUncollapse HLS Manifest Polling\nPoll from: Select same option as for HARRO here\nPoll frequency: 3000\nPush frequency: 3000\nAlert Media Sequence Stuck: 30000\nKey File Path Split: URI=\"\nMonitoring Post Sample Size: 300\nCheckbox at Enable Master Manifest Polling\nPoll Frequency: 5000\nSource Master Playlist URL: link to your master manifest.\ne.g.\nhttps://storage.googleapis.com/bitmovin-api-europe-west1-ci-output/encoding_test/liveEncoderWithScte35AutoReturn/stream.m3u8\nClick save\nIf you’ve done it correctly, you should see a green light\nGet an Output\nOutput URL generation rules can be found here:\nhttps://cds1.yospace.com/cmui/web/documentation/Main/en/r2-42-platform-documentation/live-streaming/available-live-streaming-modes.html#access-to-live-streaming-modes",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/599d10d-YoSpace_SSAI.png",
      "https://files.readme.io/e99ba89-YoSpace_UI_Config_for_SCTE.png",
      "https://files.readme.io/9125c31-getactivestreams.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/java-sdk",
    "title": "Java SDK",
    "text": "The Bitmovin API enables you to build cutting edge video workflows while leveraging emmy-award winning encoding solutions, that automatically optimize your catalog for high quality and cost efficient video streaming.\nQuick Start Guide\nThis Quick Start Video will demonstrate different approaches to setup an encoding workflow using a\nWatch Folder Workflow (No Code Required)\nand a\nfull Python API SDK example\nusing this\nColabortory Notebook\n.\nundefined\nGet Started with your own Project\nThe following steps will walk you through adding the SDK to an new or existing Project step by step.\nOverview\nStarting an encoding triggers a request for encoding instances. Once available, the input file will be downloaded, divided into chunks, and distributed between all of them. Those instances transcode each chunk into segments for all configured video/audio representations (muxings) in parallel and write them to the designated output destination.\nThis approach makes Bitmovin's encoding the fastest in the world.\nIn this tutorial you will implement a simple VOD online streaming scenario, where an input file containing a video stream and stereo audio stream is transcoded into ...\na fixed ladder of 3 video representations at different resolutions and bitrates using the H264 codec, and\nan audio rendition with the AAC codec.\nEach individual stream is wrapped into fragmented MP4 (fMP4) containers. HLS and DASH manifests are generated so the encoded content is stream- and playable on the majority of modern devices and browsers out there today.\nStep 1: Add Bitmovin SDK to Your Project\nTo get started add the Bitmovin SDK to your project.\nFor the latest version please refer to the\nGitHub repository\nor check the\nchangelog\nof the Bitmovin API. Open API SDK versions and the Bitmovin API share the same versioning.\nXML\n<dependencies>\n  <dependency>\n    <groupId>com.bitmovin.api.sdk</groupId>\n    <artifactId>bitmovin-api-sdk</artifactId>\n    <version>1.95.0</version>\n    <scope>compile</scope>\n  </dependency>\n</dependencies>\nStep 2: Setup a Bitmovin API Client Instance\nThe Bitmovin API client instance is used for all communication with the Bitmovin REST API, pass your API key to authenticate with the API. You can find your\nAPI_KEY\nin the dashboard in your\nAccount Settings\n.\nJava\nBitmovinApi bitmovinApi = BitmovinApi.builder()\n        .withApiKey(\"<API_KEY>\")\n        .withLogger(new Slf4jLogger(), Logger.Level.BASIC) // set the logger and log level for the API client)\n        .build();\nStep 3: Create an Input\nCreate Input\nAn input holds the configuration to access a specific file storage, from which the encoder will download the source file.\nWe support various types of input sources including HTTP(S), (S)FTP, Google Cloud Storage (GCS), Amazon Simple Storage Service (S3), Azure Blob Storage, and all cloud storage vendors that offer an S3 compatible interface.\nBelow we are creating an input. You can change the input type on the top right of the code panel.\nJava\nHttpsInput input = new HttpsInput();\ninput.setHost(\"<HTTPS_INPUT_HOST>\");\n\ninput = bitmovinApi.encoding.inputs.https.create(input);\nReuse Inputs\nInputs refer to a location, not a specific file.\nThis makes them easily reusable. Create an input once and reuse it for all your encodings.\nHint: All resources in our API are identified by a unique id which enables you to reuse many resources. So you only have to create one input for each location and can reuse it for all encodings.\nJava\nHttpsInput input = bitmovinApi.encoding.inputs.https.get(\n  \"<INPUT_ID>\"\n);\nStep 4: Create an Output\nCreate Output\nOutputs define where the manifests and encoded segments will be written to.\nLike inputs, outputs represent a set of information needed to access a specific storage. From this storage players can access the content for playback.\nJava\nGcsOutput output = new GcsOutput();\noutput.setAccessKey(\"<GCS_ACCESS_KEY>\");\noutput.setSecretKey(\"<GCS_SECRET_KEY>\");\noutput.setBucketName(\"<GCS_BUCKET_NAME>\");\n\noutput = bitmovinApi.encoding.outputs.gcs.create(output);\nString outputId = output.getId();\nReuse Output\nSimilar to inputs, outputs also refer to a location, not a specific file.\nSo they're exactly as reusable as inputs. Create it once and reuse it for all your encodings.\nJava\nGcsOutput output = bitmovinApi.encoding.outputs.gcs.get(\n  \"<OUTPUT_ID>\"\n);\nStep 5: Create Codec Configurations\nCodec Configurations\nCodec configurations specify the codec and its parameters (eg. media codec, bitrate, frame rate, sampling rate).\nThey are used to encode the input stream coming from your input file.\nVideo Codec Configurations\nBelow we are creating video codec configurations. We support a multitude of codecs, including H264, H265, VP9, AV1 and many more.\nFor simplicity, we use H264 now, but you can easily change that after finishing the getting started.\nJava\nH264VideoConfiguration videoCodecConfiguration1 = new H264VideoConfiguration();\nvideoCodecConfiguration1.setName(\"Getting Started H264 Codec Config 1\");\nvideoCodecConfiguration1.setBitrate(1500000L);\nvideoCodecConfiguration1.setWidth(1024);\nvideoCodecConfiguration1.setPresetConfiguration(PresetConfiguration.VOD_STANDARD);\n\nvideoCodecConfiguration1 = bitmovinApi.encoding.configurations.video.h264.create(videoCodecConfiguration1);\n\nH264VideoConfiguration videoCodecConfiguration2 = new H264VideoConfiguration();\nvideoCodecConfiguration2.setName(\"Getting Started H264 Codec Config 2\");\nvideoCodecConfiguration2.setBitrate(1000000L);\nvideoCodecConfiguration2.setWidth(768);\nvideoCodecConfiguration2.setPresetConfiguration(PresetConfiguration.VOD_STANDARD);\n\nvideoCodecConfiguration2 = bitmovinApi.encoding.configurations.video.h264.create(videoCodecConfiguration2);\n\nH264VideoConfiguration videoCodecConfiguration3 = new H264VideoConfiguration();\nvideoCodecConfiguration3.setName(\"Getting Started H264 Codec Config 3\");\nvideoCodecConfiguration3.setBitrate(750000L);\nvideoCodecConfiguration3.setWidth(640);\nvideoCodecConfiguration3.setPresetConfiguration(PresetConfiguration.VOD_STANDARD);\n\nvideoCodecConfiguration3 = bitmovinApi.encoding.configurations.video.h264.create(videoCodecConfiguration3);\nAudio Codec Configurations\nAs audio codec configuration we are creating one AAC configuration.\nWe also support many more audio codecs including Opus, Vorbis, AC3, E-AC3 and MP2.\nJava\nAacAudioConfiguration audioCodecConfiguration = new AacAudioConfiguration();\naudioCodecConfiguration.setName(\"Getting Started Audio Codec Config\");\naudioCodecConfiguration.setBitrate(128000L);\n\naudioCodecConfiguration = bitmovinApi.encoding.configurations.audio.aac.create(audioCodecConfiguration);\nReusing Codec Configurations\nAs inputs and outputs, codec configurations can also be reused.\nTo use existing video or audio codec configurations, loading them with their id.\nJava\nH264VideoConfiguration videoCodecConfiguration = bitmovinApi.encoding.configurations.video.h264.get(\n  \"<H264_CC_ID>\"\n);\n\nAacAudioConfiguration audioCodecConfiguration = bitmovinApi.encoding.configurations.audio.aac.get(\n  \"<AAC_CC_ID>\"\n);\nStep 6: Create & Configure an Encoding\nCreate Encoding\nAn encoding is a collection of resources (inputs, outputs, codec configurations etc.), mapped to each other.\nThe cloud region defines in which cloud and region your encoding will be started.\nIf you don't set it, our encoder takes the region closest to your input and output.\nInputs, outputs and codec configurations can be reused among many encodings. Other resources are specific to one encoding, like streams, which we'll create below.\nJava\nEncoding encoding = new Encoding();\nencoding.setName(\"Getting Started Encoding\");\nencoding.setCloudRegion(CloudRegion.GOOGLE_EUROPE_WEST_1);\n\nencoding = bitmovinApi.encoding.encodings.create(encoding);\nStreams\nA stream maps an audio or video input stream of your input file (called input stream) to one audio or video codec configuration.\nThe following example uses the selection mode\nAUTO\n, which will select the first available video input stream of your input file.\nYou can choose an input file from 3 predefined examples on the top right of the code panel.\nVideo Stream\nJava\nString inputPath = \"<INPUT_PATH>\";\n\nStreamInput videoStreamInput = new StreamInput();\nvideoStreamInput.setInputId(input.getId());\nvideoStreamInput.setInputPath(inputPath);\nvideoStreamInput.setSelectionMode(StreamSelectionMode.AUTO);\n\nStream videoStream1 = new Stream();\nvideoStream1.setCodecConfigId(videoCodecConfiguration1.getId());\nvideoStream1.addInputStreamsItem(videoStreamInput);\nvideoStream1 = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), videoStream1);\n\nStream videoStream2 = new Stream();\nvideoStream2.setCodecConfigId(videoCodecConfiguration2.getId());\nvideoStream2.addInputStreamsItem(videoStreamInput);\nvideoStream2 = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), videoStream2);\n\nStream videoStream3 = new Stream();\nvideoStream3.setCodecConfigId(videoCodecConfiguration3.getId());\nvideoStream3.addInputStreamsItem(videoStreamInput);\nvideoStream3 = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), videoStream3);\nAudio Stream\nJava\nStream audioStream = new Stream();\n\nStreamInput audioStreamInput = new StreamInput();\naudioStreamInput.setInputId(input.getId());\naudioStreamInput.setInputPath(inputPath);\naudioStreamInput.setSelectionMode(StreamSelectionMode.AUTO);\n\naudioStream.setCodecConfigId(audioCodecConfiguration.getId());\naudioStream.addInputStreamsItem(audioStreamInput);\n\naudioStream = bitmovinApi.encoding.encodings.streams.create(encoding.getId(), audioStream);\nStep 7: Create a Muxing\nMuxings\nA muxing defines which container format will be used for the encoded video or audio files (segmented TS, progressive TS, MP4, FMP4, ...). It requires a stream, an output, and the output path, where the generated segments will be written to.\nFMP4 muxings are used for DASH manifest representations, TS muxings for HLS playlist representations.\nFMP4\nJava\nAclEntry aclEntry = new AclEntry();\naclEntry.setPermission(AclPermission.PUBLIC_READ);\nList<AclEntry> aclEntries = new ArrayList<AclEntry>();\naclEntries.add(aclEntry);\n\ndouble segmentLength = 4D;\nString outputPath = \"<OUTPUT_PATH>\";\nString segmentNaming = \"seg_%number%.m4s\";\nString initSegmentName = \"init.mp4\";\n\nFmp4Muxing videoMuxing1 = new Fmp4Muxing();\n\nMuxingStream muxingStream1 = new MuxingStream();\nmuxingStream1.setStreamId(videoStream1.getId());\n\nEncodingOutput videoMuxingOutput1 = new EncodingOutput();\nvideoMuxingOutput1.setOutputId(outputId);\nvideoMuxingOutput1.setOutputPath(String.format(\"%s%s\", outputPath, \"/video/1024_1500000/fmp4/\"));\nvideoMuxingOutput1.setAcl(aclEntries);\n\nvideoMuxing1.setSegmentLength(segmentLength);\nvideoMuxing1.setSegmentNaming(segmentNaming);\nvideoMuxing1.setInitSegmentName(initSegmentName);\nvideoMuxing1.addStreamsItem(muxingStream1);\nvideoMuxing1.addOutputsItem(videoMuxingOutput1);\n\nvideoMuxing1 = bitmovinApi.encoding.encodings.muxings.fmp4.create(encoding.getId(), videoMuxing1);\n\nFmp4Muxing videoMuxing2 = new Fmp4Muxing();\n\nMuxingStream muxingStream2 = new MuxingStream();\nmuxingStream2.setStreamId(videoStream2.getId());\n\nEncodingOutput videoMuxingOutput2 = new EncodingOutput();\nvideoMuxingOutput2.setOutputId(outputId);\nvideoMuxingOutput2.setOutputPath(String.format(\"%s%s\", outputPath, \"/video/768_1000000/fmp4/\"));\nvideoMuxingOutput2.setAcl(aclEntries);\n\nvideoMuxing2.setSegmentLength(segmentLength);\nvideoMuxing2.setSegmentNaming(segmentNaming);\nvideoMuxing2.setInitSegmentName(initSegmentName);\nvideoMuxing2.addStreamsItem(muxingStream2);\nvideoMuxing2.addOutputsItem(videoMuxingOutput2);\n\nvideoMuxing2 = bitmovinApi.encoding.encodings.muxings.fmp4.create(encoding.getId(), videoMuxing2);\n\nFmp4Muxing videoMuxing3 = new Fmp4Muxing();\n\nMuxingStream muxingStream3 = new MuxingStream();\nmuxingStream3.setStreamId(videoStream3.getId());\n\nEncodingOutput videoMuxingOutput3 = new EncodingOutput();\nvideoMuxingOutput3.setOutputId(outputId);\nvideoMuxingOutput3.setOutputPath(String.format(\"%s%s\", outputPath, \"/video/640_750000/fmp4/\"));\nvideoMuxingOutput3.setAcl(aclEntries);\n\nvideoMuxing3.setSegmentLength(segmentLength);\nvideoMuxing3.setSegmentNaming(segmentNaming);\nvideoMuxing3.setInitSegmentName(initSegmentName);\nvideoMuxing3.addStreamsItem(muxingStream3);\nvideoMuxing3.addOutputsItem(videoMuxingOutput3);\n\nvideoMuxing3 = bitmovinApi.encoding.encodings.muxings.fmp4.create(encoding.getId(), videoMuxing3);\nAudio Muxings\nJava\nFmp4Muxing fmp4AudioMuxing = new Fmp4Muxing();\n\nMuxingStream fmp4AudioMuxingStream = new MuxingStream();\nfmp4AudioMuxingStream.setStreamId(audioStream.getId());\n\nEncodingOutput fmp4AudioEncodingOutput = new EncodingOutput();\nfmp4AudioEncodingOutput.setOutputId(outputId);\nfmp4AudioEncodingOutput.setOutputPath(String.format(\"%s%s\", outputPath, \"/audio/128000/fmp4/\"));\nfmp4AudioEncodingOutput.setAcl(aclEntries);\n\nfmp4AudioMuxing.setSegmentLength(segmentLength);\nfmp4AudioMuxing.setSegmentNaming(segmentNaming);\nfmp4AudioMuxing.setInitSegmentName(initSegmentName);\nfmp4AudioMuxing.addStreamsItem(fmp4AudioMuxingStream);\nfmp4AudioMuxing.addOutputsItem(fmp4AudioEncodingOutput);\n\nfmp4AudioMuxing = bitmovinApi.encoding.encodings.muxings.fmp4.create(encoding.getId(), fmp4AudioMuxing);\nHint: To optimize your content for a specific use-case you can also provide a custom segment length. Further, you can also customize the naming pattern for the resulting segments and the initialization segment as well.\nStep 8: Create a Manifest\nDASH & HLS Manifests\nIts recommended to use Default Manifests, one each for DASH (\nAPI-Reference\n) and HLS (\nAPI-Reference\n). They are a construct that leaves it to the encoder to determine how to best generate the manifest based on what resources have been created by the encoding. This simplifies the code greatly for simple scenarios like this guide is about.\nCreate a DASH Manifest\nJava\nfinal var manifestOutput = new EncodingOutput();\nmanifestOutput.setOutputId(outputId);\nmanifestOutput.setOutputPath(outputPath);\n\nfinal var aclEntryManifest = new AclEntry();\naclEntryManifest.setPermission(AclPermission.PUBLIC_READ);\naclEntryManifest.setScope(\"*\");\nmanifestOutput.setAcl(List.of(aclEntryManifest));\n\nvar dashManifest = new DashManifestDefault();\ndashManifest.setManifestName(\"stream.mpd\");\ndashManifest.setEncodingId(encoding.getId());\ndashManifest.setVersion(DashManifestDefaultVersion.V2);\ndashManifest.setOutputs(List.of(manifestOutput));\n\ndashManifest = bitmovinApi.encoding.manifests.dash.defaultapi.create(dashManifest);\nCreate a HLS manifest\nJava\nvar hlsManifest = new HlsManifestDefault();\nhlsManifest.setManifestName(\"stream.m3u8\");\nhlsManifest.setEncodingId(encoding.getId());\nhlsManifest.setVersion(HlsManifestDefaultVersion.V1);\nhlsManifest.setOutputs(List.of(manifestOutput));\n\nhlsManifest = bitmovinApi.encoding.manifests.hls.defaultapi.create(hlsManifest);\nStep 9: Start an Encoding\nLet's recap - We have defined:\nan\ninput\n, specifying the input file host\nan\noutput\n, specifying where the manifest and the encoded files will be written to\ncodec configurations\n, specifying how the input file will be encoded\nstreams\n, specifying which input stream will be encoded using what codec configuration\nmuxings\n, specifying the containerization of the streams\nmanifests\n, to play the encoded content using the HLS or DASH streaming format.\nNow we can start the encoding with one API call, including additional configuration so the DASH and HLS Default-Manifests are created as part of the encoding process too:\nJava\nfinal var startEncodingRequest = new StartEncodingRequest();\n\nfinal var dashManifestResource = new ManifestResource();\ndashManifestResource.setManifestId(dashManifest.getId());\nstartEncodingRequest.setVodDashManifests(List.of(dashManifestResource));\n\nfinal var hlsManifestResource = new ManifestResource();\nhlsManifestResource.setManifestId(hlsManifest.getId());\nstartEncodingRequest.setVodHlsManifests(List.of(hlsManifestResource));\n\nstartEncodingRequest.setManifestGenerator(ManifestGenerator.V2);\nbitmovinApi.encoding.encodings.start(encoding.getId(), startEncodingRequest);\nWhat happens next:\nAfter the successful start of the encoding, it will change its state from\nCREATED\nto\nSTARTED\n, and shortly after that to\nQUEUED\n.\nRUNNING\nit will be in as soon as the input file starts to get downloaded and processed. Eventually\nFINISHED\nwill indicate a successful encoding and upload of the content to the provided Output destination. More details about each encoding status can be\nfound here\n.\nStep 10: Final Review\nCongratulations!\nYou successfully started your first encoding :) and this is only the beginning. The Bitmovin Encoding API is extremely flexible and allows many customizations and provides with access to the latest codecs and video streaming optimizations and technologies out there. Have a look at the\nAPI Reference\nto learn more.\nMore Examples\nYou can find many more fully functional code examples in our Github Repository:\nGithub",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/ecfc7d1-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoder-2500-2990",
    "title": "Encoder 2.50.0 - 2.99.0",
    "text": "2.99.0\nReleased 2021-11-16\nFixed\nEnhanced Deinterlacer\npotentially created distorted output for non-standard input resolutions.\nWriting default manifests to Akamai Netstorage had failed\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.98.0\nReleased 2021-11-03\nAdded\nHDR10 to SDR and HLG to SDR conversions are now possible using the\ndynamicRangeFormat\nproperty on the H265 codec configuration\n.\nSupport for adding an\nAAC\nor\nDolby Atmos\naudio stream along with a Dolby Vision video stream to a\nMP4 muxing\n.\nChanged\nImproved the intra-segment rate-control mechanisms for VP9 encodes\nFixed\nPotential truncation of last CUE in WebVTT outputs was fixed\nEncodings using Widevine DRM have correct pssh content now\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.97.0\nReleased 2021-10-19\nAdded\nSupport\nDolbyVisionInputStream\ns together with other input stream types like\nIngestInputStream\n,\nAudioMixInputStream\n, etc.\nFixed\nFixed that corrupt packets at the beginning of an input-stream could lead to an encoding error.\nFixed that input concatenation in combination with different input frame-rates could cause an encoding to fail if the stitching point was close to the segment end.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nfMP4 and WebM muxings with Widevine or CENC Widevine DRM have the wrong PSSH set in output segments\n2.96.0\nReleased 2021-10-12\nAdded\nEncodings for audio and single-file subtitles without videostreams are supported now. The subtitle outputs are not automatically trimmed to the audio duration in this case.\nFixed\nThe longevity of live encodings has been improved.\nA fail-fast condition that was triggered for pixel format conversion YUV to YUV was fixed. This fail-fast now correctly triggers only on YUV to YUV conversion with additional Color Range conversion.\nKeyFrameConfig is now applied correctly for encodings using Dolby Digital (Plus) audio codecs\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nfMP4 and WebM muxings with Widevine or CENC Widevine DRM have the wrong PSSH set in output segments\n2.95.0\nReleased 2021-10-04\nAdded\nWith this release we've improved the encoding capabilities with regards to HDR conversions for H265 output. From now on we support\nDolby Vision to HDR10\nDolby Vision to SDR\nHDR10 to HLG\nHLG to HDR10\nSDR to HDR10\nSDR to HLG\nThe encoder detects the applicable conversion based on the input and the configured output color settings. To make it easier to correctly configure the output we have added preset configurations for the different dynamic range formats. These can be configured on the\nH265\nresource via the setting\ndynamicRangeFormat\n.\nDolby Vision input can be configured with a\nDolby Vision input stream\nFixed\nFixed a bug for H264 and H265 where the specified chroma location was not signaled correctly in the output bitstream.\nFixed a bug with CustomTags inside HLS audio manifests to be inserted at given KeyFrame positions when there is audio and video encoding. The tags were inserted occasionally a few segments before.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.94.0\nReleased 2021-09-28\nAdded\nSupport for setting\nduration=null\nfor\nTime-based Trimming\nto indicate that the whole stream should be used (considering offset).\noffset=null\nwill be defaulted to zero now. We also fixed and issue when setting\nduration=0\nwhich lead to different results for video and audio. Setting\nduration=0\nwill now lead to 0 frames for video and audio.\nFixed\nInputs with J2K or ProRes codec could have lead to oversaturated output when configured with bt709 and 1080p output.\nFixed an issue that affected some MOV inputs with an edts (EditListBox) atom. The duration of the edit list entry was not considered and therefore also the media data, that should have been discarded, was present in the output.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.93.0\nReleased 2021-09-21\nFixed\nThe\nMP3 muxing information endpoint\nnow returns the same information like the\nMP4 muxing information endpoint\n.\nThe muxing details endpoints, like\nMP4 muxing details\n, now defaults\nminBitrate\nto\n0\ninstead of\n9223372036854775807\n.\nImproved longevity of live-encodings with encryption enabled for audio using fmp4 output and aac codec has been improved.\nImproved the\nAuto-Shutdown of Live Encodings\nto detect lost input faster when using a higher\nautoShutdownConfiguration.bytesReadTimeoutSeconds\nand trigger a shutdown faster.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.92.0\nReleased 2021-09-14\nAdded\nAutomatic shutdown of a live encoding if input is lost and does not reconnect within a certain time period, configurable via the\nautoShutdownConfiguration.bytesReadTimeoutSeconds\nconfiguration when starting a\nLive Encoding\n.\nAutomatic shutdown of a live encoding after a certain period of time, configurable via the\nautoShutdownConfiguration.streamTimeoutMinutes\nconfiguration when starting a\nLive Encoding\n.\nFixed\nFixed a bug that led to stalling of Dolby Vision encodings with 50 fps inputs.\nFixed failed encodings when a Dolby Vision stream was muxed togheter with another audio stream in a progressive muxing.\nFixed the default_sample_duration of the init mp4 file for audio codecs with CMAF muxing. This fixes playback issues with DASH and HLS streams.\nManifestGenerator\nV2\n: The Period@start attribute is now set to\n0\nfor the first period. This fixes playback issues for LIVE encodings with DASH manifests in dash.js players.\nbaseMediaDecodeTime\nin the init mp4 file for HE-AAC-V1 and HE-AAC-V2 with fMP4 muxing is now set correctly. This fixes playback issues with DASH and HLS streams.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.91.0\nReleased 2021-09-07\nAdded\nAdded new property for DASH manifests which defines the compatibility of the manifest with the Standard DASH Edition. Setting the dash edition compatibility to V4 will support the endNumber attribute for SegmentTemplate DASH manifest, specifying the last available segment. This feature is supported when creating a new DASH manifest via the\nstartEncoding call\nand configuring the V2 ManifestGenerator OR when creating the manifest via the\nDASH manifest creation endpoint\nFixed\nFixed incorrectly reported number of muxed bytes for Dolby Vision encodes with a mix of fragmented and progressive outputs.\nCorrupted subtitle packets are now skipped instead of halting Live encodings\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.90.0\nReleased 2021-08-31\nAdded\nDRM is now supported for\nMP4 muxings\nwith\nDolby Digital\nand\nDolby Digital Plus\naudio codecs.\nSupport input files larger than 2TB on AWS.\nFixed\nThe color configuration conversion has been enhanced to also support a change of the transfer characteristics even if the color space and color primaries of input and output are identical (e.g. HDR10 to HLG).\nIn DASH manifests generated with ManifestGenerator\nV2\nthe AdaptationSet@id attributes were set to UUIDs which caused some players to fail because this property should be an unsigned int.\nFixed a bug for the manifestGenerator\nV2\nin combination with\nDtsPassThroughAudioCodecConfigurations\nwhich caused encodings with VOD DASH manifests to fail and VOD HLS manifests to have an incorrect targetDuration.\nWhen using\nACLs\nto provide access to an S3 bucket from another account, the\nGetBucketLocation\nquery could have failed, even though the permissions were set correctly which caused the encoding to fail\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.89.1\nReleased 2021-07-15\nFixed\nFixed the default_sample_duration of the init mp4 file for audio codecs with CMAF muxing. This fixes playback issues with DASH and HLS streams.\nFixed failing audio-only encodings when using\nconcatenation\nwith input files that include a video stream.\nFixed a bug with\nH264 picture timing trimming\n. If the start timing is before the first keyframe of the input, the start timing is now set to the timing of the first keyframe.\nFixed a bug which could lead to failed encodings when converting colorspace to BT709.\nFixed encodings failing when applying both trimming and conform filter with\nDolby Digital\nand\nDolby Digital Plus\naudio codecs.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.89.0\nReleased 2021-08-24\nAdded\nThis version is automatically mapped to 2.89.1\n2.88.0\nReleased 2021-08-10\nAdded\nImplemented support for the DTS:HD and DTS:X audio codecs. The openAPI can be found\nhere\n. Only MP4 and internally fragmented muxings are supported for DTS:HD/DTS:X. DRM configurations are not supported with DTS:HD/DTS:X codecs.\nDRM is now supported for fMP4 muxings with Dolby Digital and Dolby Digital Plus.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.87.0\nReleased 2021-08-03\nAdded\nAdded a new AudioVideoSyncMode\nRESYNC_AT_START_AND_END\nto the\nStartEncodingRequest\n. This mode pads audio streams with silence, if the audio streams are shorter than the video stream. This prevents DASH clients trying to download non-existent audio segments, if the mediaPresentationDuration is longer than the duration of the audio stream. This new mode is now also the default value.\nFixed\nWhen running a live encoding with HLS manifests having a\nliveEdgeOffset\nclose to the\nsegmentLength\n, the manifest could have stopped updating after a couple of minutes, although segments are still written correctly\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.86.0\nReleased 2021-07-27\nAdded\nManifest generator\nV2\nis now generally available and supports all manifest features of our API.\nIt can be used by directly setting manifest IDs in the\nstart encoding request\nand setting\nmanifestGenerator\nto\nV2\n.\nWe are therefore deprecating the manifest generator\nV2_BETA\noption and will remove it from our API SDKs in release\nv2.89.0\n.\nFor more information see our tutorial\nhere\n.\nFixed\nThe RFC6381 codec label (e.g. \"avc1.64001F\") for\nCMAF muxings\nis now correctly set in DASH and HLS manifests.\nIf the same input source is created twice and used twice in an encoding, we are no longer downloading the input file twice.\nFixed a bug with handling HEVC inputs with open GOPs which caused encodings to fail.\nRASL (Random Access Skip Leading) frames are now correctly removed from the very first GOP of the HEVC inputs.\nFixed SegmentTemplate timescale in DASH manifests potentially being set to a wrong value when using variable framerate inputs, causing clients to request a non-existent video segment at the end.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.85.0\nReleased 2021-07-20\nAdded\nAdded support for Thumbnail Letter and Pillarboxing. Supported aspect modes:\nCROP\n,\nPAD\n,\nSTRETCH\n. More Details described in the API Calls:\nBIF\nSprite\nThumbnail\nFixed\nMuxing an H265 video and Dolby Atmos audio stream into a single MP4 container is now supported.\nWe are now more resilient against Azure connection issues when checking if input files exist.\nAn issue was fixed where VP9 3-PASS encodings could fail if the target bitrate of a single segment was too close to the codec-level's maximum bitrate.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.84.0\nReleased 2021-07-13\nAdded\nIMSC as subtitle output format\nfrom TTML and SRT inputs. For TTML, styling passthrough is available.\nFixed\nColor conversion to the following color primaries: bt709, smpte170m, smpte240m, bt2020. As a result, color primaries conversion from smpte432 to bt2020 is not red-shifted anymore.\nLong running HEVC encodes with 3-pass and\ncutree\nsetting enabled failing in rare cases.\nFor\nDASH manifests\n, containing Dolby audio renditions, the\ncodecs\nattribute is not set on\nAdaptationSet\nlevel anymore but only on the\nRepresentation\nlevel. This also fixes a rare case of duplicate entries in the\ncodecs\nattribute which could have led to device compatibility issues.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.83.1\nReleased 2021-07-15\nAdded\nDolby Digital\nand\nDolby Digital Plus\ncodec configurations are now available as replacement for AC3 and EAC3.\nWe are therefore deprecating\nAC3\nand\nEAC3\nas the two new codec configurations are producing output, which conforms to Dolby's high certification standards.\nOur\nmulti-codec example\nnow uses Dolby Digital instead of AC3.\nFor more information see our tutorial\nhere\n.\nSprite\ngeneration no longer requires both width and height to be set, as long as one of the two values is configured, the other one is automatically computed based on the aspect ratio of the video.\nThumbnail creation\nhas been extended to allow the same options with regards to height and width as sprite generation.\nFixed\nImproved internal stall-detection to prevent additional edge cases. These edge cases previously lead to stalls resulting in unexpectedly long running encodings.\nDolby Atmos encodings previously failed if there was an additional unused audio stream along the video stream. This has been fixed.\nEncodings with\ninput concatenation\nwith inputs that have different fps have occasionally stalled. This was fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nAdaptationSets\nin\nDASH manifests\ncould contain the same codec multiple times in the\ncodecs\nattribute, when the manifest contains a Dolby Digital, Dolby Digital Plus or a Dolby Atmos rendition.\n2.83.0\nReleased 2021-07-06\nAdded\nThis version is automatically mapped to 2.83.1 because of a bug in Dolby Vision processing for segmented output.\n2.82.1\nReleased 2021-07-15\nAdded\ntargetDurationRoundingMode\nproperty on\nHLS manifest\n, which defines the rounding mode for the target duration (normal or upward rounding) for manifests generated during the encoding.\nFixed\nLive encoding restarts\ncould have taken several minutes or lead to an encoding error\nImproved internal stall-detection to prevent occurences of unexpectedly long running encodings due to stalls.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.82.0\nReleased 2021-06-29\nAdded\nThis version is automatically mapped to 2.82.1 because of a bug in Dolby Vision processing for segmented output.\n2.81.0\nReleased 2021-06-22\nAdded\nAbility to handle ephemeral infrastructure errors working with genuine S3 has been improved\nSupport for additional output resolutions when encoding from a 3840x2160 Dolby Vision mezzanine file: 480p, 320p, 240p and 160p\nFixed\nFixed a bug where\nSprites\nwith lots of thumbnails (because of a long video file or a low\nduration\nvalue) could lead to a failing or stalling encoding.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.80.0\nReleased 2021-06-15\nFixed\nThe stability of live-encodings subtitle streams using Zixi as ingest has been improved.\nA bug has been fixed for live encodings where the segment duration included fractional seconds. In those cases, the segment duration was erroneously rounded-down to the next full second.\nEncoding jobs with progressive muxings and DolbyVision output did not handle letterboxing correctly, possibly resulting in the DolbyVision filter being incorrectly applied.\nWhen using\nS3 Role-based Input\nwithout an externalId to download bigger files could have lead to a failed download if it took more than 15 minutes.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.79.1\nReleased 2021-06-09\nFixed\nA bug has been fixed for live encodings where the segment duration included fractional seconds. In those cases the segment duration was erroneously rounded-down to the next full second.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.79.0\nReleased 2021-06-08\nAdded\nAdded support for SCTE-35. When running a live encoding, calling this\nendpoint\nwill trigger the insertion of cue tags in the provided HLS manifests and ads will be inserted.\nNote: this is an experimental feature.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.78.0\nReleased 2021-06-01\nAdded\nImproved speed of h.264 encodings with short input files\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.77.2\nReleased 2021-06-16\nFixed\nEncoding jobs with Dolby Vision output that have multiple progressive and/or multiple segmented muxings failed in previous versions. This is properly supported now.\nThe RTMP live-input related error 'broken pipe' now leads to a reconnect-attempt, while previous versions failed immediately.\nSubtitle stream encoded with the\nwebvtt\nformat and uploaded to S3 did not have the right mimetype set. This has been fixed and the uploaded subtitle file's mimetype is now correctly set to \"text/vtt\".\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.77.1\nReleased 2021-05-31\nAdded\nThis version is automatically mapped to 2.77.2 because of a bug in the internal chunk-length calculation for progressive muxings in some rare edgecases\n2.77.0\nReleased 2021-05-25\nAdded\nThis version is automatically mapped to 2.77.2 because of a bug in the conform filter when inputs of different FPS were used that is fixed in 2.77.2.\n2.76.0\nReleased 2021-05-11\nAdded\nAdded support for additional\nSprite\nfeatures so that they can be used for trick-play with DASH manifests. It's now possible to additionally specify the tile format (\nhTiles\n/\nvTiles\n), JPEG quality and a creation mode.\nAdded\nImage adaptation sets\nfor DASH manifests with\nSprite representations\nto enable video players to provide tiled thumbnails based on\nSprites\n.\nFixed\nFixed a bug which could lead to incomplete progressive subtitle output in case that multiple progressive subtitle outputs were configured for the same stream.\nFixed an issue related to audio/video sync which could have occured under rare conditions if segmented and progressive muxings are mixed and the input frame-rate is variable.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.75.0\nReleased 2021-05-04\nAdded\nThe status endpoints for\nDASH\n,\nHLS\nand\nSmooth\nmanifests are now populated correctly when manifest creation is triggered via the\nstart encoding call\nFixed\nFixed some internal errors that occured for progressive muxings when both fragment length and GOP size have been configured\nA memory leak has been fixed that caused live encodings to fail after several hours.\nFixed a bug where PerTitle's complexity factor was not applied to streams with StreamMode.PER_TITLE_TEMPLATE_FIXED_RESOLUTION_AND_BITRATE.\nFixed a bug where the\nLive Stop Rest Request\nwasn't processed correctly if the encoding is just starting up.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.74.0\nReleased 2021-04-27\nAdded\nUpdates in the internals of the TTML to WebVTT subtitle conversion:\nAdded support for basic styling in TTML to WebVTT subtitle conversion, configurable via\nWebVttConfiguration\nAdded support for\nChunkedTextMuxing\nin TTML to WebVTT subtitle conversion: allows to output segmented WebVTT files\nChanged\nAdapted internal Dolby Vision processing for encodings from J2K mezzanine files for workflows with segmented muxings\nFixed\nFixed a rarely occurring timing problem at the MP4 muxing step, which caused the encoding to fail.\nPacked Audio\nsegmentlength reverts to default 4.0 seconds.\nImproved validations and error messages for invalid pixel formats and color configurations.\nGeneral stability improvements for multi-pass encodings on AWS cloud\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.73.0\nReleased 2021-04-20\nAdded\nSupport of 10-bit VP9 encoding by setting\npixelFormat\nVP9 Codec Configurations\naccordingly\nFixed\nTo avoid rate exceeded errors for assuming IAM roles, caching of temporary credentials for the\nS3 role-based\nauthentification process was introduced\nA bug with the generation of HLS manifests for LIVE encodings has been fixed, were floating-point segment lengths were rounded down.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.72.2\nReleased 2021-05-05\nFixed\nA race-condition that could result in encoding jobs to fail in rare circumstances has been fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.72.1\nReleased 2021-04-28\nFixed\nA bug has been fixed that caused live encodings to fail after several hours\nKnown Issues\nA race-condition can result in failing encoding jobs in very rare cases\nS3 role-based output for segmented muxings: No upload verification available.\n2.72.0\nReleased 2021-04-13\nAdded\nA new warning will be triggered and displayed in the\nstatus message of the encoding\nwhen the Subtitle Rate in\nBroadcast TS muxings\nis set too low and some packets got dropped.\nKnown Issues\nLive encodings may fail after several hours! This issue was resolved in\nVersion 2.72.1\nS3 role-based output for segmented muxings: No upload verification available.\n2.71.0\nReleased 2021-04-07\nAdded\nImproved bitrate distribution of\nVP9 encoded bitstreams\nin 3-pass encoding mode: The spikiness in the bitrate distribution, and consequently quality fluctuation in certain frames, in specific usecases has been reduced.\nRegion europe-west3 is now supported in GCE cloud\nV2 manifest generator (BETA)\nAdded support for the\nmode\nselector in DASH manifest representations when manifests are generated via specifying them in the\nStartEncodingRequest\n.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.70.0\nReleased 2021-03-30\nAdded\nBroadcast TS muxings\ncan now configure the PID for subtite streams\nBroadcast TS muxings\ncan now configure the rate for subtitle streams\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.69.0\nReleased 2021-03-23\nFixed\nRemoved cue identifiers from\nWebVtt\ninput if\ncueIdentifierPolicy\nis set to\nOMIT_IDENTIFIERS\nwhen using\nChunked WebVtt muxing\n.\nFixed rounding in target duration calculation based on segment length for HLS manifests.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.68.0\nReleased 2021-03-16\nFixed\nFixed a bug which lead to artifacts for some assets with mixed open and closed GOPs\nFailed encodings when converting\nSRT\nsubtitles to\nDVB-SUB\nwith an input file without a '.srt' file extension have been fixed\nFixed a bug which potentially generated skips in the output when the input had vastly fluctuating FPS\nSporadic failures when reading from\nAzure storage\nhave been fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.67.0\nReleased 2021-03-09\nAdded\nSupport for stream conditions for\ntemplate streams\nof Per-Title Encodings.\nAES-128 encryption support for\nPacked Audio Muxings\nDecreased turnaround times for encodings with short input files up to 15%\nFixed\nFixed an issue where encodings using\nAzure Output\nhave stalled\nKnown Issues\nDue to reduced logging capabilities for livestreams we do NOT recommend this version to be used for live usecases!\nS3 role-based output for segmented muxings: No upload verification available.\n2.66.0\nReleased 2021-03-02\nFixed\nFixed an issue in Per-Title algorithm where an input with extremely low spatial and temporal complexity lead to failures.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.65.0\nReleased 2021-02-23\nAdded\nOn encodings that fail with licensing errors, the corresponding retry hint is set in the\nstatus response\n.\nFixed\nVerification for\nNexGuard FileMarker Watermarking\nthat input source FPS and configured keyFrameInterval/GOP corresponds to the requirements if the frame rate is not configured in the codec configuration.\nImproved stability for Azure Cloud Storage\nFixed a bug that lead to failed encodings when applying long trimmings to perTitle encodings.\nFixed a bug for HEVC outputs in combination with fMP4 and FairPlay DRM where the parameter sets were incorrectly placed in the samples, causing playback issues in native MacOs devices.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.64.1\nReleased 2021-02-09\nFixed\nFixed a bug which in some cases lead to pixelation in black areas of the highest output-rendition when using\nNagra NexGuard FileMarker A/B Watermarking\nin multipass encodings\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.64.0\nReleased 2021-02-09\nAdded\nPacked Audio supports segment naming template.\nFixed\nWhen using empty\nsourceChannels\nin\naudio-mix input stream\nthe input channels will be automatically mapped to the same output channels by default\nFixed a color conversion issue for Dolby Vision Encodes from Prores inputs where the internal color conversion from YUV to RGB was performed using BT709 where is should be BT2020.\nFixed a rare bug for HEVC encodes with progressive mp4 output where the parameter sets were not included in the output file correctly.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.63.0\nReleased 2021-02-02\nAdded\nSecuring your assets with\nNagra NexGuard FileMarker A/B Watermarking\nis now available\nSegmented WebVTT subtitles now can be\nconfigured\nto either omit or include WebVTT cue identifiers (included by default). Please note that the following restrictions apply:\nSRT to WebVTT conversion workflow does not support cue identifiers as of now. Therefore it needs to be set to\nOMIT_IDENTIFIERS\n.\nCue identifier can not be configured for\nnon-segmented\nWebVTT outputs.\nFixed\nFixed a bug where WebVtt Encodings failed with subtitles using\nFileInputStreamType SRT\n. The bug occurred only in combination with video input files, that have negative DTS timestamps at the beginning.\nFixed a bug that audio streams were not sent to the\nLiveMediaIngest endpoint\n.\nFor a few audio-only input files, the analyze phase might have failed as the duration could not be parsed\nFixed a possible situation where duplicate DTS values are present in\nsegmented TS outputs\nin case that the\nH264 codec\nis used with a b-pyramid setting of either 'normal' or 'strict'.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.62.0\nReleased 2021-01-26\nAdded\nSupport for PackedAudioMuxing: Allows to create\nPacked Audio Segments\ncontaining encoded audio samples and ID3 tags that are packed together with minimal framing and no per-sample timestamps.\nAdded configuration of PCR interval for\nBroadcast TS\n.\nChanged\nOptimized turnaround times for Dolby Vision 2-pass and 3-pass encoding jobs up to 40%\nFixed\nThe set\nencryptionMethod\nis now used when using\nCENC encryption\nwithout Fairplay\nFixed an issue where trying to detect the subtitle format from the data stream failed and caused an encoding error, by trying to deduce the subtitle format from the file extension in this case. If both fails, the encoding will still fail.\nFixed a bug for ProgressiveMovMuxings which caused the output of muxings, that contained more than one audio stream, to only contain the first audio stream.\nFixed a bug where the Encoding failed if more than one video representation has been configured in combination with subtitles with\nFileInputStreamType SRT\n.\nFixed a rarely occurring race condition during encoding start-up that was preventing our service to use all the computing nodes.\nThe i-frame-playlist manifests for fmp4 encodings have been corrected\nDownloads might have stalled with HTTP(S) inputs and file paths containing non-alphanumeric characters\nFixed an issue with failed encodings when using\nchannelLayout\nin\naudio mix input stream\nFixed a potential issue where interlaced MOV files led to encoding errors because they were detected as interlaced Mp4 has been fixed.\nBugfix where HEVC encodes with Level 4.1 set were falsely set to level 4.0.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.61.1\nReleased 2021-01-11\nAdded\nDolby Vision output\ncan now also be created from ProRes mezzanine files\nFixed\nFixed an issue with the encoder not being able to extract cdp CEA captions\nFixed that VP9 3-Pass encodings could fail in case that the target bitrate of single segments was close to the codec-level's maximum bitrate and QMIN was set to a low value.\nIn rare conditions, an encoding might have stalled in the Analysis phase and go to error after 1 hour\nIf the\nROTATION\nstream condition\nis used and the\nrotation\nproperty is not present in the input file, a default of\n0\nwill be used.\nInput subtitle text files with exotic character encoding, BOM markers, exotic end of lines or trailing white-spaces are now sanitized before being decoded.\nA bug mispredicting the character encoding of a UTF-8 subtitle input file with many languages (not a real-life use case, but useful for testing) has been fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.60.1\nReleased 2020-12-16\nFixed\nFixed a bug that potentially caused out of sync issues of video and audio for h.264 encodings with CBR and BroadcastTS muxings\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.60.0\nReleased 2020-12-15\nAdded\nAdded DVB-SUB subtitle passthrough for BroadcastTsMuxing.\nAdded SRT to DVB-SUB subtitle conversion for BroadcastTsMuxing.\nAdded support for\nSRT file\nto\nWebVtt subtitle encoding\nwith\ntext\n/\nchunked-text muxing\n.\nFixed\nSRT subtitle burn-in subtitle feature was corrupting memory when the target pixel format was different from the one from the source video. This has been fixed now.\nA potential problem has been fixed for 3-pass encodings, where\nkey-frame-boundaries with segment-cut\nwas enabled.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.59.0\nReleased 2020-12-01\nChanged\nImproved turnaround time for\nEnhanced Deinterlace Filter\nby more than 60 percent.\nFixed\nFixed a bug where an encoding might finish successfully but not the whole content was encoded when using\nConcatenation Input-Streams\ncombined with trimming, if at the concatenation boundaries the cumulated duration is an exact multiple of the segment length.\nQuality improvement for HRD CBR compliant progressive outputs.\nSet the correct\ncontent-type\nwhen using\nProgressive MP4\n. If it's an audio-only stream it will be set to\naudio/mp4\n, if it's video only or contains both, it will be set to\nvideo/mp4\nFixed invalid outputs in\nEnhanced Deinterlace Filter\nwhen both Field and Frame stream filters are requested in the same encoding.\nFixed a crash that was triggered by encoding with the same\nEnhanced Deinterlace Filter\nmode but different FPS. Example: 50 FPS Field and 25 FPS Field is now working.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.58.0\nReleased 2020-11-17\nAdded\nCustomers using their own\nAzure account\ncan now opt-in, so that all SSH communication comes from one specific IP address. This enables a more rigid network security policy where only one IP is allowlisted for SSH connections.\nFixed\nFixed a bug in the fMP4 muxing of audio segments where the last segment contained 1 single audio frame.\nCombination of Audio Mix and Audio Merge with StreamInput are now handled properly.\nThe\nfMP4 muxing-information\nendpoint now correctly returns\n\"codec\": \"aac\"\nfor muxings with AAC audio codec.\nFixed a bug for input files that were creating an empty segment and therefore the encoding failed.\nA memory leak related to the frame rate change logic has been fixed, which possibly caused encodings to fail, when changing from a higher to a lower frame rate.\nFixed an issue, where DASH and HLS manifest creation failed for\nSINGLE_PASS\nand\nTWO_PASS\nencodings, which had PRORES or JPEG2000 inputs.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nAn encoding might finish successfully with not the whole content encoded when using\nConcatenation Input-Streams\ncombined with trimming, if at the concatenation boundaries the cumulated duration is an exact multiple of the segment length.\n2.57.0\nReleased 2020-11-03\nAdded\nAdded handling of\nSMPTE timecode\nwith three flavours -\nNON_DROP_FRAME\n,\nDROP_FRAME\n, and\nAUTO\n.\nPrivate ACLs are now taken into account for manifests and live encodings\nFixed\nDAR values are now correctly set in all progressive webm outputs for VP8/VP9.\nEncodings failing at the output validation step, with init.mp4 file missing, have been fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nA memory leak related to the frame rate change logic might cause encodings to get stuck or fail, when changing from a higher to a lower frame rate.\nAn encoding might finish successfully with not the whole content encoded when using\nConcatenation Input-Streams\ncombined with trimming, if at the concatenation boundaries the cumulated duration is an exact multiple of the segment length.\n2.56.2\nReleased 2020-11-25\nFixed\nFixed a bug where an encoding might finish successfully but not the whole content was encoded when using\nConcatenation Input-Streams\ncombined with trimming, if at the concatenation boundaries the cumulated duration is an exact multiple of the segment length.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nThis information can be found in this FAQ from now on:\nWhich output options are supported when using per-title?\n2.56.1\nReleased 2020-11-16\nFixed\nA memory leak related to the frame rate change logic has been fixed, which possibly caused encodings to fail, when changing from a higher to a lower frame rate.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nThis information can be found in this FAQ from now on:\nWhich output options are supported when using per-title?\nAn encoding might finish successfully with not the whole content encoded when using\nConcatenation Input-Streams\ncombined with trimming, if at the concatenation boundaries the cumulated duration is an exact multiple of the segment length.\n2.56.0\nReleased 2020-10-20\nAdded\nA new\nEnhanced Deinterlace Filter\nis now available.\nAzure Connect: Encoding on\ncustomer Azure infrastructure\nis now available for VoD and live encodings.\nAdded support for SRT live input streams with a bitrate >= 40Mbps.\nEnabled full VBV buffer conformance for progressive h.264 CBR outputs.\nFixed\nFixed the\ncache-control\nheader for manifest uploads in both Live and Vod workflows to contain the correct\nmax-age:value\ninstead of\nmax-age=value\nstring\nFixed a bug for the usage of Display Aspect Ratio (DAR) on VP9 and AV1 encodes if muxed with\nProgressiveWebmMuxing\n(manifestType=NONE).\nWhen using I-Frame playlists with fmp4, the\n#EXT-X-TARGETDURATION\nand\n#EXTINF\nattributes are now correctly set based on the segment length\nFixed stream playback issue caused by inconsistent first DTS in shorter segments.\nIn certain use cases, using keyframes in conjunction with concatenation workflow lead to failed encoding. This has been fixed.\nLive encodings did not work when using AWS or GCP connect and being opt-in so that all SSH communication comes from one specific IP address\nFixed an issue with default manifests for per-title that potentially lead to failed encodings.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nThis information can be found in this FAQ from now on:\nWhich output options are supported when using per-title?\nA memory leak related to the frame rate change logic might cause encodings to get stuck or fail, when changing from a higher to a lower frame rate.\nAn encoding might finish successfully with not the whole content encoded when using\nConcatenation Input-Streams\ncombined with trimming, if at the concatenation boundaries the cumulated duration is an exact multiple of the segment length.\n2.55.0\nReleased 2020-10-06\nChanged\nImproved download and upload speed for Azure significantly.\nFixed\nFixed playback issue on some devices for HEVC/fMP4 streams. The stream duration was signaled incorrectly in the container.\nFixed a bug where the I-frame playlist was missing in the HLS output.\nLive streams are now working when using\nGCP Connect\nor\nAWS Connect\nFixed a bug for Per-title encodings where specific sample and display aspect ratio combinations caused incorrect width or height of the output.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available.\nA memory leak related to the frame rate change logic might cause encodings to get stuck or fail, when changing from a higher to a lower frame rate.\n2.54.0\nReleased 2020-09-22\nChanged\nImproved download speed of input files in Azure cloud up to 150%\nBetter error messages for failed manifest outputs.\nHLS VOD manifests now by default include the framerate in the master manifest. (requires a correct streamId being set on the HLS streaminfo)\nHLS VOD manifests now include PLAYLIST-TYPE:VOD.\nFixed\nFixed an issue where the disk size for Azure encodings was too small which could have lead to failing encodings.\nFixed upload manifests to multiple outputs, even if one of the outputs fails.\nPossible failures in encodings which have MP4 muxings, with DRM and HLS_BYTE_RANGE request, have been fixed.\nAdded fix for correcting chromaloc flag for H264 and H265 encoder config.\nFixed issue that generated missing audio output when audio input had no pts. WMA Pro Audio inputs with this issue can be encoded correctly now.\nThe size of overlay filters (watermarks, text etc.) was fixed for concatenated input streams with different resolutions and same aspect ratio.\nSingle stream input video files with an RGB pixel format and the RAWVIDEO codec are encoded properly now.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available.\n2.53.0\nReleased 2020-09-08\nAdded\nEnabled multiple Per-Title progressive muxings to separate files in the same target directory.\nSupport for VoD SMOOTH manifests as option of start encoding call\nSupport for 3-pass encodings in Azure\nImproved encoding speed of high bitrate J2K inputs in Azure\nSupport for XDCAM output with the following new codecs / muxings:\nH262 / MPEG2 video codec configuration\n, with mandatory preset\nXDCAM_HD_422\nPCM audio codec configuration\nMXF muxing\nFixed\nIssue when vbv parameters are set in 3-pass algorithm with the last segment quality was fixed\nFixed some combinations of color config with VP9 codec which lead to failures\nFixed an error that led to failed 3-pass encodings in the cloud region AWS_EU_WEST_2\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available.\n2.52.0\nReleased 2020-08-25\nAdded\nAdded support for using the same input audio stream for both\nAudio-Mix filter\nand non-filter workflows.\nIf for the encoding no SAR is specified then in the resulting video SAR is available as metadata with the value of 1:1.\nChanged\nEncodings with at least one UHD+ output stream are up to 33% faster.\nImproved stability and speed of encodings with longer segment lengths and/or very high input-bitrate in GCE\nFixed\nA possibly re-connect issue has been fixed for the live use-case when a timestamp rotation occurs in the following order Video, Subtitle, and Audio and the Subtitle packet was following an Audio packet.\nBugfix for Conform filter together with trimming for video streams.\nA potential problem for live encodings has been fixed that previously led to a restart. This happened whenever a timestamp rotation occurred and the first packed with the updated timestamp has been a subtitle packet and is fixed now.\nImproved handling of Azure authentication issues.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available.\n2.51.0\nReleased 2020-08-11\nAdded\nOCR processing of DVB subtitles is now supported for live encodings with an output using the WebVTT codec and a muxing of either ChunkedText or fMP4\nChunkedTextRepresentation\nfor Segmented WebVtt in DASH Manifests in combination with DVB subtitles\nDvbSubtitleInputStream\nto allow specifying the Dvb subtitle input stream\nSupport for Segmented WebVtt in HLS Manifests in combination with DVB subtitles\nAdded the\nGCE region endpoint\nto specify the network and the subnet for GCE-Connect.\nCBR encoding for h.264 is now supported with correct model parameters.\nChanged\nPer-Title will fail now for cases where a FIXED_RESOLUTION will not fit inbetween two FIXED_RESOLUTIONS_AND_BITRATE for complex assets because of a configuration problem, i.e., bitrate ranges for FIXED_RESOLUTIONS_AND_BITRATE in combination with the minimum step size.\nThe issue with stalling encodings for inputs with reordered frames in Mov was improved. We will fail instead of stalling.\nFixed\nBugfix for Dolby Vision and trimming. The output could be missing some frames at the end of the encode.\nImproved stability of the live stream where the input ingest has out of order subtitle packets.\nLive encoding stability has been improved by detecting a stalled input and trying to re-connect\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.50.1\nReleased 2020-07-30\nAdded\nWith\n2.50.0\nwe improved the download speed for: S3, GCS and Generic S3. As per this improvement, we require the\nGetBucketLocation\npermission. If this permission is not set, we will now fall back to the slow download instead of failing.\nFixed\nImproved encoding speed for Google regions when using the customer's\nGCP infrastructure\ntogether with using the option to run all API communication from a specific IP range, as now also worker nodes are requested using the same IP range.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available\n2.50.0\nReleased 2020-07-28\nAdded\nAdded\nDolby Atmos\nsupport\nInput as ADM or DAMF Dolby Master files\nOutput to fMP4 in DASH/HLS\nSupport for Widevine, PlayReady and FairPlay DRM\nImproved bitrate distribution over segments for 3-pass encodings\nEspecially improves last segment visual quality\nChanged\nImprove the download speed from the following inputs: S3, GCS and Generic S3\nThe handling of input timestamps for Zixi/SRT live stream encodings is improved.\nFixed\nFixed an issue with customData on filters leading to \"scheduling failed\" errors\nFixed encoding failures for some VP9 encodes and tuned encoder rate control, quality and speed.\nFixed a rounding error in the Per-Title algorithm that led to a missing FIXED_RESOLUTION_AND_BITRATE rendition in some very rare circumstances.\nKnown Issues\nPer-Title encodings only work with GCS, S3, Generic S3, Akamai NetStorage or Azure Blob output.\nProgressive MP4 with CBR encodings might have incorrect HRD buffer signaling.\nS3 role-based output for segmented muxings: No upload verification available",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/reducing-turnaround-times-for-short-form-video-content",
    "title": "Reducing Turnaround Times for Short-Form Video Content",
    "text": "Overview\nBitmovin's split-and-stitch cloud architecture enables massive horizontal scale by allowing different parts of a video to be processed simultaneously and then reassembled for playback. This type of workflow works especially well with longer content but needs some adjustments when working with short-form video content of up to 5 minutes in length.\nTo optimize your encoding workflow for short-form video content, some parameters need to be adjusted for shorter turnaround times.\nFor more information on use cases and our evaluations, please read our Blog post about\nGPU Acceleration for Cloud Video Encoding\n.\nAccelerated Mode\nIf you're encoding\nvideos under 5 minutes\n, in a\nManaged Cloud\ndeployment, using\nSTABLE\nEncoder version, you'll automatically experience faster queueing times.\nHere are some things to keep in mind:\nSupported inputs types (for probing the content):\nS3, S3 role based, HTTP, HTTPs, Azure\nSupported regions*:\nAWS:\nEU_WEST_1\n,\nEU_WEST_2\n,\nEU_CENTRAL_1\n,\nAP_NORTHEAST_2\n,\nAP_SOUTHEAST_2\n,\nUS_EAST_1\n,\nUS_WEST_2\n,\nGCP:\nEUROPE_WEST_1\n,\nUS_CENTRAL_1\nAZURE:\nEUROPE_WEST\n,\nUS_EAST2\n,\nCANADA_CENTRAL\n,\nUS_EAST\n,\nUS_WEST\n,\nJAPAN_EAST\n,\nGERMANY_WESTCENTRAL\n,\nEUROPE_NORTH\n,\nUS_WEST2\n,\nAUSTRALIA_EAST\nLadder limitations:\nUp to 1080p\nHardware Encoding:\nIf Hardware Encoding is configured (by specifying \"presetConfiguration\": \"VOD_HARDWARE_SHORTFORM\" - available only for h264 and h265), certain restrictions apply as detailed in this\nguide\n. Currently*, the only region that supports Hardware Encoding is AWS\nEU_WEST_1\n.\nSome FAQs:\nIs there any additional cost?\n- No, using Accelerated Mode does not incur additional costs.\nWhy are my queuing times still high?\n- This could indicate that the encoding did not run in Accelerated Mode. Ensure there are no account limits that conflict with the above requirements.\n*This list of supported regions was last updated on on\n30.08.24\n.\nIf you're interested in using Accelerated mode but the region for your preferred Cloud provider is not listed here, please contact your Bitmovin representative.\n(Dynamic) Pre-Warmed Pools\n📘\nRequirements and known limitations\nPlease refer to\nHow to use Pre-warmed Encoder Pools\nspecific requirements and known limitations.\nWhen you start an encoding, it will first be queued whilst an encoder instance is spun up and configured, as described in\nWhat do the different encodings state mean?\nThe queue time can be a significant portion of the turnaround time, in particular for short source files. In most circumstances, you will be able to reduce that time with the use of pre-warmed encoder pools (described in detail at\nHow to use Pre-warmed Encoder Pools\n). A pre-warmed encoder pool can be\nstatic\nor\ndynamic\nin size, based on your workflow requirements and resource demands.\nMake sure to\nenable hardware-acceleration\nwith the\ngpuEnabled\nproperty for your pool. Otherwise, encodings configured with a\nVOD_HARDWARE_SHORTFORM\ncodec configuration preset are not able to benefit from hardware-acceleration.\nWhat compromise you may have to make\nPre-warmed pools may increase your encoding costs, in particular, if they’re not used with care. Only configure the pools with as many instances as you may reasonably require, and don’t forget to shut them down when not needed to avoid incurring costs.\nCreate and start a Dynamic Pre-Warmed Pool with GPU-enabled\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nPrewarmedEncoderPool pool = new PrewarmedEncoderPool();\n...\npool.setCloudRegion(CloudRegion.AWS_EU_WEST_1);\npool.setDynamicPool(true);\npool.setGpuEnabled(true);\n\ncreatedPool = bitmovinApi.encoding.infrastructure.prewarmedEncoderPools.create(pool);\n\nstartedPool = bitmovinApi.encoding.infrastructure.prewarmedEncoderPools.start(createdPool.getId());\nEncoding Configuration\nThe correct storage and region\nYour Input and Output storage should also be in the same cloud region, and you should configure your encoding to run in that same cloud region. Using hardware-acceleration is\ncurrently limited to AWS\n(\nNVIDIA T4\nGPUs on\nAmazon EC2 G4dn\ninstances).\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nEncoding encoding = new Encoding();\n...\nencoding.setCloudRegion(CloudRegion.AWS_EU_WEST_1);\n...\nencoding = bitmovinApi.encoding.encodings.create(encoding);\nA speed-focused preset: hardware-acceleration\n📘\nRequirements and known limitations\nPlease refer to\nHow to create an Encoding using hardware-acceleration\nspecific requirements and known limitations.\nAll of our codec configurations allow you to use a preset. Some of those presets are using hardware-acceleration.\nH264:\nVoD Hardware Preset Configurations\nH265:\nVoD Hardware Preset Configurations\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nH264VideoConfiguration config = new H264VideoConfiguration();\n...\nconfig.setPresetConfiguration(PresetConfiguration.VOD_HARDWARE_SHORTFORM);\n...\nreturn bitmovinApi.encoding.configurations.video.h264.create(config);\nUse a previously started Pre-Warmed Pool\nAttention: Pre-warmed Pool might take a couple of minutes to be ready. Starting an encoding immediately after the start Pre-Warmed Pool request was sent will probably result in an encoding that will not utilize the Pre-Warmed Pool.\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nvar scheduling = new Scheduling();\nscheduling.setPrewarmedEncoderPoolIds(Collections.singletonList(startedPool.getId()));\n\nvar startEncodingRequest = new StartEncodingRequest();\nstartEncodingRequest.setScheduling(scheduling);\n\nbitmovinApi.encodings.start(encoding.getId(), startEncodingRequest);",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/enabling-2-step-verification",
    "title": "Enabling 2-Step Verification",
    "text": "2-step verification adds an extra layer of security to your account. In addition to your username and password, you'll enter a code that you can get with authenticator app upon signing in.\n2-step verification drastically reduces the chances of having the personal information in your Bitmovin Account stolen by someone else.\nTo set up 2-step verification, go to your\nAccount Settings\nand follow the instructions.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoding-templates",
    "title": "Encoding Templates",
    "text": "📘\nBeta Feature\nThis feature is in beta mode. Which means it's open to everybody and fully usable and we are actively developing, gathering input and improving it quickly based on customer feedback.\nIntroduction\nTemplate format\nEncoding Templates are written in YAML\n, as there is less visual clutter in comparison to JSON, and YAML .2 is a superset of JSON. Additionally, you can enrich your Encoding Template with comments to explain configuration decisions as you would do in a custom script that uses our SDK.\nJava SDK reflection\nis possible because the new template service uses a\nJava SDK\nwhich means it follows the same hierarchy, therefore it is easier to construct new Encoding Templates from scratch and understand where to place the objects in the YAML.\nWe offer a JSON schema to\nvalidate your\nEncoding Template YAML\nfiles\n:\nhttps://raw.githubusercontent.com/bitmovin/bitmovin-api-sdk-examples/main/bitmovin-encoding-template.json\nTemplate Structure\nReferencing the Java SDK is a good starting point, the image below demonstrates the method of transposing the API Endpoints to Java.\nFrom the\nJava format\n, we can then see how to\nconstruct the YAML\nand nest the endpoints.\nSequence matters\nwhen it comes to the order of calls.\nThe image below shows the order that the template\nmust\nbe constructed in.\nAdditionally as shown in the Template Example on this page, sections that come later in the file will reference previously stated objects such as inputs, outputs and configurations all being used in the encoding section.\nSupported API functions\nNot all\nAPI\nfunctions are supported in the Encoding Templates, only those relevant to the configuration of the Encoding for VOD and Live are supported. Other features that are more organisational and operational such as, infrastructure and notifications are not supported, and if included in a template will cause the validation to fail.\nTemplate example\nAn example Encoding Template will look like this:\nyaml\nmetadata:\n  type: VOD # can be either `LIVE` or `VOD`\n  name: Standard VOD Workflow\n\ninputs:\n  https:\n    encoding_https_input: # user-defined name to be used for reference within the template\n    \t# `properties` indicates the request payload of a POST request\n      properties: # refers to the `/encoding/inputs/https` endpoint\n        host: bitmovin-sample-content.s3.eu-west-1.amazonaws.com\n        name: Bitmovin Sample Content\n\nconfigurations:\n\tvideo:\n  \th264:\n    \tencoding_h264:\n      \tproperties:\n    \t\t\t...\n\nencodings:\n  my-encoding:\n    properties:\n      name: Standard VOD Workflow\n      cloudRegion: AUTO\n      encoderVersion: STABLE\n\n    streams:\n      video_h264:\n        properties:\n          inputStreams:\n            - inputId: $/inputs/https/encoding_https_input # reference from line 7\n              inputPath: /bbb_sunflower_1080p_60fps_normal.mp4\n          codecConfigId: $/configurations/video/h264/encoding_h264\n          mode: PER_TITLE_TEMPLATE\n          \n    ...\n    \n    start: # exactly one Encoding per template is required\n      properties:\n        encodingMode: THREE_PASS\n        perTitle:\n          h264Configuration: {}\nPrerequisites\nCreate Input and Output Objects for Re-Use\nWhile you can create new input and output objects within the Encoding Template - see \"inputs\" section in the example above - it is generally recommended to setup input and output objects beforehand and use the respective UUIDs in the template.\nIn the Dashboard\nSince this is likely a one time task, we generally recommend to use the dashboard for setting this up.\nThe links for VOD:\nhttps://dashboard.bitmovin.com/encoding/inputs\nhttps://dashboard.bitmovin.com/encoding/outputs\nThe links for Live:\nhttps://dashboard.bitmovin.com/live/inputs\nhttps://dashboard.bitmovin.com/live/outputs\nWith the API\nBut since everything is API-based, you can also use any of the SDKs or directly the API to set up input and output objects.\nFor VOD you can find guides in the\nInput/Output Management section of the docs\n.\nFor Live you can find the guides in the\nLive Inputs section\nand\nLive Outputs section\nof the docs.\nHow To\nOption 1: Experience via Dashboard\nWe prepared some Encoding Template examples to get you started faster with some recommended workflows. Simply go to our Bitmovin Dashboard, browse the examples and start modifying them to your needs:\nVOD Encoding:\nhttps://dashboard.bitmovin.com/encoding/templates\nLive Encoding:\nhttps://dashboard.bitmovin.com/live/templates\nOption 2: Test via Postman\n🚧\nRecommended for Testing Only\nIntegrating calling the endpoint directly into your product is discouraged. Please see Option 3 for how to utilize templates in your production environment.\nThe current closed-beta prototype consists of a synchronous endpoint that receives an Encoding Template. This Encoding Template\nmust contain exactly one Encoding\nconfiguration, that the endpoint will trigger in our systems. This endpoint immediately\nreturns the Encoding ID\nfor further tracking via webhooks or status polling.\nEndpoint:\nhttps://api.bitmovin.com/v1/encoding/templates/start\nThere are two ways to pass the Encoding Template to this request:\nset a\nContent-Type:application/yaml\nrequest header and directly put your Encoding Template YAML into the request body\nset a\nContent-Type:application/json\nrequest header and directly put your Encoding Template as a string-serialized YAML into a\n{ configYaml: \"metadata:...\" }\nJSON request body\nDo not forget to also set the other request headers:\nrequired\n: set\nX-Api-Key\nwith your user's API keys\n(optional): set\nX-Tenant-Org-Id\ndepending on your organization structure of where you want to run the Encoding\nOption 3: Integrate Into Your Product via SDKs\nFor production deployments it's highly recommended to use the official Bitmovin OpenAPI SDKs. While Encoding Templates wrap most of the encoding configuration into a single API call, the SDK is still recommended for additional manifest creation, status checks, statistics and webhooks endpoints.\nExamples\n📘\nVOD Examples\nThe examples linked currently show VOD use cases, but those can easily be adapted for Live as well. If you want to see specific Live Encoding Templates please refer to the examples in the dashboard and use them as the .yml submitted in the SDK examples.\nTo make it easy to get started we created examples for how to utilize Encoding Templates.\nC#\nGo\nJava\nNode.js / JavaScript\nPHP\n(note: not functional yet, as there are limitations with the PHP SDK)\nPython\nFor all examples Bitmovin offers please see the\nBitmovin SDK Examples Github Repository",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/dc3128a8992c2797974394c3d78da6a4df5626e0d97ecbd0860c04b2d3146fae-Screenshot_2024-10-18_at_16.56.26.png",
      "https://files.readme.io/1e23040cd2c577089bf93b7131edc676440371ae63a1d12d037188ce75a8e558-Screenshot_2024-10-18_at_17.07.16.png",
      "https://files.readme.io/60f3e271f2b3da204d9135ec0d99db10f40d5c6249361f8931f30820d51b8cfc-Screenshot_2024-10-18_at_17.15.20.png",
      "https://files.readme.io/97848a4fec6d1ea7776d8fec12f36c9d8dfda01981f8c1b5dbff4cd8e079e489-Screenshot_2024-10-18_at_17.10.57.png",
      "https://files.readme.io/59af4d45d85fd1774a6ed6816a85d741dfe572109cd28d4927b04b516d727773-image.png",
      "https://files.readme.io/9c9f6818f96416d2210ff45301f529a645baabe1c85855c968f03875cc5a83cd-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/difference-between-inputstreams-and-direct-input-file-definition-on-a-stream",
    "title": "Difference Between InputStreams and Direct Input File Definition on a Stream",
    "text": "When configuring an encoding, and specifying how to access the input file, there are 2 mechanisms available to you:\nSupplying the information directly in the\nStream\npayload\n.\nOr creating an\nIngestInputStream\nand providing its identifier in the\nStream\nThe 2 mechanisms are functionally equivalent, and require the same type of information:\ninputId\n, the identifier of the\nInput\nstorage\ninputPath\n, with the full path to the input file on that storage\nselectionMode\nand\nposition\n, to define what stream to use in the input file.\nThe difference between the 2 mechanisms is historical. Specifying the information directly on the\nStream\nwas sufficient for most use cases, until we added support for more complex workflows where more than one input file is involved, such as\ntrimming and concatenation\n, Dolby Vision, Dolby Atmos, etc.\nFor those, we had to create a more flexible and more consistent way to specify files (and transformations on files), with multiple subtypes of\nInputStream\nobjects, the primary one for specifying video and audio input files being the\nIngestInputStream\n.\nSo, whilst for simple use cases the 2 methods are equivalent, we strongly recommend, if only for consistency, to use the second one. In the future any new input functionality will be provided through\nInputStreams\n.\nConversion\nTo convert your existing code to use the\nIngestInputStream\nis very simple. Here is an example for Java:\nBefore\nJava\nprivate static Stream createStream(\n      Encoding encoding, Input input, String inputPath, CodecConfiguration codecConfiguration)\n      throws BitmovinException {\n    StreamInput streamInput = new StreamInput();\n    streamInput.setInputId(input.getId());\n    streamInput.setInputPath(inputPath);\n    streamInput.setSelectionMode(StreamSelectionMode.AUTO);\n    \n    Stream stream = new Stream();\n    stream.addInputStreamsItem(streamInput);\n    stream.setCodecConfigId(codecConfiguration.getId());\n    \n    return bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream);\n}\nAfter\nJava\nprivate static Stream createStream(\n      Encoding encoding, Input input, String inputPath, CodecConfiguration codecConfiguration)\n      throws BitmovinException {\n    IngestInputStream ingestInputStream = new IngestInputStream();\n    ingestInputStream.setInputId(input.getId());\n    ingestInputStream.setInputPath(inputPath);\n    ingestInputStream.setSelectionMode(StreamSelectionMode.AUTO);\n    ingestInputStream = bitmovinApi.encoding.encodings.inputStreams.ingest.create(encoding.getId(), ingestInputStream);\n    \n    StreamInput streamInput = new StreamInput();\n    streamInput.setInputStreamId(ingestInputStream.getId());\n    \n    Stream stream = new Stream();\n    stream.addInputStreamsItem(streamInput);\n    stream.setCodecConfigId(codecConfiguration.getId());\n    \n    return bitmovinApi.encoding.encodings.streams.create(encoding.getId(), stream);\n  }\nNote that unlike\nStreamInput\n, which is an abstraction internal to the SDKs used in the construction of the\nStream\n,\nIngestInputStream\nis an API endpoint in its own right, and you therefore need to call it (as shown on line 8 above), before you can use it in the definition of the\nStream\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/adjusting-the-per-title-algorithm-for-custom-requirements",
    "title": "Adjusting the Per-Title Algorithm for Custom Requirements",
    "text": "Yes, you can. Our template based approach enables great flexibility and helps you cover your use-case best, e.g.: satisfy DRM requirements, target dedicated playout platforms, and many more. Besides the required minimal and maximal resolution (you don’t really need/want a 4k resolution if you target an audience mainly using mobiles) other boundaries in context of bitrate and frame-rate, as well as encoding parameters can be specified as well.\nA use-case where only a\nuser-defined set of resolutions\ncan be configured as well as a use-case where you want to\napply DRM to your Per-Title encoding\nor use it together with\nSSAI workflow\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/configuring-codec-to-maintain-original-video-aspect-ratio",
    "title": "Configuring Codec to Maintain Original Video Aspect Ratio",
    "text": "You can create an codec configuration, that will keep the aspect ratio of the original video by just omitting either the width or the height when you create it. Doing so, our service will maintain the aspect ratio of your input file, and calculate the omitted value based on that.\nIn the following an example for an encoding profile that will keep aspect ratio:\nOpen API SDK Java Example - Create an H264 Codec Configuration that maintains the aspect ratio:\nJava\n//Maintain the aspect ratio of the input file by providing \"height\" only\nH264VideoConfiguration config = new H264VideoConfiguration();\nconfig.setName(\"H264 - 1080p\");\nconfig.setPresetConfiguration(PresetConfiguration.VOD_STANDARD);\nconfig.setBitrate(4800000L);\nconfig.setHeight(1080L):\n\nreturn bitmovinApi.encoding.configurations.video.h264.create(config);\n\n//Maintain the aspect ratio of the input file by providing \"width\" only\nH264VideoConfiguration config = new H264VideoConfiguration();\nconfig.setName(\"H264 - 1080p\");\nconfig.setPresetConfiguration(PresetConfiguration.VOD_STANDARD);\nconfig.setBitrate(4800000L);\nconfig.setWidth(1920L);\n\nreturn bitmovinApi.encoding.configurations.video.h264.create(config);\nThis example is based on our\nOpen API SDK for Java\n, which is available on Github. Please see our\nOpen API SDK overview\nfor all our supported programming languages, and our\nGH API SDK Example Repository\nfor further examples.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rest-api-services-11000-11500",
    "title": "REST API Services 1.100.0 - 1.149.0",
    "text": "1.149.0\nChanged\nLive input stream status is set to\nNOT_AVAILABLE\nwhen a restart is happening.\n1.148.0\nChanged\nInternal stability improvements\n1.147.0\nAdded\nNew\ncheck permissions endpoint\n(currently for S3 outputs only) examines if permissions, on the storage to be used as output target by the Bitmovin encoder, are configured correctly.\nChanged\nEncodings started with the\nSimple Encoding API\nnow use the BETA Encoder version.\n1.146.0\nAdded\nLIVE_STANDARD\npreset is now available for\nH265\nCodec Configuration.\n1.145.0\nChanged\nInternal stability improvements\n1.144.0\nFixed\nFixed a bug that caused live events to be lost once the live stream has ended.\nA bug that led to incorrect brightness levels on encodes with DolbyVision inputs using 2 Pass & 3 Pass Encoding. Fail-fast had been added for the impacted encoder versions: 2.137.0 - 2.148.0.\n1.143.0\nFixed\nFor Live encodings an incorrect resolution was assumed if no input was connected and resolution not configured.\n1.142.0\nFixed\nPer-Title encodings with\ndirect file upload\nfailing in some scenarios\n1.141.0\nDeprecated\nEncoder version older than 2.2.0 are no longer supported.\n1.140.0\nAdded\nLive Encodings started with the\nSimple Encoding API\nnow support video-only input\nImproved error message for LIVE Encodings in case unsupported Manifest configurations for HLS, with Marlin, Playready, Primetime or Clearkey, are used.\n1.139.0\nAdded\nAdded a fail-fast for enableHLGSignaling set to true: ColorTransfer needs to be: ARIB STD-B67, UNSPECIFIED, or not set. In the last 2 cases, ARIB STD-B67 will be set implicitly. To avoid creating undesired output any other specific color transfer will result in a fail-fast for encoder versions newer than 2.138.0 (incl.) or equal to LTS verion 2.103.1.\nTo easily access the resulting Manifest files of the VoD encoding jobs, output artifacts have been added to the output objects that are returned by\nSimple VoD Encoding Job Response\n.\n1.138.0\nFixed\nRe-Subscribing to an account which was expired for more than 14 days did not re-activate the CDN feature\nPerformance- and stability improvements for high load scenarios\n1.137.0\nAdded\nAdded the Live profile\nLOWER_LATENCY\nto the\nSimple Encoding API\nwhich introduces much lower latency than the regular profile. Additional information about the Simple Encoding API Live profiles can be found\nhere\n1.136.0\nChanged\nInternal stability improvements\n1.135.0\nFixed\nFixed an issue with notifications called very often (like\nEncoding Status Changed Webhook\n) that could lead to notifications (email or webhook) being delayed or being delivered multiple times.\n1.134.0\nChanged\nInternal stability improvements.\nBitmovin’s\nSimple Encoding API\ncombines our Per-Title and 3-pass optimizations together with HLS and DASH packaging in a single API call. And now you can experience it with our SEA based wizard in our all new\nVOD Encoding Home\n. Try it out!\n1.133.0\nAdded\nHLG output which is backward compatible with SDR is now supported: Using\nenableHlgSignaling\nin addition to setting the ARIB STD-B67 color transfer function will perform an ARIB STD-B67 conversion but signal it as BT.2020 10 bit. The ARIB STD-B67 conversion will be signaled in the SEI messages.\n1.132.0\nFixed\nWhen calling the list call for\nDASH\n,\nHLS\nor\nSmooth\nmanifests with an\nencodingId\n,\nlimit\nand\noffset\nparameters were ignored.\n1.131.0\nFixed\nFixed wrong and invalid return value\nloudnessControl.dialnorm=0\nfor\nDolby Digital\nand\nDolby Digital Plus\nAudio Configurations when not specifying a\ndialnorm\nproperty at all in the\nloudnessControl\nobject during codec configuration resource creation. This behavior caused the Bitmovin Python SDK to fail parsing the response from the Bitmovin API.\n1.130.0\nChanged\nInternal stability improvements.\nThe Bitmovin Content Delivery Network (CDN) removes the need to set up your own output storage, automatically distributing your content around the globe and enabling fast, efficient delivery to all your viewers. Available\nnow in beta\n, try it today.\n1.129.0\nChanged\nInternal stability improvements.\nStreams, our all-in-one solution combining Bitmovin’s encoding, player and analytics together with built-in storage and pre-configured CDN is now available, with pay as you go\npricing\n. Drag-and-drop your videos for Per-Title optimized ABR encoding, an embeddable player with QoE analytics and delivery to a worldwide audience. Read more\nhere\n.\n1.128.0\nRemoved\nExperimental feature of object detection\n1.127.0\nFixed\nEncodings containing 0-Duration-Trimming(s) on\nvideo concatenation configurations\ncould have stalled.\n1.126.0\nFixed\nFixed an error that caused DASH Manifests with ManifestGenerator=\nV2\nto fail when including DTS:X audio with 5.1.4 channel layout.\n1.125.0\nAdded\ninput color primaries\nand the\ninput color transfer\nfor an encoding can now be overridden (\nH262\n,\nH264\n,\nH265\n,\nVP8\n,\nVP9\n,\nAV1\n)\n1.124.0\nFixed\nDASH manifests V2: ContentProtections were being written twice into the same AdaptationSet under particular conditions.\n1.123.0\nChanged\nInternal stability improvements.\n... and while we are working on the next release, have you already seen how Bitmovin’s Simple Encoding API combines our Per-Title and 3-pass optimizations together with HLS and DASH packaging in a single API call? Read more\nhere\nand give it a try!\n1.122.0\nAdded\nSimple Encoding API: Live Encodings will now automatically shut down 10 minutes after the ingest has stopped\nAdded CDN usage statistics retrieval API:\nthe\nAPI\nprovides an overview of the total usage statistics for a selected time period (both for storage and transfer usages) as well as an individual report of the usages per days.\nmore about it can be found in the\ndocumentation\n1.121.0\nFixed\nFixed a bug that could lead to long request duration and potential timeouts during\nencoding start calls\ndue to issues during input source probing.\n1.120.0\nChanged\nThe\nprofile\nfor\nH264\nand\nH265\nvideo output does not need to be specified anymore when a dynamic range format is set. A default value for the profile is used based on the given dynamic range format.\nFixed\nWhen a dynamic range format was specified, color config values set via the API could have been ignored\nWhen a dynamic range format was specified, default values for color config settings could have not applied correctly\n1.119.0\nChanged\nStatistics for\nSRT Live Stream Input\nare now returned sorted by date with latest entries being first.\nAdded a check to verify that\nlive stream conditions\nare not added to encodings using a version older than 2.120.0.\n1.118.0\nFixed\nIn DASH Manifest V2 via\nstart encoding call\n, fixed the calculation of the\npresentationTimeOffset\nDASH attribute for Segment Template and Segment Timeline Representations in DASH ManifestGenerator.V2 when\nstartSegmentNumber\nor\nstartKeyframeId\nis set in order to keep audio and video in sync.\n1.117.0\nChanged\nImproved validation and applying of defaults for\nselectionMode\nand\nposition\n:\nDvbSubtitleInputStream: \"selectionMode\" now defaults to AUTO. \"position\" defaults to 0 if the chosen selectionMode requires a position. Specifying a position is not allowed when using selecionMode AUTO.\nIngestInputStream and Stream.inputStreams: Only default \"position\" to 0 when \"selectionMode\" is not AUTO.\n1.116.0\nAdded\nAdded new\nOutput Paths endpoint\nwhich makes it possible to retrieve all of the manifests output paths for an encoding.\n1.115.0\nAdded\nStream Conditions\ncan now also be applied to\nLive Encodings\n.\n1.114.0\nAdded\nManifest Generator V2 now supports Dolby Vision encodings.\nFixed\nActivating PSNR quality metrics for a stream\nno longer results in a 404 error when using the API key of a tenant user.\nOverriding DynamicRange Format values with specific settings now works as specified in the OpenAPI documentation.\n1.113.0\nAdded\nBitmovin CDN storage\nis now supported as output for Live encodings via\nSimple Encoding API\nHLS manifest creation with ManifestGenerator.V2\nnow fails with appropriate error messages in the following cases:\nIf an empty streamId is given for a\nStream\n/\nMediaInfo\n, but there are more than one streams present in the muxing.\nIf a streamId is given for a\nStream\n/\nMediaInfo\nthat is not contained in the muxing whose id is given.\nThe cloud region\nAUTO\nis now correctly resolved when used for\nEncodings\nwith\nCDN output\n.\nChanged\nSimple Encoding Livestreams\ncan now automatically detect the aspect ratio of the input.\nFixed\nFixed a bug where the\nlive Encoding details\ncould not be retrieved if the stream key was not set.\n1.112.0\nAdded\nManifest Generator V2\nnow also supports HLS manifest generation from multiple encodings. The same known limitations as for single encoding apply:\nLIVE-2-VOD is not supported yet\nDolby Vision is not supported by V2 yet\nFixed\nDefault Manifests now support writing the manifest files to multiple outputs.\nFixed a bug where the creation of a\nScte35\nwould sometimes result in a 500 error.\n1.111.0\nAdded\nBitmovin CDN storage\nis now available for VOD(beta). The Bitmovin Content Delivery Network (CDN) helps setting up your workflows faster by simplifying the distribution of your content. It's easy to use and removes the need to set up your own output storage. Your content will be automatically distributed around the globe, enabling fast and efficient delivery to all your viewers without any additional effort.\nNote that for your first use, it may take up to 5 minutes after encoding completes before CDN distribution is ready. Subsequent videos will be available as soon as encoding has finished.\nThis beta release is available for organizations who signed up for a TRIAL after 2022-04-05 or subscribed to a Starter or PAYG (pay as you go) plan after that date.\nFurther information can be found\nhere\n.\nFixed\nFixed\nprogressive MP4\nworkflow for\nDolby Vision\nin combination with\nAAC\naudio streams.\n1.110.0\nFixed\nIdentified and fixed a very rare issue which could potentially lead to less parallelity processing batch jobs when the audio merge feature was used\n1.109.0\nAdded\nManifest Generator V2\nis now available for single-encoding HLS manifests by specifying\nmanifestGenerator=V2\nin the\nHLS Start-Manifest Request\n. The minimum encoder version for that feature is\n2.111.0\n. Differences to the\nLEGACY\ngenerator include:\nAdditions\nV2 supports assoc-language and the characteristics which are not supported by LEGACY.\nImprovements\nThe order of the attributes inside the elements can be different. They are now ordered in a stable way. We remain spec compliant in this point.\nI-FramePlaylists referencing a muxing that does not support I-FramePlaylists will cause the manifest to fail in contrary to the LEGACY implementation that writes an incorrect I-FramePlaylist file.\nThe target duration is not updated to the highest occurring value in all media playlists in some cases by Legacy HLS manifest generation. V2 now handles this more correctly.\nThe codec value of Variant Streams for ProgressiveTsMuxings and TsMuxings which contain a video and an audio stream is now more correct. In the Legacy generator it looks like this\navc1.4D401E,ac-3,avc1.4D401E,ac-3\nand in V2 we only write it once e.g.\navc1.4D401E,ac-3\n.\nFor MediaInfos/StreamInfos where the provided streamId does not belong to the muxing, the manifestGenerator V2 sets the segmentDurations more accurately than the LEGACY one. We might fail fast for cases like this in the future.\nIf multiple ClosedCaptionMediaInfos are specified for a single manifest, V2 will only add a single EXT-X-MEDIA tag for each of them instead of adding duplicate entries like legacy does.\nKnown Limitations\nLIVE-2-VOD is not supported yet.\nDolby Vision does not work with V2 at the moment.\nSimple Encoding Api\nA name can now be set for Simple Live Encodings.\n1.108.0\nAdded\nImproved the internal\nencoding cloud region\nselection for continental regions like\nASIA\n,\nEUROPE\netc. to take the input and output cloud providers and their regions into consideration. This will minimize potential egress costs.\n1.107.0\nAdded\nThe Simple Encoding API now also supports live encodings. Further information can be found\nhere\n.\n1.106.0\nFixed\nTime-based Trimming\nconfigurations with a negative duration caused encodings to stall. Such configs are now disallowed and will result in a 400 Bad Request response during creation.\n1.105.0\nAdded\nFor\nAV1 encodings\nthese presets are now available\nVOD_QUALITY - Higher quality\nVOD_STANDARD - Default quality and speed\nVOD_SPEED - Faster encoding\nAdded validations for time based trimming input streams: The encoding now fails fast if the (rounded) duration of all the time based trimming input streams is 0 or negative.\nThe Manifest Generator V2 now supports generating SMOOTH manifests containing more than 1 encoding.\n1.104.0\nAdded\nAdded support for Per-Title\nAV1\nencodings to the\nSimple Encoding API\n.\nFixed\nThe H264 presets VOD_STANDARD and VOD_QUALITY now use the H264 profile 'MAIN' instead of 'HIGH'. See documentation\nhere\n.\n1.103.0\nAdded\nManifest Generator V2 is now available for single-encoding SMOOTH manifests by specifying\nmanifestGenerator=V2\nin the SMOOTH Start-Manifest Request. The minimum encoder version for that feature is\n1.108.0\n. Differences to the LEGACY generator include:\nV2 now correctly sets the \"QualityLevels\" attribute in the StreamIndex elements to the count of QualityLevel elements present.\nV2 now correctly sets the \"Index\" attribute in the QualityLevel elements by increasing the number with each element.\nFixed\nFixed info messages for GCE infratstructure creation.\n1.102.0\nAdded\nCloud Region\nAWS_EU_NORTH_1\n(Stockholm) is now available for encoder version\n2.107.0\nand newer.\nSeveral performance improvements to the HLS manifest generation via the\nHLS Manifest Start call\n. HLS manifest generation with many renditions is now up to 5 times faster.\nFixed\nHLS Manifest generation failed when the encoding containted an MP3 Audio Muxing even if the HLS Manifest did not reference the MP3 Muxing.\n1.101.0\nAdded\nSeveral performance improvements to the DASH manifest generation via the\nDash Manifest Start call\n. DASH manifest generation with many renditions is now up to 20 times faster.\nFixed\nUpdated the documentation for the\ndaily statistics\ncalls to make it clear that offset and limit parameters are not used.\n1.100.0\nAdded\nAdded PTSAlignMode to\nfMP4 muxings\n. Setting it to\nALIGN_ZERO_NEGATIVE_CTO\nwill shift the first composition time offset (CTO) to 0. If B-frames are used, some CTOs will be negative. Therefore, TrackRun (trun) version 1 atoms are being used. This can only be set for H264 and H265 streams.\nFixed\nFixed a bug where Per-Title analysis with\nNextGuard watermarking\nand fractional framerates failed",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoder-21500-21990",
    "title": "Encoder 2.150.0 - 2.199.0",
    "text": "2.199.0\nReleased 2024-06-04\nChanged\nInternal stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.198.0\nReleased 2024-05-28\nFixed\nHLS manifests used as HTTP or HTTPS input were sometimes not detected correctly due to additional query parameters in the URL.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.197.0\nReleased 2024-05-14\nFixed\nEncodings having incompatible input and output color configuration were sometimes stalling. These will now properly fail with an explicit error message.\nFixed an issue that caused delays for up to one minute in firing\nLive Input Stream Changed webhooks\n.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.196.0\nReleased 2024-04-30\nAdded\nImplemented usage of r-attribute in segment references of SegmentTimeline Dash manifests generated with manifest generator V2. As a result, repeated subsequent segment references with the same duration will be removed and replaced with an r-attribute to reduce the size of the Dash manifest.\nEncoding name is now part of the Triggered event messages (for both LIVE and VOD).\nFixed\nFixed wrong VIDEO-RANGE attribute in HLS manifests created with manifest generator V2 for encodings performing a conversion from Dolby Vision to SDR.\nFixed CacheControl and MimeType for live manifests uploaded to an Azure Output.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.195.0\nReleased 2024-04-16\nAdded\nInvalid or unspecified color properties (color space, color range, color transfer, color primaries) would trigger a warning message.\nFixed\nTranscoding from a long duration input with Dolby E could result in a failed encoding. This has been fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.194.0\nReleased 2024-04-03\nFixed\nFixed an issue where encodings using progressive mov muxings would fail when the output file contained special characters (e.g. spaces).\nImproved scheduling of 8k Encodings, which were previously getting stalled sometimes.\nEncodings with fractional segment lengths, using keyframe feature, could sometimes result in missing subtitle segments. This has been fixed now.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.193.1\nReleased 2024-03-20\nAdded\nVOD Encoder now supports transcoding\nDolby E\n(as\ninput\n) to Dolby Digital/Dolby Digital Plus (as output). Current support comes with following limitations:\nDolby E must be in 2 individual mono PCM channels in the input. One stereo channel in the input is not supported.\nOnly the first program can be transcoded\nPrograms with more than 6 channels are not supported.\nAny other usage of the API could result in incorrect output. If you intend to use Dolby E as input, please contact support.\nFixed\nOn setting handleVariableInputFps option to true, for some specific contents (where the number of consecutive B-Frames suddenly increased after a few hundred frames), the output was longer than the input. This has been fixed now.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.192.0\nReleased 2024-03-12\nAdded\nAll LIVE Encoding customers, using Managed Cloud, can now schedule their encodings on\nAkamai Connected Cloud\n, using the live encoding\nHD option\n.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.191.0\nReleased 2024-02-28\nAdded\nSupport for variable segment durations in fMp4 IFramePlaylist for HLS manifests created with manifest generator V2.\nChanged\nUpgraded Zixi Edge Compute (ZEC) to v16.11 for the Zixi Input for Live Encoding.\nFixed\nFor the manifest generator V2, the HLS version in fMp4 IFramePlaylist is now set to the same version as the HLS media manifest. Before, the HLS version of the IFramePlaylist was always 4, which caused problems for features that are not supported with HLS version 4.\nA regression introduced in 2.175.0 was producing darker HLG output when using HLG signalling, used for backward-compatible HLG. Fixed in the current version.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.190.0\nReleased 2024-02-06\nFixed\nIn case the audio track is much shorter than the video and\nRESYNC_AT_START_AND_END\nis selected in the\nStartEncodingRequest\n, the encoding is not slowing down anymore.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.189.0\nReleased 2024-01-31\nFixed\nA faulty initial input connection on a reconnect could block the stream from writing output segments and regenerating itself. This has been fixed by forcing a reconnect if corrupt metadata was probed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.188.0\nReleased 2024-01-23\nFixed\nFixed a regression introduced in\n2.178.0\nwhere an encoding may stall if a progressive MP4 muxing is configured where the output file extension is not \".mp4\".\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.187.0\nReleased 2024-01-12\nAdded\nAdded support for\nAV1\nwith\nfMP4\nin\nHLS\nmanifests.\nFairPlay\nis also supported for this combination.\nChanged\nAll progressive and fragmented MP4 muxings with an HEVC stream now have the codec tag name\nhvc1\n(Apple recommendation) due to reported playback issues on Apple devices and the Quicktime player in the case of the codec tag name being\nhev1\n. Consequently, the codec tag names of HEVC streams in progressive MP4 muxings have been changed from\nhev1\nto\nhvc1\nin the following two cases:\nWhen down-converting Dolby Vision to HDR10 or SDR\nWhen the muxing contains a Dolby Atmos stream\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.186.0\nReleased 2024-01-09\nChanged\nThe Frame-rate change logic has been improved. This implies that starting with this version every Frame-rate conversion will result in constant output segment length behaviour.\nThe old behaviour where in case of fractional frame rate changes from e.g. 23.976 fps to 24.0 fps the output segment length in frames for 4 seconds segments was alternating between 96 and 97 frames is removed.\nThe new rate change behaviour introduces a new fail fast error if the encoding configuration is not compliant. Read more about the scenarios that will result in this fail-fast error\nhere\n.\nFixed\nEncodings started using an\nAV1 codec configuration\nwithout specifying a bitrate will now fail to be started as it eventually resulted in a transcoding error.\nDifferent trimming configurations for multiple audio streams were not applied correctly. This has been fixed.\nPerform interlaced-aware scaling from interlaced input to interlaced output.\nImproved output quality of Dolby Vision encodes at lower bitrates.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.185.0\nReleased 2023-12-12\nAdded\nIPCM audio as input is now\nsupported\n.\nFixed\nFor JPEG2000 4K input with exactly 1 output rendition (1080p or higher) resulted in failed encodings. This has been fixed now.\nFixed encodings with Dolby Vision JPEG2000 inputs that failed with the error \"\nColor conversion between YUV formats where only the color range changes is not supported\n\". The error was triggered by an unsuccessful detection of the input color properties.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.184.1\nReleased 2023-11-30\nChanged\nInternal stability improvements.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.184.0\nReleased 2023-11-28\nAdded\nSupport for a new H264 profile\nHIGH422\n. This can be configured when creating an\nh264 codec configuration\n.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.183.0\nReleased 2023-11-14\nFixed\nWhen using h264 hardware acceleration of Apple Silicon chips and ingesting via OBS with 60 fps there was no output written for Live streams.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.182.0\nReleased 2023-10-31\nFixed\nImproved handling of network failures that could lead to failed encodings.\nFixed\nCustom Tags with PositionMode KEYFRAME\nnot working properly for encodings with FPS change since version Encoder v2.179.0.\nDeinterlace filter (enhanced or normal) created with\nautoEnable\nvalue not set to\nALWAYS_ON\n, sometimes resulted in activating the deinterlace filter for non-interlaced inputs.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.181.0\nReleased 2023-10-17\nFixed\nFixed a bug that caused errors when SRT is used with backup ingest streams that are coming from a different IP address.\nSignificantly improved the performance of generating Live HLS manifests with longer timeshift and DVR windows.\nThe\nAudioVolume\nfilter is now working for Live Encodings.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.180.0\nReleased 2023-10-03\nAdded\nSupport for new Azure region:\nUS_WEST\nFixed\nFixed an AV out of sync issue with a specific combination of frame rate conversion and segment length.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.179.0\nReleased 2023-09-26\nAdded\nCreation of\nkeyframes\nnow works with Smart Chunking.\nAdded support of keyframes creation for Dolby Vision encodings.\nSupport for new Azure regions:\nASIA_EAST\n,\nASIA_SOUTHEAST\n,\nBRAZIL_SOUTH\n,\nCANADA_CENTRAL\n,\nFRANCE_CENTRAL\n,\nINDIA_CENTRAL\n,\nINDIA_SOUTH\n,\nJAPAN_EAST\n,\nJAPAN_WEST\n,\nKOREA_CENTRAL\n,\nUS_CENTRAL\n,\nUS_EAST2\n,\nUS_SOUTH_CENTRAL\nFixed\nFixed a bug where encodings that used progressive subtitles together with segmented subtitles were outputting wrong progressive subtitles.\nFixed performance issues in HLS manifest generation that caused delays in live encodings with a big timeshift.\nChanged\nImproved startup time for AWS regions, leading to shorter queuing times.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.178.0\nReleased 2023-09-19\nFixed\nSRT input was getting disconnected after few seconds of the first connection.\nConnection failures, sometimes happening due to backup SRT inputs, are now fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nEncoding may stall with this version if a progressive MP4 muxing is configured where the output file extension is not \".mp4\". Please use\n2.188.0\nor more recent version.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.177.0\nReleased 2023-09-05\nFixed\nFor multi-pass encodings with prores or j2k inputs, overriding the colorConfig object attached to a codec configuration (for example\nhere\nfor h264) was not working as expected.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.176.0\nReleased 2023-08-30\nFixed\nFixed a bug where some specific mov inputs resulted in failed encoding.\nFurther improved stability for long-running live encodings.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.175.1\nReleased 2023-08-17\nFixed\nFixed timescale for WebVtt chunked text muxings in DASH manifests. Previously, Timescale was set to same value as for video representation, even when the duration of WebVtt segments didn't match with the timescale.\nFix codec string in DASH manifests for WebVtt this was set to 'vtt' now it is set correctly to 'wvtt'.\nEncodings with an input that has change in color configuration (at frame level) would have failed. This color conversion is now handled.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nUsing HLG signalling, used for backward-compatible HLG, could result in darker output. Please use\n2.191.0\nor more recent version.\n2.174.0\nReleased 2023-07-25\nFixed\nImproved stability for long-running live encodings.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.173.0\nReleased 2023-07-18\nFixed\nA regression that was introduced in 2.164.0 leading to missing analysis details for streams in cases when pertitle and audio was configured.\nFixed an issue with VP9 encodings where it could happen that a level violation was triggered in the encoder for SINGLE_PASS encodings. This was fixed by disabling alternate reference frames for all VP8/VP9 SINGLE_PASS encodings.\nQueueing time considerably reduced when using Pre-warmed Encoder Pools.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.172.0\nReleased 2023-07-11\nAdded\nInternal stability improvements.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.171.0\nReleased 2023-07-05\nAdded\nImproved stability of Dolby Atmos audio-only encodings, which could have failed in the case the input duration was a multiple of the defined segment duration.\nFixed\nFixed a buffer overflow in Audio Service when using Akamai MSL/Netstorage output.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.170.0\nReleased 2023-06-20\nFixed\nFor Live Encodings, when using\nAesEncryptionDrm\nwith a specified\nkeyFileUri\n, the keyFile was not properly uploaded to the designated output. This has been fixed.\nImproved resilience in case of intermittent infrastructure hiccups. If recovery is not possible, the encoding would eventually fail rather than stalling.\nImprove s3 download speed in case the bucket location is not set for the encoding or can't be queried, which can be caused by a missing\ns3:GetBucketLocation\npermission.\nAdded\nThe behaviour of\nisDefault\nin hls manifest's mediainfo was changed. Setting a value of\nisDefault = false\nwill now be flagged in manifests as\nDEFAULT=NO\nwhen using manifest V2.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.169.0\nReleased 2023-06-06\nAdded\nInternal stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.168.0\nReleased 2023-06-01\nFixed\nFixed a bug which could cause Dolby Vision encodings to fail on AWS in case they had large Metadata files and segmented output configured.\nFixed a bug which could cause Dolby Vision encodings from JPEG2000 or high-resolution ProRes mezzanine files to stall in case segmented output was configured.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.167.0\nReleased 2023-05-23\nFixed\nUnder rare conditions, inputs with negative timestamps at the start (e.g. due to B-frames before the first I-frame in presentation order) could result in audio and video being out of sync.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.166.0\nReleased 2023-05-16\nFixed\nIn cases where Dolby Vision with segmented muxing is configured,\nMuxingInformation\nwas not calculated correctly for any additional audio or subtitle stream. This was fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.165.0 (LTS)\nReleased 2023-05-09\nFixed\nThe issue impacting long running LIVE/VOD Encodings, identified in the previous release, is fixed.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.164.0\nReleased 2023-05-03\nAdded\nautoLevelSetup\n: property for video configurations (\nh264\n,\nh265\n,\nav1\n,\nvp9\n) to enable/disable the automatic calculation of level, maximum bitrate, and buffer size based on the lowest level that satisfies maximum property values for picture resolution, frame rate, and bitrate. In the case the target level is set explicitly, the maximum bitrate and buffer size are calculated based on the defined level. The\nautoLevelSetup\nis enabled by default and is automatically disabled if the maximum bitrate or buffer size is set.\nFixed\nPerTitle Algorithm was sometimes losing a configured FIXED_RESOLUTION_AND_BITRATE when the configuration contained two FIXED_RESOLUTION_AND_BITRATE with the same resolution.\nFixed rare failures for encodings using three-pass with specific content.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nLong running LIVE/VOD Encodings could potentially fail. We highly recommend using\nthe STABLE version\nor the higher versions (from 2.165.0 onwards) for such encodings.\n2.163.0\nReleased 2023-04-25\nChanged\nIn addition to the\nencoding start\ncall, the limit for the\nMaximum VOD encodings queued\nis now also checked for the\nencoding create\ncall.\nDebug\nmessages will not be returned by the\nEncoding Status API\nanymore.\nAdditional internal stability improvements.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.162.0\nReleased 2023-04-18\nFixed\nFixed a bug where using a closed caption media info lead to an error in HLS manifests.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.161.0\nReleased 2023-04-12\nChanged\nInternal stability improvements.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.160.0\nReleased 2023-04-04\nAdded\nSmart Chunking is now enabled for VP9.\nFixed\nFixed a bug where paths in manifests were not correctly updated for live encodings after automatically detecting the output resolution.\nZixi\nand\nSRT\ningest systems have been completely overhauled, resulting in improved stability and error handling.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.159.0\nReleased 2023-03-29\nAdded\nFor DASH manifests, when using\nprofile: ON_DEMAND\nand\nmanifestGenerator: V2\n, the\nduration\nproperty of Periods is now set when the manifest has multiple Periods.\nWebM Muxing now supports SPEKE PlayReady encryption for VP9 video streams.\nFixed\nFixed a bug that caused the LIVE input probing to fail in some situations.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.158.0\nReleased 2023-03-14\nAdded\nWebM Muxing now supports CENC PlayReady encryption for VP9 video streams using fMP4\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.157.0\nReleased 2023-03-07\nAdded\nSmart Chunking enabled in Dolby Vision to HDR10 and SDR workflows with frame rate changes.\nChanged\nInternal stability improvements.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.156.0\nReleased 2023-02-28\nAdded\nSmart Chunking is enabled for Dolby Vision workflows without frame rate changes.\nChanged\nFor HDR manifests the\nManifestGenerator.V2\ndoes not write the\nprofile\nattribute on AdaptationSet level anymore.\nFixed\nImproved the turnaround times for encodings with 4K inputs shorter than 16 seconds.\nCreation of a GcsInput or GcsOutput was failing randomly with \"Unable to query location of the bucket\" if a bucket with the same name existed on AWS S3. The same could also happen for S3Input or S3Output for a bucket with the same name on GCS.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.155.0\nReleased 2023-02-23\nFixed\nStability improvements for Dolby Vision workflows with ProRes inputs\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nCreating a GcsInput or GcsOutput might fail randomly with \"Unable to query location of the bucket\" if a bucket with the same name exists on AWS S3. The same may happen for S3Input or S3Output and a bucket with the same name on GCS.\n2.154.0\nReleased 2023-02-14\nAdded\nSmart Chunking\nWe completely overhauled the general bitrate distribution in our outputs to prevent quality drops at segment borders thus leading to a smoother playback experience.\nThis optimization is automatically applied to all encodings except when using the following features which will be added over the course of the next releases:\nDolby Vision mezzanine input\nConcatenation\nset Keyframe\nVP9\nor\nAV1\noutput codec\nPSNR\nfeature\nChanged\nCodec settings for VP9 will now be automatically optimized for Live Encodings.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\nCreating a GcsInput or GcsOutput might fail randomly with \"Unable to query location of the bucket\" if a bucket with the same name exists on AWS S3. The same may happen for S3Input or S3Output and a bucket with the same name on GCS.\n2.153.0\nReleased 2023-02-07\nChanged\nWe completely overhauled our muxing engine for\nfMP4 output\nwith codecs H.264, AAC, HE-AAC, and HE-AACv2. For more details about the changes,\nread this article\n.\nFixed\nManifest segment durations were calculated inaccurately when a concatenation contained fractional and non-fractional framerate changes. Now the segment duration calculation is based on the number of encoded frames.\nCorrected an issue which lead to Nielsen audio watermarking with multiple output video renditions to not be correctly applied under certain conditions.\nKnown Issues\nLive Encodings using fMP4 or Webm muxings could potentially fail after running longer than appr. 6 hours. We highly recommend using\nthe STABLE version\nor the higher versions (from 2.154.0 onwards) for such encodings.\nS3 role-based output for segmented muxings: No upload verification available.\n2.152.0\nReleased 2023-01-31\nChanged\nInternal stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.151.0\nReleased 2023-01-24\nAdded\nLIVE_STANDARD\npreset is now available for\nH265\nCodec Configuration.\nFixed\nConfiguring a lot of muxings sometimes resulted in failed encoding.\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.\n2.150.0\nReleased 2023-01-17\nChanged\nInternal stability improvements\nKnown Issues\nS3 role-based output for segmented muxings: No upload verification available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/preview-and-programme-dvr-intervals-with-timeshift",
    "title": "Preview and Programme DVR Intervals",
    "text": "Overview\nThese two workflows can be built leveraging two configuration options both available as of\nversion 1.195.0\nof our API:\nSegment naming obfuscation pattern (for preview)\nThe API endpoint\nRestLiveManifestTimeshift\n(for preview and Programme intervals)\nSegment name obfuscation\nThis feature can be enabled by configuring the segmentNamingTemplate property of a given Muxing in our API.\nExample Java code:\nmuxing.setSegmentNamingTemplate(\"segment_%number%_{segment_rand_chars:4}.ts\");\n%number% is required for the sequence number of the segment\n{segment_rand_chars:4} is the placeholder for the random characters in the segment name. Each segment will have different random characters. 4 is the number of random characters inserted for the placeholder (the maximum and default is 32).\n📘\nUseful to know\nThe placeholder {rand_chars:4} is also possible, but it will assign the same random characters for all segments. This is useful to prevent overwritten segments when the same output folder is used.\nReset Live Manifest Timeshift\nWhen issued, this option will removes older segments from live manifests. This resets or reduces the\ntime-shift (DVR) window\n.\nThe\nresidualPeriod\nvalue is configurable and will determines how many seconds will remain in the time-shift window.\nThe original time-shift window does not change. Newer segments will be added and not removed from the\nmanifest until the original time-shift duration is reached again.\n❗️\nBe informed\nCurrently, only HLS live manifests are supported.\nPreview workflow\nTo implement a “Preview Feature” for the Live Encoder, you will need to use both options described above. This workflow allows the first section of a live event stream to be published to a small subset of an audience, this could be to perform a signal line up and ensure all video and audio signals are levelled correctly and free of error. In this workflow customers would not want the main audience to have access to the preview content.\nSingle Manifest Concept\nWith this approach it is possible to use the same manifest for both the preview and live event phases. To\ngo live\nthere is a new ResetLiveManifestTimeShift API call, that removes all the media segments from the manifest so that after sharing the manifest with viewers after the API call, the segments that are part of the preview phase are not present anymore.\nUsually, segments are numbered and people can easily guess the URLs of segments in the preview phase (e.g.: segment_0.ts, segment_1.ts, .., segment_100.ts). To prevent this “guessing” of preview phase segments we added the segment namining obfuscation feature. Using this feature adds a random text as part of each segment name and it’s almost impossible to guess the segment URLs give a certain segment number. (e.g. seg_e466d03bce_0.ts, seg_bd5558d871_1.ts, .. seg_55d32aaec7_25.ts).\nHow to set-up the Live Encoding\nFirst, make sure the Segment Naming Obfuscation template is used in the Live Encoding configuration.\nThe following sequence diagram illustrates the whole “Preview” workflow:\nThe Streamer starts ingesting RTMP, SRT, or Zixi into the Live Encoder\nThe “Preview Phase” starts and the Encoder starts uploading the manifest files and media segments\nTo “Go Live” the Streamer calls the ResetLiveManifestTimeShift endpoint\nThe Encoder removes all old segments from the HLS manifest files\nThe Streamer can share the manifest URL with the Viewers\nThe Viewers can start watching the manifest and don’t have access to the media segments that were part of the “Preview Phase”. Thanks to the segment naming obfuscation, the URLs cannot be guessed by looking at the segment sequence numbers.\nProgramme DVR Intervals\nFor a 24/7 live linear channel, you may wish to use the ResetLiveManifestTimeShift endpoint to ensure that viewers are only able to view a DVR that relates to the current live programme, with previous programmes perhaps not available for Catch Up or only available as VOD at a certain later moment in time.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/b5361bc-4a6bb0e8-4b8b-4776-8467-46975f33f1e4.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/configuring-an-aws-s3-bucket-for-content-playback-testing",
    "title": "Configuring an AWS S3 Bucket for Content Playback Testing",
    "text": "When you use an S3 bucket as Output for your encodings, they will not automatically allow you to test streaming your encoded content from them. The default security model with S3 does not allow for this.\nTo enable this model, you will need to do the following:\nAllow Public Access ACLs\nFor creating new buckets we have this tutorial:\nHow to create an S3 Encoding Input or Output with the Bitmovin API\nFor existing buckets:\nGo to the\nPermissions\ntab of the S3 Bucket a. e.g.:\nhttps://s3.console.aws.amazon.com/s3/buckets/\n?tab=permissions\nIf it says\nAccess: Bucket and objects not public\nour Encoder cannot set objects to have the PUBLIC\nACCESS ACL.\na. In\nBlock public access (bucket settings)\npush the\nEdit\nbutton\nb. uncheck 🔳\nBlock\nall\n_\npublic access.\nc. Hit\nSave changes\nd. In the\nPermissions overview,\nit should say\nAccess: Objects can be public\nWhen you create the bucket, disable the bucket settings that block public access, which is set by default on new buckets. If you use the AWS Console, they are\nBlock new public ACLs and uploading public objects,\nand\nRemove public access granted through public ACLs\n. See the\nAWS Documentation on Block Public Access\nfor details.\nIf you leave those enabled, you will need to ensure that your scripts set ACL permissions to PRIVATE on objects output to this bucket (for example muxings, manifests, etc.), or your encodings will fail.\nNote: Encodings started in the\nhttps://bitmovin.com/dashboard/\nare configured to set the PUBLIC_ACCESS ACL on output files. Allowing Public Access ACLs in the bucket is required for these Encodings to work.\nCORS configuration\nTo allow players to request content for streaming from your S3 bucket, you will also need to allow origin access with a CORS configuration.\nGo to the\nPermissions\ntab of the S3 Bucket\na. e.g.:\nhttps://s3.console.aws.amazon.com/s3/buckets/\nbucket-name\n?tab=permissions\nScroll down until\nCross-origin resource sharing (CORS)\nand hit the\nEdit\nButton next to it\na. e.g.:\nhttps://s3.console.aws.amazon.com/s3/buckets/\nbucket-name\n/property/cors/edit\nNow you can paste or edit the CORS configuration based on your needs.\nThe following is a good default policy that provides unrestricted access for streaming.\nJSON\n[\n    {\n        \"AllowedHeaders\": [\n            \"Authorization\"\n        ],\n        \"AllowedMethods\": [\n            \"GET\",\n            \"HEAD\"\n        ],\n        \"AllowedOrigins\": [\n            \"*\"\n        ],\n        \"ExposeHeaders\": [],\n        \"MaxAgeSeconds\": 3000\n    }\n]\nFor more information on how to enable CORS on S3 buckets, see the\nAWS Documentation\nTroubleshooting\nTo troubleshoot upload issues to your AWS S3 bucket are explained\nhere\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/managing-your-subscription",
    "title": "Managing Your Subscription",
    "text": "After you sign up, your account is running a trial subscription which lasts for 30 days by default, beginning with your registration date. After that time, it expires automatically. In order to continuously use any of our services a paid subscription is required.\nManaging your subscriptions\nOnce you've completed your\nPayment and Billing\ninformation, you can subscribe to a plan via\nPlans and Pricing\n.\nOnce you've subscribed, you can see all your active subscriptions:\n💡\nTip\nGet insights into your usage, billing period & renewal dates and more by clicking on the\nSee details\nbutton below the product subscription.\nThis will redirect you to a plan overview page:\nNeed a custom plan? Let's talk!\nFor more advanced requirements, whether volume or complexity,\ncontact us\nfor info about our Professional and Enterprise plans.\nHow do I cancel a subscription?\nYou can cancel a subscription for each of our services at any time via\nPlans and Pricing\n. The cancellation will be scheduled immediately, and executed by the end of your current billing period, which is typically at the beginning of each month, or at the renewal date of your yearly subscription. In case you exceed the included volume of your subscription until the end of your billing period, a final invoice will be created to charge for the exceeded amount.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/90217a1229d9d4906567f726e65c779941f459407dfc60ba993f5bb97107d280-Screenshot_2024-10-25_at_11.18.06_AM.png",
      "https://files.readme.io/bd7876c6e878f8da52542f627a9cf3c402291babcff86eabdb8f9b48f2ed3a11-Screenshot_2024-10-25_at_11.20.44_AM.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/bitmovin-encoder-lifecycle",
    "title": "Bitmovin Encoder Lifecycle",
    "text": "Overview\nThe feature set of the Bitmovin Encoder naturally evolves over time, including additional features, fixes and performance improvements in newer releases to increase efficiency. Upgrading to new releases is however possible as soon as a new release is available, and Bitmovin recommends the use of the Encoder version currently in Stable stage for production environments.\nWhen the version you are currently using gets deprecated, you will need to update to a newer version, as we will make versions unavailable when reaching their end of life date. Thus, users must ensure that they use a newer version of the Bitmovin Encoder before the end of life date, in order to guarantee uninterrupted functionality.\nPlease note that this document is referring to the\nEncoder\nand not the Bitmovin REST API. Release processes and cycles are different for\nREST API Services\n.\nEncoder Version Lifecycle\nA release of the Bitmovin Encoder will follow lifecycle stages as described in this table. Note that all releases will not necessarily go through each stage.\nVersion Status\nSchedule\nDescription\nBETA\nEvery new release\nNew releases of the Bitmovin Encoder will be in status\nBETA\nby default.\nBETA\nversions will contain the newest features and updates. Features in Beta versions might be subject to change.\nSTABLE\n(Recommended)\nTypically about every 4 weeks we update the STABLE version\nA\nBETA\nversion eventually becomes a\nStable\nversion. A STABLE version offers the most complete, up-to-date, tested, and stable codebase.\nSTABLE\nversions are intended to be used in production environments.\nSUPPORTED\nWhenever a new release is promoted to STABLE\nA\nSUPPORTED\nversion was\nSTABLE\nonce but got replaced by a newer release of the Bitmovin Encoder. It doesn't contain the latest feature set, and may hold functionality that is outdated.\nFurthermore,\nSUPPORTED\nversions might not offer the most efficient encoding capabilities. If you're using a\nSUPPORTED\nversion, we recommend that you upgrade to the\nSTABLE\nversion, as\nSUPPORTED\nversions will ultimately be deprecated.\nLONG TERM SUPPORT (LTS)\nOnce a year\nA\nLTS\nversion was\nSTABLE\nat one time and will be supported for 2 years from the date of the release. Support includes security- and vulnerability fixes but does not contain functional enhancements (features) or non-security or non-vulnerability related fixes. Enterprise support is required to use this version.\nDEPRECATED\n4 months after release date\nA\nDEPRECATED\nversion is outdated and should not run in production environments. If you're using a\nDEPRECATED\nversion, update to the\nSTABLE\nversion at your earliest convenience but anytime before the end of life date.\nEND OF LIFE\n6 months after release date\nA\nEND OF LIFE\nversion is no longer available (this includes all relevant documentation).\nWhich Encoder Version should I use?\nThe Bitmovin Team recommends to use the\nSTABLE\nversion in most cases. Using this version tag ensures that your video encoding workflows are always benefiting from our latest patches, security fixes, and performance improvements. Furthermore, you won't have to care about upgrading encoder versions by yourself on a regular basis.\nWhat happens if I use an \"End Of Life\" encoder version in my configuration?\nEncodings setup with an encoder version that is no longer available as it reached its\nEnd Of Life\n, it will be overwritten with\nSTABLE\ninstead.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/performance-and-optimization",
    "title": "Reducing Turnaround Times for Short-Form Video Content",
    "text": "Overview\nBitmovin's split-and-stitch cloud architecture enables massive horizontal scale by allowing different parts of a video to be processed simultaneously and then reassembled for playback. This type of workflow works especially well with longer content but needs some adjustments when working with short-form video content of up to 5 minutes in length.\nTo optimize your encoding workflow for short-form video content, some parameters need to be adjusted for shorter turnaround times.\nFor more information on use cases and our evaluations, please read our Blog post about\nGPU Acceleration for Cloud Video Encoding\n.\nAccelerated Mode\nIf you're encoding\nvideos under 5 minutes\n, in a\nManaged Cloud\ndeployment, using\nSTABLE\nEncoder version, you'll automatically experience faster queueing times.\nHere are some things to keep in mind:\nSupported inputs types (for probing the content):\nS3, S3 role based, HTTP, HTTPs, Azure\nSupported regions*:\nAWS:\nEU_WEST_1\n,\nEU_WEST_2\n,\nEU_CENTRAL_1\n,\nAP_NORTHEAST_2\n,\nAP_SOUTHEAST_2\n,\nUS_EAST_1\n,\nUS_WEST_2\n,\nGCP:\nEUROPE_WEST_1\n,\nUS_CENTRAL_1\nAZURE:\nEUROPE_WEST\n,\nUS_EAST2\n,\nCANADA_CENTRAL\n,\nUS_EAST\n,\nUS_WEST\n,\nJAPAN_EAST\n,\nGERMANY_WESTCENTRAL\n,\nEUROPE_NORTH\n,\nUS_WEST2\n,\nAUSTRALIA_EAST\nLadder limitations:\nUp to 1080p\nHardware Encoding:\nIf Hardware Encoding is configured (by specifying \"presetConfiguration\": \"VOD_HARDWARE_SHORTFORM\" - available only for h264 and h265), certain restrictions apply as detailed in this\nguide\n. Currently*, the only region that supports Hardware Encoding is AWS\nEU_WEST_1\n.\nSome FAQs:\nIs there any additional cost?\n- No, using Accelerated Mode does not incur additional costs.\nWhy are my queuing times still high?\n- This could indicate that the encoding did not run in Accelerated Mode. Ensure there are no account limits that conflict with the above requirements.\n*This list of supported regions was last updated on on\n30.08.24\n.\nIf you're interested in using Accelerated mode but the region for your preferred Cloud provider is not listed here, please contact your Bitmovin representative.\n(Dynamic) Pre-Warmed Pools\n📘\nRequirements and known limitations\nPlease refer to\nHow to use Pre-warmed Encoder Pools\nspecific requirements and known limitations.\nWhen you start an encoding, it will first be queued whilst an encoder instance is spun up and configured, as described in\nWhat do the different encodings state mean?\nThe queue time can be a significant portion of the turnaround time, in particular for short source files. In most circumstances, you will be able to reduce that time with the use of pre-warmed encoder pools (described in detail at\nHow to use Pre-warmed Encoder Pools\n). A pre-warmed encoder pool can be\nstatic\nor\ndynamic\nin size, based on your workflow requirements and resource demands.\nMake sure to\nenable hardware-acceleration\nwith the\ngpuEnabled\nproperty for your pool. Otherwise, encodings configured with a\nVOD_HARDWARE_SHORTFORM\ncodec configuration preset are not able to benefit from hardware-acceleration.\nWhat compromise you may have to make\nPre-warmed pools may increase your encoding costs, in particular, if they’re not used with care. Only configure the pools with as many instances as you may reasonably require, and don’t forget to shut them down when not needed to avoid incurring costs.\nCreate and start a Dynamic Pre-Warmed Pool with GPU-enabled\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nPrewarmedEncoderPool pool = new PrewarmedEncoderPool();\n...\npool.setCloudRegion(CloudRegion.AWS_EU_WEST_1);\npool.setDynamicPool(true);\npool.setGpuEnabled(true);\n\ncreatedPool = bitmovinApi.encoding.infrastructure.prewarmedEncoderPools.create(pool);\n\nstartedPool = bitmovinApi.encoding.infrastructure.prewarmedEncoderPools.start(createdPool.getId());\nEncoding Configuration\nThe correct storage and region\nYour Input and Output storage should also be in the same cloud region, and you should configure your encoding to run in that same cloud region. Using hardware-acceleration is\ncurrently limited to AWS\n(\nNVIDIA T4\nGPUs on\nAmazon EC2 G4dn\ninstances).\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nEncoding encoding = new Encoding();\n...\nencoding.setCloudRegion(CloudRegion.AWS_EU_WEST_1);\n...\nencoding = bitmovinApi.encoding.encodings.create(encoding);\nA speed-focused preset: hardware-acceleration\n📘\nRequirements and known limitations\nPlease refer to\nHow to create an Encoding using hardware-acceleration\nspecific requirements and known limitations.\nAll of our codec configurations allow you to use a preset. Some of those presets are using hardware-acceleration.\nH264:\nVoD Hardware Preset Configurations\nH265:\nVoD Hardware Preset Configurations\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nH264VideoConfiguration config = new H264VideoConfiguration();\n...\nconfig.setPresetConfiguration(PresetConfiguration.VOD_HARDWARE_SHORTFORM);\n...\nreturn bitmovinApi.encoding.configurations.video.h264.create(config);\nUse a previously started Pre-Warmed Pool\nAttention: Pre-warmed Pool might take a couple of minutes to be ready. Starting an encoding immediately after the start Pre-Warmed Pool request was sent will probably result in an encoding that will not utilize the Pre-Warmed Pool.\nBitmovin API SDK for Java Example:\n(\nAPI-Reference\n|\nGithub\n)\nJava\nvar scheduling = new Scheduling();\nscheduling.setPrewarmedEncoderPoolIds(Collections.singletonList(startedPool.getId()));\n\nvar startEncodingRequest = new StartEncodingRequest();\nstartEncodingRequest.setScheduling(scheduling);\n\nbitmovinApi.encodings.start(encoding.getId(), startEncodingRequest);",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/enable-usage-reports",
    "title": "Enabling Usage Reports",
    "text": "Leverage\nUsage Reports\nto stay up-to-date about your product usage.\nYou can choose between a\nweekly\nand a\nmonthly usage report\n:\nDisable or enable the periodic reports you want to receive.\nTo send usage reports to additional people, just click on the\nEdit\nbutton next to either the\nMonthly\nor\nWeekly\nreport option and add their email(s):\nAdd multiple email addresses by separating them with commas.\nFinally, click\nSave\n.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/076653c-Screenshot_2022-08-26_at_00.27.58.png",
      "https://files.readme.io/c188e99-Screenshot_2022-08-26_at_00.28.44.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/rest-api-services-11500-11990",
    "title": "REST API Services 1.150.0 - 1.199.0",
    "text": "1.199.0\nReleased 2024-07-16\nAdded\nAdded new flag\nshiftProgressiveMuxingStartPostion\nto\nResetLiveManifestTimeShift\nAPI call. Per default and if this flag is set to true, any progressive MP4Muxing that is configured for the related Live Encoding will not contain the media data that was encoded before this API call was executed. This API is used to exclude the \"preview\" part of a Live Event from live manifests and progressive MP4 outputs.\nS3 role based inputs\nwill now benefit from\naccelerated mode\nin regions that support it.\nChanged\nRemoved support for the experimental encoding history API.\nFixed\nThe order of MPD audio adaptation sets within the manifest, specifically tied to using the 'labels' attribute is now correct regardless of setting the labels attribute.\nsegmentCut\nparameter has been removed from method\nCreate Keyframes\nand will always be interpreted as\ntrue\n.\n1.198.0\nReleased 2024-07-09\nChanged\nAudioPassthroughConfiguration\ncurrently supports\naac\n,\npcm_s16le\nand\npcm_s24le\n. Attempting any other codecs/formats would result in a fail-fast.\n1.197.0\nReleased 2024-06-25\nFixed\nFixed a bug which could lead to Webhooks being delayed.\n1.196.0\nReleased 2024-06-18\nFixed\nCustom tags\nattached to a variant stream having a\nMP4 muxing\nwith a byte range HLS_BYTE_RANGE request as a source will now properly be included in the manifest.\nCustom tags\nattached to a variant stream having an\nIFrame playlist\nconfigured will now properly be included for all types of supported manifests.\nGO API SDK\nFixed an issue where some of properties of the\nDolbyAtmosAudioConfiguration\nwere not correctly defined leading to some inconsistencies in the Go SDK\n1.195.0\nReleased 2024-06-11\nAdded\nAdded new\nResetLiveManifestTimeShift\nendpoint to remove segments from HLS manifests. This feature can be used to \"go live\" for Live Events.\nAdded a new placeholder type for the\nsegmentNamingTemplate\nproperty of certain Muxing types.\n{segment_rand_chars:x}\nwill be replaced by a random alphanumeric string of length x (default 32) for each segment. This is intended to avoid guessing segment URLs by replacing segment numbers.\n1.194.0\nReleased 2024-06-04\nFixed\nFixed an issue where for Live2Vod HLS manifests the subtitle playlist had segments with\n#EXTINF:0.0\n. The segments now have the correct duration.\nAdded beta support for encoding on Oracle Cloud Infrastructure (OCI). The Oracle Cloud integration still undergoing testing. Some issues or limitations may be present.\n1.193.0\nReleased 2024-05-28\nAdded\nAzure regions UK_SOUTH and NORTH_CENTRAL_US\nValidation that prevents creating invalid textfilters without text and timecode\nFixed\nStarting an encoding with an invalid timefilter will now fail-fast instead of causing the encoding to stall\n1.192.0\nReleased 2024-05-14\nAdded\nAzure inputs\nwill now benefit from\naccelerated mode\nin regions that support it.\nChanged\nInternal stability improvements\n1.191.0\nReleased 2024-04-30\nAdded\nImplemented usage of r-attribute in segment references of SegmentTimeline Dash manifests generated with manifest generator V2. As a result, repeated subsequent segment references with the same duration will be removed and replaced with an r-attribute to reduce the size of the Dash manifest.\nAdd possibility to\nun-assign stream keys\nfrom a LIVE encoding\nImproved error message when LIVE Encoding can't be started because a stream key can't be assigned to it.\n1.190.0\nReleased 2024-04-16\nChanged\nInternal stability improvements\n1.189.0\nReleased 2024-04-03\nAdded\nAdded an\nendpoint\nto query existing global webhooks triggering when any manifest generation successfully finishes.\nAdded an\nendpoint\nto query existing global webhooks triggering when any manifest generation fails.\nFixed\nFixed random cases of stalled encodings when:\ninput resolution greater 4k + input duration shorter than 20s + number of output streams less than 3\nhighest rendition is greater 1080p + neither width nor height are set for the highest rendition + input duration shorter than 20s + number of output streams less than 3\n1.188.0\nReleased 2024-03-20\nChanged\nInternal stability improvements\n1.187.1\nReleased 2024-03-12\nAdded\nSimplified Cloud-Connect setup on AWS: AMI allowlisting is now done automatically. No need to request or wait until the next release.\nChanged\neventType\n,\ncategory\n, and\nresourceType\nare now read-only for Notification requests. These properties are set depending on the type of Notification.\n1.186.0\nReleased 2024-02-28\nAdded\nThe same stream key ID cannot be used for multiple static ingest points. A fail-fast has been put in place.\nFixed\nFor the manifest generator V2, the HLS version in fMp4 IFramePlaylist is now set to the same version as the HLS media manifest. Before, the HLS version of the IFramePlaylist was always 4, which caused problems for features that are not supported with HLS version 4.\nAdd support for variable segment durations in p4 IFramePlaylist for HLS manifests created with manifest generator V2\nFixed an issue where starting an already-started Live Encoding causes it to be set to ERROR state. Now the original Encoding state is not changed.\n1.185.0\nReleased 2024-02-06\nChanged\nRemoved DvbTeletextInputStream from the API. DVB Teletext streams are not supported by the Bitmovin Encoder.\n1.184.0\nReleased 2024-01-31\nAdded\nConfiguring static ingest points using the\nredundant RTMP input\nis now supported, making it possible to use one of our public RTMPS servers e.g. live-input.bitmovin.com\nRe-usable\nstream keys\ncan be created now via our API, enabling the option to reserve stream keys and re-assigning them to live encodings.\n1.183.2\nReleased 2024-01-23\nChanged\nA LIVE encoding can only be restarted if it is in Running state. In any other state, it is not possible anymore. For workflows where settings need to be maintained and remain consistent, we recommend ensuring these are set in the configuration and creating a new encoding with the same configuration settings applied.\n1.182.0\nReleased 2024-01-12\nChanged\nInternal stability improvements\nKnown Issues\nRestarting Live Encoding from cold (restarting a Live Encoding after it has already been stopped): Billing minutes are calculated incorrectly in the statistics when stopped Live Encodings are re-started. This leads to wrong statistics data shown in the dashboard, in reports and returned from the API. This only affects statistics, the actual billed minutes are correct.\n1.181.0\nReleased 2024-01-09\nAdded\nAdded configuration settings for\nsigningRegion\nand\naccessStyle\n(path or virtual hosted) to Generic S3 inputs and outputs to support more S3 providers.\nChanged\nFurther improvements on the Dynamic Pool algorithm, which now also takes into consideration the failed requests.\nKnown Issues\nRestarting Live Encoding from cold (restarting a Live Encoding after it has already been stopped): Billing minutes are calculated incorrectly in the statistics when stopped Live Encodings are re-started. This leads to wrong statistics data shown in the dashboard, in reports and returned from the API. This only affects statistics, the actual billed minutes are correct.\n1.180.0\nReleased 2023-12-12\nAdded\nGPU encoding is enabled for\nAWS Cloud Connect\ncustomers and can be configured using VOD_HARDWARE_SHORTFORM preset for\nH264\nand\nH265\n.\nChanged\nIt is not possible anymore to create a codec configuration (e.g. for\nH264\n) with one dimension set explicitly to 0 or less. Encodings with such a configuration would now fail-fast. Leaving one or both dimensions unspecified is still accepted.\nFixed\nFixed bug that caused wrong URIs in HLS I-Frame playlists.\n1.179.0\nReleased 2023-11-28\nAdded\nFeaturing a new preset (\nH264\nand\nH265\n) to run your encodings faster than before -\nVOD_HARDWARE_SHORTFORM\n. Aimed at\nshort-form content\n(input duration under 5 minutes), this new preset makes use of GPUs. This currently comes with restricted feature set and hence the following limitations hold:\nVOD only encoding scheduled on AWS (Azure and GCP currently are not supported) for YUV420 to YUV420 pixel format.\nAny additional features used (video filters, Per-Title, DolbyVision, Forensic Watermark, BurnInSubtitle, Thumbnail, PSNR, MultiPass) would result in a fail-fast.\nCurrently not supported for Cloud-Connect.\nChanged\nRemoving an Infrastructure Account (AWS | Google | Azure) will remove also the stopped pre-warmed pools. If a pool is still running on this infrastructure then the removal of the infrastructure will fail without removing any pre-warmed pools.\nRemoving a \"Region Setting\" will remove also the stopped pre-warmed pools from that particular region. If a pool is still running on this region then the removal of the region setting will fail without removing any pre-warmed pools.\n1.178.0\nReleased 2023-11-14\nAdded\nAccelerated Mode for short-form content: In region\nAWS EU-WEST-1\nall encodings using the\nSTABLE\nencoder version and inputs shorter than 5 minutes are now automatically benefitting from drastically reduced queuing times.\nCorrect error messages when trying to start encodings in Azure cloud regions with old encoder versions which do not support those new regions:\nEncoder versions < 2.179.0:\nAZURE_ASIA_EAST\n,\nAZURE_ASIA_SOUTHEAST\n,\nAZURE_BRAZIL_SOUTH\n,\nAZURE_CANADA_CENTRAL\n,\nAZURE_FRANCE_CENTRAL\n,\nAZURE_INDIA_CENTRAL\n,\nAZURE_INDIA_SOUTH\n,\nAZURE_JAPAN_EAST\n,\nAZURE_JAPAN_WEST\n,\nAZURE_KOREA_CENTRAL\n,\nAZURE_US_CENTRAL\n,\nAZURE_US_EAST2\nand\nAZURE_US_SOUTH_CENTRAL\nEncoder versions < 2.180.0:\nAZURE_US_WEST\n1.177.0\nReleased 2023-10-31\nAdded\nPre-warmed Pools can now dynamically adjust their size (increase/decrease) based on the pool usage. This feature can be activated using the new dynamicPool flag when\ncreating a Pre-warmed Pool\n.\n1.176.0\nReleased 2023-10-17\nAdded\nIntroduced new sorting and filtering parameters for\nEncoding Filters\n.\nFixed\nFixed cases where ENCODING_STATUS_CHANGED webhooks were not triggered for certain encodings.\n1.175.0\nReleased 2023-10-03\nChanged\nInternal stability improvements\n1.174.0\nReleased 2023-09-26\nFixed\nFixed DASH subtitle duration and timescale issues for WebVTT ChunkedText muxings.\n1.173.0\nReleased 2023-09-19\nAdded\nAdded support for ChunkedTextMuxing with WebVttSubtitleConfigurations in\nDASH V2 Default manifests\nand\nHLS V1 Default manifests\n.\nFixed\nUsing\nCustom channelLayout\nfor audio muxings with\nHD Option LIVE Encoding\nresulted in failed encodings.\n1.172.0\nReleased 2023-09-05\nChanged\nImprovement of internal scheduling decisions to avoid out-of-memory errors which occurred when the width and height of video streams were not explicitly set.\n1.171.0\nReleased 2023-08-30\nChanged\nInternal stability improvements\n1.170.0\nReleased 2023-08-09\nFixed\nLIVE encodings stayed in QUEUED state if an error occurred in hd start call.\n1.169.0\nReleased 2023-07-25\nAdded\nEnabled starting of\nHD option encodings\nvia API clients.\nFixed\nThe return value of\noutput-paths\nfor an encoding always returns a JSON array, even when there is no output.\n1.168.0\nReleased 2023-07-18\nAdded\nQueueing time considerably reduced when using Pre-warmed Encoder Pools.\nLive encodings can now be started using HD encoder options in the dashboard. With these options a lower and simplified pricing using \"Live Units\" is applied. For more information see:\nhttps://bitmovin.com/live-encoding-live-streaming/\n1.167.0\nReleased 2023-07-11\nChanged\nIn the\nList all encodings endpoint\n, the\nincludeTotalCount\nflag is now by default set to\nfalse\n.\n1.166.0\nReleased 2023-07-05\nFixed\nAdded SPEKE authentication method via API key. This method will set the X-API-Key header in HTTP requests to the SPEKE provider.\nAdded\nManifest creation is now possible for Bitmovin CDN outputs. The AssetID needs to be added to the output path of the manifest outputs and the individual segment paths.\nIntroduced a fail-fast when trying to create DASH manifest representations with incorrect linked muxings\n1.165.0\nReleased 2023-06-20\nFixed\nFor Live Encodings, when using\nAesEncryptionDrm\nwith a specified\nkeyFileUri\n, the keyFile was not properly uploaded to the designated output. This has been fixed.\n1.164.0\nReleased 2023-06-06\nChanged\nInternal stability improvements\n1.163.0\nReleased 2023-06-01\nAdded\nManifest Generator V2 is now available for DASH manifests by specifying\nmanifestGenerator=V2\nin the DASH Start-Manifest Request. For customers who sign up after June 1st 2023\nV2\nwill be the default manifestGenerator. The minimum encoder version for that feature is 2.121.0. Differences to the\nLEGACY\ngenerator can be found\nhere\n.\nAdded a check for Live encodings to allow\nProgramDateTimeSource EMBEDDED\nonly for SRT and ZIXI ingest. Configuring it with RTMP ingest now triggers a fail-fast.\nAdded a check for Live encodings to allow\nProgramDateTimeSource EMBEDDED\nonly with ManifestGenerator V2. Configuring it with ManifestGenerator LEGACY will now result in a fail-fast.\n1.162.0\nReleased 2023-05-23\nChanged\nInternal stability improvements.\n1.161.0\nReleased 2023-05-16\nAdded\nAn option to configure the source of\nProgramDateTime\nfor\nLive HLS manifests\n.\nSupported sources are SYSTEM_CLOCK (default, using UTC time of segment written) and EMBEDDED (using the first UTC timecode in the source video stream and current date).\n1.160.0\nReleased 2023-05-09\nChanged\nInternal stability improvements.\n1.159.0\nReleased 2023-05-03\nAdded\nautoLevelSetup\n: property for video configurations (\nh264\n,\nh265\n,\nav1\n,\nvp9\n) to enable/disable the automatic calculation of level, maximum bitrate, and buffer size based on the lowest level that satisfies maximum property values for picture resolution, frame rate, and bitrate. In the case the target level is set explicitly, the maximum bitrate and buffer size are calculated based on the defined level. The\nautoLevelSetup\nis enabled by default and is automatically disabled if the maximum bitrate or buffer size is set.\n1.158.0\nFixed\nFixed an inconsistency when reserving a static IP in Google Compute Engine.\n1.157.0\nFixed\nFixed a bug where using a closed caption media info lead to an error in HLS manifests.\n1.156.0\nChanged\nInternal stability improvements.\n1.155.0\nAdded\nFixed handling of certain HTTP/HTTPS input URLs in the Simple Encoding API.\n1.154.0\nAdded\nFor DASH manifests, when using\nprofile: ON_DEMAND\nand\nmanifestGenerator: V2\n, the\nduration\nproperty of Periods is now set when the manifest has multiple Periods.\n1.153.0\nChanged\nInternal stability improvements.\n1.152.0\nChanged\nInternal stability improvements.\n1.151.0\nChanged\nFor HDR manifests the\nManifestGenerator.V2\ndoes not write the\nprofile\nattribute on AdaptationSet level anymore.\nFixed\nThe\noutput permission check\ndoes not check for the\nGetBucketLocation\npermission anymore if\ncloudRegion\nof the S3Output is set.\n1.150.0\nChanged\nThe \"STANDARD\" mode for\nLIVE Encoding\nnow defaults to SINGLE_PASS (previously was TWO_PASS).",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/php-sdk",
    "title": "PHP SDK",
    "text": "The Bitmovin API enables you to build cutting edge video workflows while leveraging emmy-award winning encoding solutions, that automatically optimize your catalog for high quality and cost efficient video streaming.\nQuick Start Guide\nThis Quick Start Video will demonstrate different approaches to setup an encoding workflow using a\nWatch Folder Workflow (No Code Required)\nand a\nfull Python API SDK example\nusing this\nColabortory Notebook\n.\nundefined\nGet Started with your own Project\nThe following steps will walk you through adding the SDK to an new or existing Project step by step.\nOverview\nStarting an encoding triggers a request for encoding instances. Once available, the input file will be downloaded, divided into chunks, and distributed between all of them. Those instances transcode each chunk into segments for all configured video/audio representations (muxings) in parallel and write them to the designated output destination.\nThis approach makes Bitmovin's encoding the fastest in the world.\nIn this tutorial you will implement a simple VOD online streaming scenario, where an input file containing a video stream and stereo audio stream is transcoded into ...\na fixed ladder of 3 video representations at different resolutions and bitrates using the H264 codec, and\nan audio rendition with the AAC codec.\nEach individual stream is wrapped into fragmented MP4 (fMP4) containers. HLS and DASH manifests are generated so the encoded content is stream- and playable on the majority of modern devices and browsers out there today.\nStep 1: Add Bitmovin SDK to Your Project\nAdd this to your composer.json:\nFor the latest version please refer to the\nGitHub repository\nor check the\nchangelog\nof the Bitmovin API. Open API SDK versions and the Bitmovin API share the same versioning.\nJSON\n{\n  \"require\": {\n        \"php\": \"^7.1\",\n        \"bitmovin/bitmovin-api-sdk-php\": \"^1.95.0\"\n  }\n}\nAnd then execute:\nShell\ncomposer install\nStep 2: Setup a Bitmovin API Client Instance\nThe Bitmovin API client instance is used for all communication with the Bitmovin REST API, pass your API key to authenticate with the API. You can find your\nAPI_KEY\nin the dashboard in your\nAccount Settings\n.\nPHP\nrequire './vendor/autoload.php';\n\nuse BitmovinApiSdk\\BitmovinApi;\nuse BitmovinApiSdk\\Common\\Logging\\ConsoleLogger;\nuse BitmovinApiSdk\\Configuration;\n\n$bitmovinApi = new BitmovinApi(Configuration::create()\n    ->apiKey(API_KEY)\n    ->logger(new ConsoleLogger())\n);\nStep 3: Create an Input\nCreate Input\nAn input holds the configuration to access a specific file storage, from which the encoder will download the source file.\nWe support various types of input sources including HTTP(S), (S)FTP, Google Cloud Storage (GCS), Amazon Simple Storage Service (S3), Azure Blob Storage, and all cloud storage vendors that offer an S3 compatible interface.\nBelow we are creating an input. You can change the input type on the top right of the code panel.\nPHP\n$input = new HttpsInput();\n$input->name(\"<INPUT_NAME>\");\n$input->host(\"<HTTPS_INPUT_HOST>\");\n$input = $bitmovinApi->encoding->inputs->https->create($input);\nReuse Inputs\nInputs refer to a location, not a specific file.\nThis makes them easily reusable. Create an input once and reuse it for all your encodings.\nHint: All resources in our API are identified by a unique id which enables you to reuse many resources. So you only have to create one input for each location and can reuse it for all encodings.\nPHP\n$input = $bitmovinApi->encoding->inputs->https->get(\"<INPUT_ID>\");\nStep 4: Create an Output\nCreate Output\nOutputs define where the manifests and encoded segments will be written to.\nLike inputs, outputs represent a set of information needed to access a specific storage. From this storage players can access the content for playback.\n$output = new GcsOutput();\n$output->accessKey(\"<GCS_ACCESS_KEY>\");\n$output->secretKey(\"<GCS_SECRET_KEY>\");\n$output->bucketName(\"<GCS_BUCKET_NAME>\");\n$output->name(\"<GCS_OUTPUT_NAME>\");\n$output = $bitmovinApi->encoding->outputs->gcs->create($output);;\n$outputId = $output->id;\nReuse Output\nSimilar to inputs, outputs also refer to a location, not a specific file.\nSo they're exactly as reusable as inputs. Create it once and reuse it for all your encodings.\n$output = $bitmovinApi->encoding->outputs->gcs->get(\"<OUTPUT_ID>\");\nStep 5: Create Codec Configurations\nCodec Configurations\nCodec configurations specify the codec and its parameters (eg. media codec, bitrate, frame rate, sampling rate).\nThey are used to encode the input stream coming from your input file.\nVideo Codec Configurations\nBelow we are creating video codec configurations. We support a multitude of codecs, including H264, H265, VP9, AV1 and many more.\nFor simplicity, we use H264 now, but you can easily change that after finishing the getting started.\nPHP\n$codecConfigVideoName1 = \"Getting Started H264 Codec Config 1\";\n$codecConfigVideoBitrate1 = 1500000;\n$videoCodecConfiguration1 = new H264VideoConfiguration();\n$videoCodecConfiguration1->name($codecConfigVideoName1);\n$videoCodecConfiguration1->presetConfiguration(PresetConfiguration::VOD_STANDARD());\n$videoCodecConfiguration1->width(1024);\n$videoCodecConfiguration1->bitrate($codecConfigVideoBitrate1);\n$videoCodecConfiguration1->description(\"{$codecConfigVideoName1}_{$codecConfigVideoBitrate1}\");\n$videoCodecConfiguration1 = $bitmovinApi->encoding->configurations->video->h264->create($videoCodecConfiguration1);\n\n$codecConfigVideoName2 = \"Getting Started H264 Codec Config 2\";\n$codecConfigVideoBitrate2 = 1000000;\n$videoCodecConfiguration2 = new H264VideoConfiguration();\n$videoCodecConfiguration2->name($codecConfigVideoName2);\n$videoCodecConfiguration2->presetConfiguration(PresetConfiguration::VOD_STANDARD());\n$videoCodecConfiguration2->width(768);\n$videoCodecConfiguration2->bitrate($codecConfigVideoBitrate2);\n$videoCodecConfiguration2->description(\"{$codecConfigVideoName2}_{$codecConfigVideoBitrate2}\");\n$videoCodecConfiguration2 = $bitmovinApi->encoding->configurations->video->h264->create($videoCodecConfiguration2);\n\n$codecConfigVideoName3 = \"Getting Started H264 Codec Config 3\";\n$codecConfigVideoBitrate3 = 750000;\n$videoCodecConfiguration3 = new H264VideoConfiguration();\n$videoCodecConfiguration3->name($codecConfigVideoName3);\n$videoCodecConfiguration3->presetConfiguration(PresetConfiguration::VOD_STANDARD());\n$videoCodecConfiguration3->width(640);\n$videoCodecConfiguration3->bitrate($codecConfigVideoBitrate3);\n$videoCodecConfiguration3->description(\"{$codecConfigVideoName3}_{$codecConfigVideoBitrate3}\");\n$videoCodecConfiguration3 = $bitmovinApi->encoding->configurations->video->h264->create($videoCodecConfiguration3);\nAudio Codec Configurations\nAs audio codec configuration we are creating one AAC configuration.\nWe also support many more audio codecs including Opus, Vorbis, AC3, E-AC3 and MP2.\nPHP\n$aacConfig = new AacAudioConfiguration();\n$aacConfig->name(\"Getting Started Audio Codec Config\");\n$aacConfig->bitrate(128000);\n$codecConfigAudio = $bitmovinApi->encoding->configurations->audio->aac->create($aacConfig);\nReusing Codec Configurations\nAs inputs and outputs, codec configurations can also be reused.\nTo use existing video or audio codec configurations, loading them with their id.\nPHP\n$codecConfigVideo = $bitmovinApi->encoding->configurations->video->h264->get(\"<H264_CC_ID>\");\n$codecConfigAudio = $bitmovinApi->encoding->configurations->audio->aac->get(\"<AAC_CC_ID>\");\nStep 6: Create & Configure an Encoding\nCreate Encoding\nAn encoding is a collection of resources (inputs, outputs, codec configurations etc.), mapped to each other.\nThe cloud region defines in which cloud and region your encoding will be started.\nIf you don't set it, our encoder takes the region closest to your input and output.\nInputs, outputs and codec configurations can be reused among many encodings. Other resources are specific to one encoding, like streams, which we'll create below.\nPHP\n$encoding = new Encoding();\n$encoding->name(\"Getting Started Encoding\");\n$encoding->cloudRegion(CloudRegion::GOOGLE_EUROPE_WEST_1());\n$encoding = $bitmovinApi->encoding->encodings->create($encoding);\nStreams\nA stream maps an audio or video input stream of your input file (called input stream) to one audio or video codec configuration.\nThe following example uses the selection mode\nAUTO\n, which will select the first available video input stream of your input file.\nYou can choose an input file from 3 predefined examples on the top right of the code panel.\nVideo Stream\nPHP\n$inputPath = \"<INPUT_PATH>\";\n$streamInput = new StreamInput();\n$streamInput->inputId($input->id);\n$streamInput->inputPath($inputPath);\n$streamInput->selectionMode(StreamSelectionMode::AUTO());\n\n$videoStream1 = new Stream();\n$videoStream1->inputStreams([$streamInput]);\n$videoStream1->codecConfigId($videoCodecConfiguration1->id);\n$videoStream1 = $bitmovinApi->encoding->encodings->streams->create($encoding->id, $videoStream1);\n\n$videoStream2 = new Stream();\n$videoStream2->inputStreams([$streamInput]);\n$videoStream2->codecConfigId($videoCodecConfiguration2->id);\n$videoStream2 = $bitmovinApi->encoding->encodings->streams->create($encoding->id, $videoStream2);\n\n$videoStream3 = new Stream();\n$videoStream3->inputStreams([$streamInput]);\n$videoStream3->codecConfigId($videoCodecConfiguration3->id);\n$videoStream3 = $bitmovinApi->encoding->encodings->streams->create($encoding->id, $videoStream3);\nAudio Stream\nPHP\n$streamInput = new StreamInput();\n$streamInput->inputId($input->id);\n$streamInput->inputPath($inputPath);\n$streamInput->selectionMode(StreamSelectionMode::AUTO());\n\n$inputStreamAudio = new Stream();\n$inputStreamAudio->inputStreams([$streamInput]);\n$inputStreamAudio->codecConfigId($codecConfigAudio->id);\n$inputStreamAudio = $bitmovinApi->encoding->encodings->streams->create($encoding->id, $inputStreamAudio);\nStep 7: Create a Muxing\nMuxings\nA muxing defines which container format will be used for the encoded video or audio files (segmented TS, progressive TS, MP4, FMP4, ...). It requires a stream, an output, and the output path, where the generated segments will be written to.\nFMP4 muxings are used for DASH manifest representations, TS muxings for HLS playlist representations.\nFMP4\nPHP\n$aclEntry = new AclEntry();\n$aclEntry->permission(AclPermission::PUBLIC_READ());\n$segmentLength = 4;\n$outputPath = \"<OUTPUT_PATH>\";\n$initSegmentName = \"init.mp4\";\n$segmentNaming = \"seg_%number%.m4s\";\n\n$videoMuxingOutput1 = new EncodingOutput();\n$videoMuxingOutput1->acl([$aclEntry]);\n$videoMuxingOutput1->outputId($outputId);\n$videoMuxingOutput1->outputPath(\"{$outputPath}/video/1024_1500000/fmp4\");\n\n$videoMuxingStream1 = new MuxingStream();\n$videoMuxingStream1->streamId($videoStream1->id);\n\n$videoMuxing1 = new Fmp4Muxing();\n$videoMuxing1->outputs([$videoMuxingOutput1]);\n$videoMuxing1->streams([$videoMuxingStream1]);\n$videoMuxing1->segmentLength($segmentLength);\n$videoMuxing1->initSegmentName($initSegmentName);\n$videoMuxing1->segmentNaming($segmentNaming);\n$bitmovinApi->encoding->encodings->muxings->fmp4->create($encoding->id, $videoMuxing1);\n\n$videoMuxingOutput2 = new EncodingOutput();\n$videoMuxingOutput2->acl([$aclEntry]);\n$videoMuxingOutput2->outputId($outputId);\n$videoMuxingOutput2->outputPath(\"{$outputPath}/video/768_1000000/fmp4\");\n\n$videoMuxingStream2 = new MuxingStream();\n$videoMuxingStream2->streamId($videoStream2->id);\n\n$videoMuxing2 = new Fmp4Muxing();\n$videoMuxing2->outputs([$videoMuxingOutput2]);\n$videoMuxing2->streams([$videoMuxingStream2]);\n$videoMuxing2->segmentLength($segmentLength);\n$videoMuxing2->initSegmentName($initSegmentName);\n$videoMuxing2->segmentNaming($segmentNaming);\n$bitmovinApi->encoding->encodings->muxings->fmp4->create($encoding->id, $videoMuxing2);\n\n$videoMuxingOutput3 = new EncodingOutput();\n$videoMuxingOutput3->acl([$aclEntry]);\n$videoMuxingOutput3->outputId($outputId);\n$videoMuxingOutput3->outputPath(\"{$outputPath}/video/640_750000/fmp4\");\n\n$videoMuxingStream3 = new MuxingStream();\n$videoMuxingStream3->streamId($videoStream3->id);\n\n$videoMuxing3 = new Fmp4Muxing();\n$videoMuxing3->outputs([$videoMuxingOutput3]);\n$videoMuxing3->streams([$videoMuxingStream3]);\n$videoMuxing3->segmentLength($segmentLength);\n$videoMuxing3->initSegmentName($initSegmentName);\n$videoMuxing3->segmentNaming($segmentNaming);\n$bitmovinApi->encoding->encodings->muxings->fmp4->create($encoding->id, $videoMuxing3);\nAudio Muxings\nPHP\n$aclEntry = new AclEntry();\n$aclEntry->permission(AclPermission::PUBLIC_READ());\n\n$audioMuxingOutput = new EncodingOutput();\n$audioMuxingOutput->acl([$aclEntry]);\n$audioMuxingOutput->outputId($outputId);\n$audioMuxingOutput->outputPath(\"{$outputPath}/audio/128000/fmp4\");\n\n$audioMuxingStream = new MuxingStream();\n$audioMuxingStream->streamId($inputStreamAudio->id);\n\n$audioMuxing = new Fmp4Muxing();\n$audioMuxing->outputs([$audioMuxingOutput]);\n$audioMuxing->streams([$audioMuxingStream]);\n$audioMuxing->segmentLength($segmentLength);\n$audioMuxing->initSegmentName($initSegmentName);\n$audioMuxing->segmentNaming($segmentNaming);\n$bitmovinApi->encoding->encodings->muxings->fmp4->create($encoding->id, $audioMuxing);\nHint: To optimize your content for a specific use-case you can also provide a custom segment length. Further, you can also customize the naming pattern for the resulting segments and the initialization segment as well.\nStep 8: Create a Manifest\nDASH & HLS Manifests\nIts recommended to use Default Manifests, one each for DASH (\nAPI-Reference\n) and HLS (\nAPI-Reference\n). They are a construct that leaves it to the encoder to determine how to best generate the manifest based on what resources have been created by the encoding. This simplifies the code greatly for simple scenarios like this guide is about.\nCreate a DASH Manifest\nPHP\n$aclEntry = new AclEntry();\n$aclEntry->permission(AclPermission::PUBLIC_READ());\n\n$encodingOutput = new EncodingOutput();\n$encodingOutput->acl([$aclEntry]);\n$encodingOutput->outputPath($outputPath);\n$encodingOutput->outputId($outputId);\n\n$dashManifest = new DashManifestDefault();\n$dashManifest->encodingId($encoding->id);\n$dashManifest->manifestName(\"stream.mpd\");\n$dashManifest->version(DashManifestDefaultVersion::V1());\n$dashManifest->outputs([$encodingOutput]);\n\n$dashManifest = $bitmovinApi->encoding->manifests->dash->default->create($dashManifest);\nCreate a HLS manifest\nPHP\n$aclEntry = new AclEntry();\n$aclEntry->permission(AclPermission::PUBLIC_READ());\n\n$manifestOutput = new EncodingOutput();\n$manifestOutput->outputPath($outputPath);\n$manifestOutput->outputId($outputId);\n$manifestOutput->acl([$aclEntry]);\n\n$hlsManifest = new HlsManifestDefault();\n$hlsManifest->encodingId($encoding->id);\n$hlsManifest->manifestName(\"master.m3u8\");\n$hlsManifest->version(HlsManifestDefaultVersion::V1());\n$hlsManifest->outputs([$encodingOutput]);\n\n$hlsManifest = $bitmovinApi->encoding->manifests->hls->default->create($hlsManifest);\nStep 9: Start an Encoding\nLet's recap - We have defined:\nan\ninput\n, specifying the input file host\nan\noutput\n, specifying where the manifest and the encoded files will be written to\ncodec configurations\n, specifying how the input file will be encoded\nstreams\n, specifying which input stream will be encoded using what codec configuration\nmuxings\n, specifying the containerization of the streams\nmanifests\n, to play the encoded content using the HLS or DASH streaming format.\nNow we can start the encoding with one API call, including additional configuration so the DASH and HLS Default-Manifests are created as part of the encoding process too:\nPHP\n$dashManifestResource = new ManifestResource();\n$dashManifestResource->manifestId($dashManifest->id);\n$hlsManifestResource = new ManifestResource();\n$hlsManifestResource->manifestId($hlsManifest->id);\n\n$startEncodingRequest = new StartEncodingRequest();\n$startEncodingRequest->manifestGenerator(ManifestGenerator::V2());\n$startEncodingRequest->vodDashManifests([$dashManifestResource]);\n$startEncodingRequest->vodHlsManifests([$hlsManifestResource]);\n\n$bitmovinApi->encoding->encodings->start($encoding->id, $startEncodingRequest);\nWhat happens next:\nAfter the successful start of the encoding, it will change its state from\nCREATED\nto\nSTARTED\n, and shortly after that to\nQUEUED\n.\nRUNNING\nit will be in as soon as the input file starts to get downloaded and processed. Eventually\nFINISHED\nwill indicate a successful encoding and upload of the content to the provided Output destination. More details about each encoding status can be\nfound here\n.\nStep 10: Final Review\nCongratulations!\nYou successfully started your first encoding :) and this is only the beginning. The Bitmovin Encoding API is extremely flexible and allows many customizations and provides with access to the latest codecs and video streaming optimizations and technologies out there. Have a look at the\nAPI Reference\nto learn more.\nMore Examples\nYou can find many more fully functional code examples in our Github Repository:\nGithub",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/f671dc6-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/retrying-transfer-errors-with-re-transfer-for-successful-encoding-jobs",
    "title": "Retrying Transfer Errors with Re-Transfer for Successful Encoding Jobs",
    "text": "Context\nIdeally, all parts of a workflow are 100% reliable and work as expected every single time. As we all know, this is not the case and that's why a system has to be designed in a resilient way to make up for the instability of single parts within.\nOne way in which Bitmovin helps to make the workflow more resilient is the re-transfer functionality. Encoded content is stored on a temporary storage for 72 hours and if anything goes wrong during the transfer to your output storage, it can be retried without needing to re-encode the content.\nBasic usage\nRe-transfer needs to be enabled for your account. Please reach out if you are not sure if it's enabled for you.\nTo get started with using the re-transfer functionality, all you have to do is the following steps shown as example with our Java SDK.\n1. Configure the transfer error webhook\nConfigure the transfer-error webhook to be informed about a transfer problem for a specific encoding.\nJava\nWebhook transferErrorWebhook = new Webhook();\ntransferErrorWebhook.setUrl(\"https://yourservice.com/webhooks/transfer-error\");\ntransferErrorWebhook.setMethod(WebhookHttpMethod.POST);\ntransferErrorWebhook = bitmovinApi\n    .notifications\n    .webhooks\n    .encoding\n    .encodings\n    .transferError\n    .createByEncodingId(\"<INSERT THE ENCODING ID>\", transferErrorWebhook);\n2. Configure the encoding finished webhook\nConfigure a webhook to be informed when the transfer retry finished successfully and the encoding status changes.\nJava\nWebhook encodingFinishedWebhook = new Webhook();\nencodingFinishedWebhook.setUrl(\"https://yourservice.com/webhooks/encoding-finished\");\nencodingFinishedWebhook.setMethod(WebhookHttpMethod.POST);\nencodingFinishedWebhook = bitmovinApi\n    .notifications\n    .webhooks\n    .encoding\n    .encodings\n    .finished\n    .createByEncodingId(\"<INSERT THE ENCODING ID>\", encodingFinishedWebhook);\n3. Trigger the transfer retry\nThis is how a transfer-error webhook receiving application could look like. Please see the\ntransfer-error webhook specification\nfor details on the webhook payload for access to the information you will need to pass to the next call.\nCall the\ntransfer retries endpoint\nfor this encoding ID to start the transfer retry.\nJava\nWebhookPayload webhookPayload = getWebhookPayload(); // getWebhookPayload() deserializes the webhook payload into a WebhookPayload object\nString encodingId = webhookPayload.getEncoding().getId();\nTransferRetry transferRetry = bitmovinApi\n    .encoding\n    .encodings\n    .transferRetries\n    .create(encodingId);\n4. Success\nWhen the transfer retry finished successfully, all the files should be present in the Output storage that was set for the encoding, and the encoding-finished webhook gets triggered.\nSummary\nWith the examples above you should be able to set up your workflow to handle encodings which are successful but are not able to transfer to your output storage.\nThis small but impactful change can save significant costs, as you don't have to run the encoding again just for retrying the transfer.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/creating-manifests-with-the-bitmovin-api",
    "title": "Creating Manifests with the Bitmovin API",
    "text": "Overview\nEncoding your content into a streamable format is the first big step towards improving viewer experience. For ABR (adaptive bitrate) capable players to make the most out of it, creating a manifest file is the second step. Our service supports multiple kinds of manifests and multiple ways of creating them, to ideally suit your requirements and workflow.\nThere are four dimensions to be considered:\ndimension\noptions\nmanifest type\nDASH\n/\nHLS\n/\nSmooth Streaming\nconfiguration method\ncustom\nvs\ndefault\ngeneration method\njust-in-time\n(JIT) vs\npost-encoding\ngenerator version\nV2\nvs\nLEGACY\nManifest type\nDifferent players / end user devices require different manifest types. The most widespread standard is DASH (Dynamic Adaptive Streaming over HTTP). HLS (HTTP Live Streaming) is the de-facto standard on all Apple devices, while Smooth Streaming covers a couple of applications in the Microsoft universe.\nTo serve the widest range of viewers possible, it's advisable to create multiple manifests for the same encoded content, which is just a finger exercise with Bitmovin.\nConfiguration method\nThis decision depends on the degree of customization that is required regarding the exact structure of the manifest file and its contents (video and audio renditions, thumbnails, sprites, subtitles, DRM information,...).\nA \"default manifest\" is the worry-free option that will be configured fully automatically, depending on what output your encoding creates. This is a good starting point for most low to medium complexity workflows.\nIf you need more control, go for a custom manifest. This will be your choice for complex workflows, e.g. if you want to configure ad breaks.\nSee\nDefault vs custom manifests\nfor further details. Once you have created a Manifest resource and configured it to your choice, the next step is trigger the generation of the actual manifest file(s).\nGeneration method\nThis is about triggering the process of building the actual manifest text file(s), based on the manifest API resource that you configured beforehand.\nJust-in-time (JIT)\nJIT manifests are generated along with an encoding (technically speaking, on the same cloud instance). For publishing new content as quickly as possible, this is the way to go. For LIVE encodings it is the only option.\nFrom an API perspective, the encoding process and manifest generation are triggered via the same “start” call. To generate manifests just-in-time, add them to the following list properties of the\nStart Encoding\nor\nStart Live Encoding\nrequest:\nDASH:\nvodDashManifests\nHLS:\nvodHlsManifests\nSmooth Streaming:\nvodSmoothManifests\n(VoD only)\nPreview manifests\nare a special kind of JIT manifests that allow to “live” stream the output of a VoD encoding while it’s still in progress. A preview manifest is typically used in addition to a regular manifest for extremely time-critical applications. Just add the IDs of the Manifest resources to the following list properties of the\nStart Encoding\nrequest:\nDASH:\npreviewDashManifests\nHLS:\npreviewHlsManifests\nPost-encoding\nAs the name suggests, post-encoding manifests can be generated at any later time after encoding.\nThey allow combining outputs from multiple encodings (e.g. using different codecs) into one manifest, which we refer to as a\nmulti-encoding\nmanifest. It’s also possible to make LIVE encodings (or parts thereof) available as VoD content (see\nHow to implement a\nLive-to-VoD\nworkflow with the Bitmovin API\n).\nPost-encoding manifest generation is triggered by a separate \"start\" call:\nDASH:\nStart DASH Manifest Creation\nHLS:\nStart HLS Manifest Creation\nSmooth Streaming:\nStart Smooth Streaming Manifest Creation\nEnsure that all referenced encodings are in state\nFINISHED\nbefore generating a manifest for them, otherwise the generation can fail. If you do not want to care about encoding state, consider using just-in-time generation.\nComparison\njust-in-time\npost-encoding\nA single start call triggers both the encoding process and the manifest generation, simplifying workflow implementation (fire & forget).\nEncoding process and manifest generation are triggered via separate start calls.\nThe minor version of the manifest generator is bound to the used encoder version. To take advantage of the latest improvements and fixes, use the most recent encoder version.\nAlways uses the most recent minor version of the manifest generator. However, some improvements/fixes may rely on information that is only provided by newer encoder versions. If this is the case, release notes will mention the minimum required encoder version.\nFor LIVE encodings and preview manifests, this is the only option.\nFor Live-to-VoD and multi-encoding manifests (combining the output of multiple encodings), this is the only option.\nGenerator version\nThe manifest generator is the component that builds the actual manifest text file(s) that are written to the output storage, based on a manifest API resource which you configured beforehand.\nIf your organization signed up before 2023-06-01, you may be using an outdated manifest generator by default and should consider migrating to the current version. For a detailed list of improvements and migration guide, see\nManifest Generator V2\n.\nIf your organization is newer than that, you're automatically using the new generator version and cannot use the old one, so there is no choice to take.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/constraining-bandwidth-in-per-title-encoding",
    "title": "Constraining Bandwidth in Per-Title Encoding",
    "text": "Overview\nThis article shall give an example of the settings and best practices that we can use in the encoding configurations with Per-Title when we have a use case where we are restricted to specific requirements for renditions or bitrates. Following this tutorial, it is expected to have basic knowledge about the Per-Title functionality and how to set up a basic configuration. A comprehensive overview of the available parameters is available in\nPer-Title Configuration Options explained\n.\nUse case requirements\nThe described use case has the following requirements and shall demonstrate an example.\nWe want to encode a fixed amount of H.264 renditions with specified resolutions.\nWe want to specify the maximum bitrate for each rendition.\nThe Per-Title algorithm shall calculate the optimal bitrate for each rendition to achieve an optimised bitrate ladder (\nWhat is Per-Title Encoding?\n).\nAn example could be if we want to take the recommended bitrate ladder from Apple for H.264 and limit the bitrate to these specifications. Still, according to the complexity of the content, we want to save as much bandwidth as possible.\nResolution\nBitrate\n416 x 234\n145k\n640 x 360\n365k\n768 x 432\n730k\n768 x 432\n1100k\n960 x 540\n2000k\n1280 x 720\n3000k\n1280 x 720\n4500k\n1920 x 1080\n6000k\n1920 x 1080\n7800k\nTable1: HTTP Live Streaming (HLS) Authoring Specification for Apple Devices\nStream Modes\nWhen configuring Per-Title, we need to specify at least one Per-Title template for the stream mode to enable the algorithm to do its magic.\nThese three different kinds of Per-Title-Templates are available.\nPER_TITLE_TEMPLATE\nPER_TITLE_TEMPLATE_FIXED_RESOLUTION\nPER_TITLE_TEMPLATE_FIXED_RESOLUTION_AND_BITRATE\nFor a simple Per-Title encoding, we usually specify one\nPER_TITLE_TEMPLATE\n, enable\nautoRepresentations\n, and Per-Title will take care of the rest. More information can be found in our tutorial\nHow to create a Per-Title Encoding\n.\nBut according to our requirements, we need control over the resolutions and bitrates of each of our renditions; therefore, we will specify one\nPER_TITLE_TEMPLATE_FIXED_RESOLUTION_AND_BITRATE\nper required rendition.\nCreating stream templates and setting encoding parameters\nTo specify the resolution and the maximum bitrate, we must create one stream template for each rendition. We assign individual H.264 configurations with the required\nwidth\nor\nheight\nspecified. In the\nper_title_settings\nof the streams, we set our boundaries via the\nstream_fixed_res_bit_settings\n. The here-defined\nmin_bitrate\nand\nmax_bitrate\nper rendition are crucial to enable Per-Title to pick the correct renditions. Together with the common settings\nmin_bitrate_step_size\n,\nmax_bitrate_step_size\n,\nmin_bitrate\nand\nmax_bitrate\nin the Per-Title settings'\nstart_encoding_request\n, it is essential to find the right combination. Suppose these settings are not optimised to each other; in that case, Per-Title will be forced to drop desired renditions or add additional unwanted renditions. In the worst case, finding renditions based on the given boundaries is impossible, leading to an encoding error.\nTo start with, we can specify all the settings specific to the single renditions in an object to keep a clear overview.\nPython\nEncodingProfile = collections.namedtuple('EncodingProfile', 'width min max')\nvideo_profiles = [\n    EncodingProfile(width=416, min=36_250, max=145_000),\n    EncodingProfile(width=640, min=91_250, max=365_000),\n    EncodingProfile(width=768, min=182_500, max=730_000),\n    EncodingProfile(width=768, min=275_000, max=1_100_000),\n    EncodingProfile(width=960, min=500_000, max=2_000_000),\n    EncodingProfile(width=1280, min=750_000, max=3_000_000),\n    EncodingProfile(width=1280, min=1_125_000, max=4_500_000),\n    EncodingProfile(width=1920, min=1_500_000, max=6_000_000),\n    EncodingProfile(width=1920, min=1_950_000, max=7_800_000)\n]\nThe\nwidth\nand the\nmax\n(bitrate) are clear; this comes from our initial requirements. The\nmin\n(bitrate) needs a bit of gut feeling (or complicated calculation). You don't want to set this parameter too low, even if this seems to widen the boundaries. As said before, this parameter works especially in combination with the\nmin_bitrate_step_size\nand the\nmax_bitrate_step_size\nin the\nstart_encoding_request\nsettings.\nPython\n# Start Encoding Job\nstart_encoding_request = StartEncodingRequest(\n    per_title=PerTitle(\n        h264_configuration=H264PerTitleConfiguration(\n            min_bitrate=36_250, \n            max_bitrate=7_800_000, \n            min_bitrate_step_size=1.2,\n            max_bitrate_step_size=4.0,\n            target_quality_crf=22, \n            complexity_factor=1.0,\n        )\n    )\n)\nbitmovin_api.encoding.encodings.start(encoding_id=encoding.id,start_encoding_request=start_encoding_request)\nWhile the\nwidth\n(or the\nheight\n) is specified in the configuration,\nPython\n# Video Configuration\nvideo_configuration = H264VideoConfiguration(\n    width=video_profile.width,\n    preset_configuration=PresetConfiguration.VOD_HIGH_QUALITY,\n)\nvideo_configuration = bitmovin_api.encoding.configurations.video.h264.create(h264_video_configuration=video_configuration)\nthe individual\nmin_bitrate\nand\nmax_bitrate\nare configured in the stream settings.\nPython\n# Video Streams\nstream_fixed_res_bit_settings = StreamPerTitleFixedResolutionAndBitrateSettings(\n    min_bitrate=video_profile.min,\n    max_bitrate=video_profile.max,\n    bitrate_selection_mode=BitrateSelectionMode.COMPLEXITY_RANGE,\n    low_complexity_boundary_for_max_bitrate=1_950_000,\n    high_complexity_boundary_for_max_bitrate=7_800_000\n)\nstream_per_title_settings = StreamPerTitleSettings(fixed_resolution_and_bitrate_settings=stream_fixed_res_bit_settings)\nvideo_stream = Stream(\n    input_streams=[stream_input], \n    codec_config_id=video_configuration.id, \n    mode=StreamMode.PER_TITLE_TEMPLATE_FIXED_RESOLUTION_AND_BITRATE,\n    per_title_settings=stream_per_title_settings\n)\nvideo_stream = bitmovin_api.encoding.encodings.streams.create(encoding_id=encoding.id, stream=video_stream)\nFor this use case, I found promising results for the\nmin_bitrate\nby dividing the\nmax_bitrate\n/\nmax_bitrate_stepsize\nand setting the\nmax_bitrate_stepsize = 4\n. Increasing the\nmax_bitrate_stepsize\nwas also necessary to set the proper boundaries for this use case. Here we have a good example of what happens when not finding the correct settings – Per-Title was forced to generate only seven renditions out of the nine desired renditions when using the default setting of\nmax_bitrate_stepsize = 1.9\n.\nThe same counts for the\nmin_bitrate_step_size\n; this can also mess up the algorithm if not chosen carefully. Having found the right combination of bitrate and step size settings, one must understand that the normal Per-Title behaviour calculates the renditions from bottom to top. To gain some influence over the bitrate of the highest rendition, we can use the\nCOMPLEXITY_RANGE\nin the\nbitrate_selection_mode\nof the stream settings.\nPython\n# Video Streams\nstream_fixed_res_bit_settings = StreamPerTitleFixedResolutionAndBitrateSettings(\n    min_bitrate=video_profile.min,\n    max_bitrate=video_profile.max,\n    bitrate_selection_mode=BitrateSelectionMode.COMPLEXITY_RANGE,\n    low_complexity_boundary_for_max_bitrate=1_950_000,\n    high_complexity_boundary_for_max_bitrate=7_800_000\n)\nHere the algorithm first calculates the bitrate of the top rendition in the ladder based on the complexity analysis of the source file and the configured Per-Title settings (such as\ntargetQualityCrf\nand\ncomplexityFactor\n) and takes this into account while picking the renditions. It then compares this top bitrate with a configurable complexity range defined by its minimum and maximum bitrates. To ensure that the highest rendition falls into this range, in this use case, I specified the min and max values according to the specified min and max bitrates of this rendition.\nWith all these settings in place, we achieve the desired outcome with all the required renditions.\nInfluence the bitrate of the highest rendition\nThe main advantage of Per-Title is to determine the optimal renditions based on the complexity of the content. When setting the proper boundaries, Per-Title can significantly save bandwidth and storage for low-complex content compared to static renditions or ensure that quality requirements are still fulfilled with very high-complex content.\nThe default settings for using the contents’ calculated complexity to pick the optimum bitrates usually provide good results. Nevertheless, it is still possible to tweak the rendition ladder to achieve an overall higher or lower quality (or lower or higher bitrates). It is also possible to process different kinds of content with various complexities individually, for example, to reduce the bitrates of videos that are known to have high complexity.\nOne can influence these factors with the\ntarget_quality_crf\nand the\ncomplexity_factor\nin the\nstart_encoding_request\n. With the\ntarget_quality_crf\n, it is possible to shift the quality of the entire ladder up or down (within the boundaries limited by all the other settings). The default CRF for H.264 is specified as 22; anything below will increase the quality (and the bitrates), and anything above will reduce accordingly.\nPython\n# Start Encoding Job\nstart_encoding_request = StartEncodingRequest(\n    per_title=PerTitle(\n        h264_configuration=H264PerTitleConfiguration(\n            min_bitrate=36_250, \n            max_bitrate=7_800_000, \n            min_bitrate_step_size=1.2,\n            max_bitrate_step_size=4,\n            target_quality_crf=22, \n            complexity_factor=1.0,\n        )\n    )\n)\nThe\ncomplexity_factor\nhas basically the same effect. The default here is\n1.0\n; a higher factor will affect Per-Title to consider the content more complex and assign higher bitrates, while a lower factor (>0) will cause the content to be considered lower complex and assign lower bitrates. Small changes to\ncomplexity_factor\n(ie +/- 0.2) is recommended for your initial testing.\nThis is especially useful to specify the desired quality for an entire library with one target CRF but still be able to treat various kinds of content differently.\nAppendix\nPlease refer to the\nper_title_constraints.py\nPython script for a full example!\nFrequently asked questions\nQ.\nThe default output from the algorithm is too high in quality\nA1. Bring\nall\nassets up or down in quality using\ntargetCRF\n(increase to reduce quality)\nA2. Bring\nall\nassets up or down in quality using\ncomplexity factor\n(reduce to reduce quality). Note: only change by 0.1 initially. Recommend changing one, not both.\nQ.\nMy complex assets are too high in quality, but my simple assets are perfect. I want to change the\nbias\n.\nA1. Adjust the gap between\nlow\nand\nhigh_complexity_boundaries\n, think of this like a magnifying glass. The smaller the gap between the numbers, the more variation in quality you see between a complex and simple asset (you are bringing the magnifying glass closer to the page)\nQ.\nIn my assets, on average the lower\nrenditions\nare too low in quality, the upper renditions are perfect.\nA1. Adjust the\nminimum\nand\nmaximum\nbitrate, found in the start request. In this scenario increasing the\nminimum\nwould likely produce the output you are looking for.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/c72ab6b-image.png",
      "https://files.readme.io/7ec1bd9-image.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/bitmovin-video-encoder-v1-to-v2-migration-guide",
    "title": "Bitmovin Video Encoder v1 to v2 migration guide",
    "text": "Breaking changes that may impact your encoding workflows\nSome of the changes to our new encoder will impact your existing encoding workflows. In this section, we’ll cover those key breaking changes and show you what they entail exactly. The list below explains how these changes reflect in our API, where to find the related section in our documentation and which exact properties are concerned.\nStart Encoding (Encoding Mode): Default set to TWO_PASS This change sets the new default encoding mode to two-pass encoding from single-pass encoding as the previous default.\nAPI call:\nStart Encoding\nAPI property: encodingMode\nNew value: TWO_PASS\nOld value: STANDARD\nThe following changes impact the encoder’s\ninput resilience\n:\nAdd Stream (Decoding Error Mode): Default set to\nDUPLICATE_FRAMES\nWith this setting, the encoding will not fail if a frame cannot be decoded, but instead other frames are duplicated to compensate. The encoding can continue and an error message is displayed.\nAPI call:\nAdd Stream\nAPI property:\ndecodingErrorMode\nNew value:\nDUPLICATE_FRAMES\nOld value:\nFAIL_ON_ERROR\nStart Encoding (Variable FPS):\nhandleVariableInputFps\nset to true per default\nImpacts the way input streams with variable FPS are being handled. With the new default, the encoder will process files with a dynamic framerate automatically and adjust (e.g. by dropping or duplicating frames) if a constant framerate is required for the output file. For input streams with a fixed framerate, this change does not have any impact.\nAPI call:\nStart Encoding\nAPI property:\nhandleVariableInputFps\nNew value:\ntrue\nOld value:\nfalse\nStart Encoding (Tweaks):\naudioVideoSyncMode\nset to\nRESYNC_AT_START\nby default\nBy setting the synchronization mode to “RESYNC_AT_START” the encoder re-syncs the audio and the video stream, based on the values transported in the metadata. This is useful for workflows containing previously encoded content.\nAPI call:\nStart Encoding\nAPI property:\ntweaks\n->\naudioVideoSyncMode\nNew value:\nRESYNC_AT_START\nOld value:\nSTANDARD\nMigration quick guide: how to revert back to the old defaults\nIf you intend to use Bitmovin Encoder in the exact way that you used to before the update, you can do so by simply changing the settings from the new default values to the old values. The table below shows you which properties you will need to change to revert back to the previous behavior.\nAPI property\nChange to value\nAPI call in our API reference\nStart Encoding >\nencodingMode\nSINGLE_PASS\nStart (\nencodingMode\n)\nStart Encoding > trimming >\nignoreDurationIfInputTooShort\nFALSE\nStart (\nignoreDurationIfInputTooShort\n)\nStart Encoding >\nhandleVariableInputFps\nFALSE\nStart (\nhandleVariableInputFps\n)\nStart Encoding > tweaks >\naudioVideoSyncMode\nSTANDARD\nStart (\naudioVideoSyncMode\n)\nAdd Stream >\ndecodingErrorMode\nFAIL_ON_ERROR\nAdd Stream\nGet Started with a Bitmovin SDK\nBitmovin API SDK\nDescription\nJava\nIntegrate the SDK into your Java Project easily and add it to the config of your dependency manager like\nMaven\nor\nGradle\n.\nLearn more\nJavascript/Typescript\nIntegrate the SDK into your Javascript/Typescript based project easily by adding it as a dependency via\nNPM\n.\nLearn more\nPython\nIntegrate the SDK into your Python based project easily by adding it as a dependency via\npip\nor\nSetuptools\n.\nLearn more\n.NET\nIntegrate the SDK into your .NET based project easily by adding it as a dependency via\nnuget\n.\nLearn more\nPHP\nIntegrate the SDK into your PHP based project easily by adding it as a dependency via\ncomposer\n.\nLearn more\nVisit our\nGithub Example Repository\nthat provides you with examples for all Bitmovin SDK's available.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/bitmovin-bug-bounty-program",
    "title": "Bitmovin Bug Bounty Program",
    "text": "Bitmovin offers a Bug Bounty Program! Please\nsee this page\nto learn more about the scope of the program and how it works.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/encoder-release-notes",
    "title": "Encoder",
    "text": "Releases\nStable\n:\n2.225.0\nBeta\n:\n2.230.0\nLTS\n:\n2.165.0",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/troubleshooting-s3-output-failures-with-correct-credentials",
    "title": "Troubleshooting S3 Output Failures with Correct Credentials",
    "text": "When you are using cloud storages like AWS S3, or Google Cloud Storage a minimum set of requirements, and valid credentials are necessary for our encoding service to use your bucket as output destination and to operate on it accordingly.\nCommon Issues and how to solve them\nInvalid Access/Secret Key pairs\n- Those values are always a mix of letters, numbers and special characters, most people simply select those strings and copy&paste them. While doing that, it can happen that you accidentally copy a whitespace, tabulator or something else along with it, that eventually causes them to become invalid. Therefore, please ensure that only the string you want to copy&paste is selected and nothing else.\nDefault Bucket ACL rules\n- Amazon introduced additional default ACL's that are enabled by default when you create a bucket. Those can objects added to the bucket to be made publicly accessible.\nIf you want them to become publicly accessible when they are uploaded to your bucket by our encoding service,\nadjust your settings accordingly\n.\nAlternatively, make sure that you set the ACLPermissions to\nPRIVATE\non all resources associated with files to be written to the output (Muxings, Manifests, Sidecar, etc.)\nWrong Bucket Region\n- When creating an Input/Output resource in the Bitmovin API, it determines the bucket region automatically. Therefore, its highly recommended to use AUTO instead of selecting it manually. If you do that anyway, make sure to select the right region your bucket is located in, otherwise the upload process would fail.\nInsufficient Permissions\n- Our service requires a minimum set of permissions to be able to use a bucket as output destination. Learn more about these requirements for\nnecessary permissions\n.\nIf you use a custom policy, make sure that the Resource ARN match the output path you have configured.\nOverwriting existing files on the storage fails\n- If you look at the\nminimum set of requirements\n, you will see that being able to delete objects is not part of it. This also implies that we are not able to overwrite objects, e.g. if an object with the same name and path already exists. If you want to be able to do that, you would need to add the appropriate permission to your IAM policy.",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/managing-multiple-organizations",
    "title": "Managing Multiple Organizations",
    "text": "Your user can also be added to multiple organisations and/or you can create multiple suborganizations. This can be useful to distinguish between environments (development vs. production) or simply to separate organizations and teams.\nFirst, it's important to understand some key concepts of suborganizations:\n💡\nOrganizations & Suborganizations\nA suborganization ...\ncan only be created with a valid subscription plan.\nis part of a main organization and cannot have its own subscription; the suborganization uses the same plan as the main organization.\nonly has limited resources such as player licenses or encodings (depending on the configuration when creating the suborganization).\ndoesn't have its own billing and usage/overage tracking; this is billed via the main organisation.\nAlso, note that\nyou can create a maximum of 10 suborganizations.\nif you want to add a user to multiple suborganizations, you need to explicitly invite them to all of them.\nCreating a suborganization\nTo create a suborganization, go to the\nOrganization Overview\npage.\nClick on\n+ Add\nnext to\nSub-Organizations\nand provide a name & description:\nNext, you can configure the limits for the suborganization:\nOnce you click \"Save\" the new suborganization will be created and you can manage your suborganizations via the\nOrganization Overview\npage.\nSwitching between organizations\nYou can switch to a different (sub-)organization by clicking on the organization tab in the top right corner of the\nBitmovin Dashboard\n(next to your profile).",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/694fc5b-sub-org_step_1.png",
      "https://files.readme.io/a9a4f64-sub-org_step_2.png",
      "https://files.readme.io/4c0a00a-2022-08-25_17_49_19-Bitmovin_Dashboard.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/live-encoding-with-aws-mediatailor",
    "title": "Live Encoding with AWS Elemental MediaTailor",
    "text": "AWS Elemental Mediatailor\nis the SSAI service of AWS. To work with Bitmovin Live Encoding the following HlsManifestAdMarkerType should be enabled:\nEXT_X_CUE_OUT_IN\n. It also works with\nEXT_X_SPLICEPOINT_SCTE35\nand\nEXT_X_DATERANGE\n.\nPrerequisites\nYou will require your own AWS account and access to MediaTailor.\nEnsure you're familiar with Live Encoding settings match the requirements in\nLive Encoding with HLS, SCTE-35 and SSAI\nHow to Setup\nStart a Bitmovin Live Encoder\ne.g.: use the full Java API code example from\nLive Encoding with HLS, SCTE-35 and SSAI\nuse the Dashboard Wizard - in step 5, selecting HLS with TS muxing and selecting the compatible tags\nStart your Live ingest stream\ne.g.: Use the example method from the “Sample ingest with a demo file” from\nLive Encoding with HLS, SCTE-35 and SSAI\nSetup a MediaTailor configuration\nFor the “Content source” section enter the output HLS manifest path (it’s important to just add the path to the playlist and not the full playlist URL!)\nFor the “Ad decision server” enter the URL of your Ad server. (If you don’t have one you can use this simple VAST 3.0 XML file URL:\nhttps://storage.googleapis.com/bitmovin-content-cdn-origin/content/assets/scte35/vast3_10s_ad_example.xml\n)\nYou can follow this tutorial to setup your own simple VAST 3.0 XML file:\nBuild your own VAST 3.0 response XML to test with AWS Elemental MediaTailor | Amazon Web Services\nPlayback the AWS manifest URL\nCopy the “HLS playback prefix” and add the name of your manifest file (e.g.\nstream.m3u8)\ne.g.:\nhttps://f619923700b54c6583f41803d712433a.mediatailor.eu-west-1.amazonaws.com/v1/master/8d64d84b4083631c0cf405b35e994de0274ef50f/Bitmovin_SCTE-35_Live_Encoding_Tutorial/stream.m3u8\nNavigate to\nBitmovin's Test Player » Try our HTML 5 Video Player Demo\nand test the stream. Ideally, you should see ads being played at the correct positions.\nMediaTailor - Example configuration\nMediaTailor - Playback URL",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/4813a34-AWS_MediaTailor_workflow.png",
      "https://files.readme.io/a695010-MediaTailor_UI_SCTE_setup.png",
      "https://files.readme.io/d63bf3f-AWS_MediaTailor_Example_Setup.png",
      "https://files.readme.io/d695417-AWS_MediaTailor_Playback_URL.png"
    ],
    "videos": []
  },
  {
    "url": "https://developer.bitmovin.com/encoding/docs/aws-s3-role-based-live-outputs",
    "title": "AWS S3 Role Based Live Outputs",
    "text": "Overview\nDepending on the security policy your organisation has in place, you may be asked to provide Bitmovin with access to an S3 bucket with an IAM based role created in your organisation for third party organisations to use S3 resources.\nS3 role-based Inputs\nresp.\nS3 role-based Outputs` are an alternative way for our services to access your AWS (Amazon Web Services) S3 bucket to be used as a Live Output.\nInstead of you providing your AWS Access/Secret key pair to our Encoding or Analytics service, we provide you with an AWS IAM (Identity and Access Management) user name, which you can grant specific access rights in your account so it can access your desired S3 bucket.\nIn this tutorial, we will\ncreate an\nS3 bucket in your AWS account\nwhich will serve as the input or output storage for your encodings.\ncreate an\nS3 role based input or output in your Bitmovin account\nusing the Bitmovin API and Bitmovin Dashboard. This step will give you the\nexternalId\nwhich is used to communicate safely with your IAM role which you create in step 3\ncreate an\nIAM role in your AWS account\n, and attach an IAM policy to it. This policy states which of your buckets can be accessed by our user, and which permissions are granted to it.\nNOTE\n: This tutorial needs to be repeated for EACH account/sub-organization you want a S3-Role-Based access for.\nRequirements\na\nBitmovin account\n. Please make sure that you run this tutorial on the same Bitmovin account (and sub-organization, if applicable) that you want to use for production. If you don't have or know your production account yet, please be aware to repeat this tutorial later on your production account.\nan\nAWS account\nto create the input and/or output bucket and the Role that allows the Bitmovin system to access that bucket.\nS3 role-based buckets can be used for segmented outputs with\nencoder version\n2.29.0\nor higher.\nCreate an AWS S3 Bucket\nIn the AWS Management Console, open the\nS3 section\n.\nClick on the\nCreate Bucket\nbutton which starts the bucket creation wizard\nIn the \"Name and Region\" panel, choose a bucket name (for example\nmy-bitmovin-bucket\n) and a Region (for example\n(EU) Ireland)\n), then press\nNext\nConfigure the\nObject Ownership\ndepending on your needs a.\nACLs enabled\n, this is\nrequired\nif PUBLIC_ACCESS is configured for Encoding outputs. Encodings that are started in our\nBitmovin Dashboard\nset the PUBLIC_ACCESS to enable preview playback.\nb. ACLs disabled. If this policy is used Encoding outputs need to be set to have the PRIVATE ACL.\nConfigure\nBlock Public Access settings for this bucket\na. The default settings will\nBlock\nall\npublic access\nb. To\nenable playback\nfor manifests and files from the bucket,\nuncheck\n🔳\nBlock\nall\npublic access.\nFinish going through the wizard and click\nCreate Bucket\nTo allow players to request content for streaming from your S3 bucket, you will also need to allow origin access with a CORS configuration. See\nHow can I configure an AWS S3 Bucket to test playback of my content?\non how to configure this for your bucket.\nYour bucket is now ready to be used.\nCreate a Bitmovin S3 role-based Live Output\nBefore you actually create the Role in your AWS account, you need to create an S3 Role based Live Output with the Bitmovin API.\nThe minimal required information to create a Role based S3 input or output are the following :\nthe name of your S3 bucket that you created in the previous step\nYour AWS account number\nThe name you intend to give the Role in your AWS account (e.g.\nbitmovinCustomerS3Access\n).\nNow, you have the following variables:\nbucketName\n: the name of your bucket as in 1 above\nroleArn\n: from 2 and 3 above, create the Amazon Resource Name of the Role in the format of\narn:aws:iam::YOUR-AWS-ACCOUNT-NUMBER:role/YOUR-INTENDED-ROLE-NAME\nExternal ID\nFor more control over who can assume your AWS IAM Role, an\nexternalId\n(shared secret) should be used. You will create the\nexternalId\nwith the Bitmovin API and then configure the AWS Role with it.\nThe\nexternalId\nis returned by the Bitmovin API after creating an S3-role-based Output. Enable it by selecting one of the following\nexternalIdMode\n:\nexternalIdMode\n:\nGLOBAL\n- A consistent and unique ID is used, which is used for every S3 role-based output that you create in one Bitmovin account.\nGENERATED\n- A unique but random UUID is returned.\nCUSTOM\n(\ndeprecated\n) Define a custom external ID that you can use in your AWS IAM role definition.\nexternalId\n(\ndeprecated\n): required when using\nCUSTOM\nas\nexternalIdMode\n.\nWarning:\nCUSTOM\nis now deprecated and must not be used.\nNote:\nIf you use\nexternalIdMode.GLOBAL\n, each new S3 Role-based input or output you create within the same account will return the same\nexternalId\n. As you will configure your AWS Role with this\nexternalId\n, please be aware that if you run the same code on a different Bitmovin account or even a different Bitmovin sub-organization, the Bitmovin API will return a different\nexternalId\n. This will lead AWS to deny access to the Role.\nTherefore, please create the Role-based input or output on the same Bitmovin account and sub-organization that you will use later in production.\nNote:\nIf you use\nexternalIdMode.GENERATED\n, each new S3 Role-based input or output you create will generate a new\nexternalId\n. Thus, you can only assume your AWS Role by re-using the S3 Role-based input or output whose\nexternalId\nyou used to configure the AWS Role.\nCreating the S3 Role Based Input or Output\nNow, using\nbucketName\n,\nroleArn\nand\nexternalIdMode\nfrom above, create an S3 Role based output (resp. input) with the Bitmovin API.\nSee the Java and cURL examples below.\nIn the response, you will receive the\nexternalId\nto be used in the next section when you create the Role.\nCreate an AWS IAM Role\nIn order to continue, you will have to create a\nRole\nin your AWS account.\nLogin to your AWS account.\nClick on \"Services\" near the top left.\nLook for \"Security, Identity & Compliance\" and click on \"IAM\". You are now in the\nIdentity and Access Management (IAM)\npage of your account.\nOn the left pane, click on \"Access Management\" -> \"Roles\".\nClick on \"Create Role\". The\nCreate Role\npage appears.\nThe page shows you four boxes of which you can select one for a type of trusted entity. Click on the \"Another AWS account\" box.\nIn the field \"Account ID\", enter\n630681592166\n.\nNext to \"Option\", check the \"Require external ID\" checkbox. A box opens asking you to enter an external ID.\nEnter the\nexternalId\nthat you got in the previous section.\nClick on \"Next: Permissions\"\nAssign a policy to the role by selecting it in the policy list.\n(Note: The pre-defined\nAmazonS3FullAccess\npolicy is known to be suitable but since it provides unrestricted access to your bucket, you might need to create a custom policy with fine-tuned access rights. Please review details of the permissions required for\nbuckets for Encoding Input and Output\nor\nbuckets for Analytics Exports\nbuckets\n)\nClick on \"Next: Tags\". The\nAdd Tags\npage appears, on which you optionally can assign tags to the role.\nClick \"Next: Review\". The\nReview\npage appears. Give the new role a name.\nRole name MUST match the YOUR-INTENDED-ROLE-NAME specified in Create an S3 role-based Input/Output > in\nroleArn\nClick \"Create Role\". You are now back in the\nIdentity and Access Management(IAM)-Roles\npage, and the system tells you \"The role Bitmovin has been created\". You also see the new role in the list of roles in your account.\nIf you want to learn more about Roles in AWS, please see their\ndocumentation\n.\nJSON Payload\nIf you prefer using the AWS CLI tools, you can create this role with the following JSON payload.\nJSON\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::630681592166:user/bitmovinCustomerS3Access\"\n      },\n      \"Action\": \"sts:AssumeRole\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"sts:ExternalId\": \"<EXTERNAL_ID_RETURNED_BY_BITMOVIN_API>\"\n        }\n      }\n    }\n  ]\n}\n(Java) S3 Role-Based Output Example\nThis example uses our latest\nOpen API client for Java\n, which is available on Github. This example shows how to create an Output.\nCreate a new S3 Role-Based Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\n\nAclEntry aclEntry = new AclEntry();\naclEntry.setPermission(AclPermission.PRIVATE);\n\nList<AclEntry> acl = new ArrayList<>();\nacl.add(aclEntry);\n\nS3RoleBasedOutput s3RoleBasedOutput = new S3RoleBasedOutput();\ns3RoleBasedOutput.setBucketName(\"<BUCKET_NAME>\");\ns3RoleBasedOutput.setRoleArn(\"<AWS_ARN_ROLE>\");\ns3RoleBasedOutput.setExternalIdMode(ExternalIdMode.GLOBAL);\ns3RoleBasedOutput.setAcl(acl);\n\ns3RoleBasedOutput = bitmovinApi.encoding.outputs.s3RoleBased.create(s3RoleBasedOutput);\nHint: In case you chose to enable\nBlock public access\non your S3 bucket (recommended), you would have to make sure that the ACL is set to\nPRIVATE\non the output (as shown above) as well as on your Muxing configurations.\nTo create an Input is fairly similar, but you just use the\nS3RoleBasedInput\nresource and the\nbitmovinApi.encoding.inputs.s3RoleBased\nendpoint\nUse an existing S3 Role-Based Output\nJava\nbitmovinApi = BitmovinApi.builder().withApiKey(\"YOUR_BITMOVIN_API_KEY\").build();\nS3RoleBasedOutput s3RoleBasedOutput = bitmovinApi.encoding.outputs.s3RoleBased.get(\"YOUR_S3_ROLE_BASED_OUTPUT_ID\");\nUsing the Bitmovin Dashboard\nIn the\nBitmovin Dashboard\nnavigate to Live Encoding ->\nOutputs\nPress\n+ Create\nand select\nROLEBASED S3\nand a form will be provided where you can enter a Bitmovin name, that can be freely chosen will be seen by users in the dashboard when selecting an output, as well as the details required from AWS. Optionally you can also provide a description, for other users or admins to describe the function of the live output.\nRemember to select the correct cloud correctly.\nOnce your happy press\nCreate\nand the bucket will be saved.\nUsing the Bitmovin API\nCreate a new S3 Role-Based Output\nAPI reference:\ncreate a Role-Based S3 Output\n:\nShell\ncurl -X POST \\\n  https://api.bitmovin.com/v1/encoding/outputs/s3-role-based \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY' \\\n  -d '{\n    \"bucketName\": \"<BUCKET_NAME>\",\n    \"roleArn\": \"<AWS_ARN_ROLE>\",\n    \"externalIdMode\": \"<GLOBAL|GENERATED>\"\n}'\nGet an existing S3 Role-Based Output\nAPI reference:\nget an S3 Role-Based Output\nShell\ncurl  https://api.bitmovin.com/v1/encoding/outputs/s3-role-based/YOUR_S3_ROLE_BASED_OUTPUT_ID \\\n  -H 'Content-Type: application/json' \\\n  -H 'x-api-key: YOUR_BITMOVIN_API_KEY'\nUsing the Live Output\nThe bucket will appear in the Outputs list, and in the Wizard under AWS Role Based outputs.\nYou can confirm the bucket is created in the API by using\nList S3 Role-based Outputs",
    "images": [
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/fbf05e1-4370cbd-bitmovin-logo-horizontal-docs-black.svg",
      "https://files.readme.io/132bb24-Screenshot_2024-04-05_at_18.41.26.png",
      "https://files.readme.io/56ef1be-Screenshot_2024-04-06_at_12.28.24.png",
      "https://files.readme.io/a8ef86d-image.png"
    ],
    "videos": []
  }
]